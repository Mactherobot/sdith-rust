\documentclass[twoside,11pt,openright]{report}

\usepackage[latin1]{inputenc}
\usepackage[american]{babel}
\usepackage{a4}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[]{amsmath}
\usepackage{epsfig}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[useregional]{datetime2}
\DTMlangsetup[en-US]{showdayofmonth=false}
\usepackage{lipsum}
\usepackage{ctable} % for \specialrule command

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newcommand{\definitionautorefname}{Definition}

\addto\extrasamerican{%
  \def\chapterautorefname{Chapter}%
  \def\sectionautorefname{Section}%
  \def\subsectionautorefname{Section}%
  \def\lemmaautorefname{Lemma}%
}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]

\newtheoremstyle{lemma}% <name>
{3pt}% <Space above>
{3pt}% <Space below>
{\normalfont}% <Body font>
{}% <Indent amount>
{\upshape}% <Theorem head font>
{:}% <Punctuation after theorem head>
{.5em}% <Space after theorem headi>
{}% <Theorem head spec (can be left empty, meaning `normal')>

\renewcommand*\sfdefault{lmss}
\renewcommand*\ttdefault{txtt}

\newcommand{\todo}[1]{{\color[rgb]{.5,0,0}\textbf{$\blacktriangleright$#1$\blacktriangleleft$}}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{empty}
\pagenumbering{roman}
\vspace*{\fill}\noindent{\rule{\linewidth}{1mm}\\[4ex]
{\Huge\sf SD-in-the-Head rust implementation and optimization}\\[2ex]
{\huge\sf Hugh Benjamin Zachariae, 201508592 \\ Magnus Jensen,
201708626}\\[2ex]
\noindent\rule{\linewidth}{1mm}\\[4ex]
\noindent{\Large\sf Master's Thesis, Computer Science\\[1ex]
  \today \\[1ex] Advisor: Diego F. Aranha\\[15ex]}\\[\fill]}
\epsfig{file=logo.eps}\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{plain}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

\todo{in English\dots}

\chapter*{Resum\'e}
\addcontentsline{toc}{chapter}{Resum\'e}

\todo{in Danish\dots}

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

\todo{\dots}

\vspace{2ex}
\begin{flushright}
  \emph{Hugh Benjamin Zachariae and Magnus Jensen}\\
  \emph{Aarhus, \today.}
\end{flushright}

\tableofcontents
\cleardoublepage
\pagenumbering{arabic}
\setcounter{secnumdepth}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}\label{ch:intro}

few pages. Introduce what we have done and how the paper is structured

\section{Post-Quantum Cryptography}\label{sec:quantum}

Digital signatures are essential for ensuring a secure internet. As our individual reliance on the internet grows, they play a critical role in identifying and verifying the authenticity of the sources we interact with over the network. Digital signatures ensure that software or web pages come from trusted sources and have not been tampered with, forming a cornerstone of IT security. They also enable the verification of the signer's identity and help detect unauthorized changes to data. Additionally, digital signatures support non-repudiation, meaning they provide proof to a third party that a signature was genuinely created by the signer.

The most widely used digital signature schemes rely on public-key cryptography. Each signer has a public and a private key. The private key allows the signer to sign messages, while the public key is used to verify the signature. The security of a signature is grounded in the computational complexity of specific mathematical problems. Standardization efforts in the early 2000s~\cite{pub2000digital} led to the adoption of algorithms such as RSA, DSA, and ECDSA, which leverage intractability of factoring large integers and solving discrete logarithms to ensure robust protection.

\subsection{NIST call to action}

In 1994, the discovery of Shor's algorithm~\cite{shor1997} introduced a polynomial-time method for solving these problems using quantum computing. At that time, quantum computers were theoretical constructs. However, in 2001, Chuang et al.~\cite{vandersypen2001experimental,buchmann2004post} successfully implemented Shor's algorithm on a 7-qubit quantum computer. Two decades later, significant advancements in quantum technology have brought us closer to the point where quantum computers could be a reality.

A necessary pre-requisite for a post-quantum secure signature scheme, is the existence of a problem that is intractable for quantum computers. Several candidates from complexity theory have been proposed.

In 2016, the National Institute of Standards and Technology (NIST), initiated a call for quantum resistant public-key cryptosystems~\cite{nistcall}. Initial efforts led to the standardization of three signature schemes, namely Dilithium~\cite{ducas2018crystals}, Falcon~\cite{fouque2018falcon} and SPHINCS+~\cite{bernstein2019sphincs+}. All three of the schemes are based on the hardness of structured lattice problems. While these schemes are promising, they are all based on the same underlying assumption, which could be a potential risk if the assumption is broken. This has led to the most recent call in 2022 for additional quantum resistant signature schemes based on different assumptions to diversify the standards available for the future.

\subsection{Code-based problems}

With the introduction of public-key cryptosystems by Diffie and Hellman in 1976, we saw the introduction to schemes like RSA, that is based on factoring of large primes and ElGamal which is based on the discrete logarithm problem. While less known, there exists schemes based on coding theory -- its own discipline of information theory which predates the introduction of public-key cryptography. In 1978, McEliece introduced a PK scheme based on coding theory~\cite{mceliece1978public} -- only two years later -- based on the hardness of decoding random linear codes \cite{berlekamp1978inherent}.

While the scheme never saw widespread adoption due to its large key sizes, the interest in code-based crypto-schemes have seen a resurgence as it is not vulnerable to known quantum attacks like Shor's algorithm. Recent developments in code-based cryptosystems have shown that the performance needed for code based cryptographic schemes practically is within reach. Introductions of linear based codes on finite fields and MPCitH (Multi-Party Computation in the Head) threshold schemes \cite{baum2020concretely} show promising results. With the SD-in-the-Head scheme~\cite{aguilarsyndrome11,feneuil2023threshold} presented in this report, such techniques are combined to define an efficient post-quantum secure signature scheme.

\section{Our contributions}

\todo{describe how we progressed through the project in an interesting way. What did we set out to do, what were the hurdles.}

In this report, we present an implementation of the SD-in-the-Head protocol in Rust. While the NIST standardization process initially prioritizes selecting the appropriate protocols, the ultimate goal is to produce a well-documented specification that enables implementors to safely and accurately reproduce the chosen protocols.

To this end, it is crucial for both NIST and the authors to provide an in-depth specification accompanied by clear, well-documented examples to facilitate a smooth transition to the new standard.

Our objectives include delivering a high-performance, thoroughly documented implementation, complemented by extensive test coverage to ensure correctness and security.

Additionally, we aim to identify and address potential pitfalls and challenges encountered during the implementation process and supply these to the authors.

Finally, this project serves as an opportunity to test our capabilities by implementing a cutting-edge cryptographic protocol in a modern, memory-safe language like Rust. As the final product, we will deliver this report alongside a Rust library and a client implementation for the SD-in-the-Head protocol.


\subsection{Choosing the SD-in-the-Head protocol}
The SD-in-the-Head protocol offers a highly modular design, enabling extensive configurability and support for various subroutines. This flexibility is particularly evident in the two variants included in the specification~\cite{aguilarsyndrome11}: the \textit{hypercube} and \textit{threshold} variants, both of which demonstrate significant performance improvements over the original protocol~\cite{feneuil2022syndrome,aguilar2023return,feneuil2023threshold}.

The threshold variant, in particular, provides an opportunity to explore and implement several cryptographic primitives, including Galois Fields~\cite{brownadvanced}, Merkle Trees~\cite{becker2008merkle}, Linear Secret Sharing (Shamir), and Multi-Party Computation in the Head (MPCitH)~\cite{baum2020concretely}.

Additionally, the authors supply a detailed and well-crafted specification, along with a reference implementation in C++, which serves as a robust starting point for our work.


\subsection{Choosing Rust}
We chose to implement the SD-in-the-Head protocol in Rust for several compelling reasons. Rust, as a modern systems programming language, offers inherent memory and thread safety without compromising performance. Its strong safety guarantees are a key reason why Rust is included in the NIST list of safer programming languages \cite{nistsaferlanguages}. These properties are particularly critical when implementing cryptographic primitives, where memory safety and robustness against concurrency issues are foundational to secure software development. Additionally, Rust provides fine-grained control over hardware resources, enabling the optimization of performance -- an essential consideration in cryptographic implementations.

Our choice of Rust was also driven by a desire to investigate its strengths and limitations in the context of implementing and optimizing cryptographic protocols. To this end, we prioritized code readability and maintainability. For maintainability, we incorporated rigorous testing frameworks and benchmarking tools to validate the correctness and efficiency of the implementation. These measures also facilitate meaningful comparisons with alternative signature schemes. For readability, we emphasized clear, well-documented, and logically structured code.

Given the inherent flexibility and modular nature of the SD-in-the-Head protocol, our implementation leverages Rust's feature flags to enhance modularity and interchangeability. This approach not only supports a high degree of configurability but also ensures that the implementation remains adaptable to future changes and extensions.

Other signature schemes. How many signatures can be generated per second.

\section{Structure of the report}
The report is structured as follows: \autoref{ch:prelim} provides the necessary background information on the cryptographic primitives and sub-protocols used in the SD-in-the-Head protocol. \autoref{ch:spec} presents the specification of the SD-in-the-Head protocol, focusing on the threshold variant. \autoref{ch:impl} details our implementation of the protocol in Rust, including the development process, design choices, and optimizations. \autoref{ch:bench} evaluates the performance of our implementation and compares it to the reference implementation and other alternatives. Finally, \autoref{ch:conclusion} summarizes our contributions.

\todo{Keep this up to date}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preliminaries}\label{ch:prelim}

In this section, we outline the mathematical and cryptographic foundations necessary to understand the SD-in-the-Head scheme and its components, providing a step-by-step progression from the basics to the construction of the protocol.

We begin by introducing the essential subroutines that underpin the SD-in-the-Head protocol. This includes \textbf{Galois Finite Fields} $\mathbb{F}_q$ \cite{martinez2023syndromes, reed1960polynomial, brownadvanced} along with cryptographic primitives; \textbf{collision-resistant hash functions} and \textbf{Merkle tree commitment scheme} \cite{becker2008merkle}. In addition, we provide short introductions to the foundational protocols of \textbf{Secure Multi-Party Computation} (MPC) and \textbf{Zero-Knowledge} (ZK) proofs, which serve as the underlying cryptographic protocols.

Next, we delve into the Syndrome Decoding (SD) problem, which forms the backbone of the SD-in-the-Head protocol. This includes an overview of the problem definition \cite{aguilarsyndrome11, mceliece1978public, berlekamp1978inherent, baldi2013optimization}, and its polynomial representation. This will support an understanding of how the protocol operates on hard instances of the SD problem.

Building upon these foundations, we lay the groundwork for the construction of the SD-in-the-Head protocol. First, we discuss the \textbf{Multi-Party-Computation-in-the-Head} (MPCitH) paradigm, explaining how it is leveraged to construct efficient ZK proofs \cite{ishai2007zero}. Additionally we detail the role of the MPC preprocessing \cite{baum2020concretely} and linear secret-sharing scheme \cite{feneuil2023threshold}, which dramatically improves the efficiency of the MPCitH construction, and verification. Next, we introduce the \textbf{Fiat-Shamir heuristic}, a powerful tool that transforms an interactive ZK protocol into a non-interactive signature scheme \cite{fiat1986prove}.

\section{Galois Finite Field}\label{sec:gf256}
Finite fields lays the basis for many cryptographic protocols and primitives. In essence, finite field theory describes the research into the relation and properties between numbers, while disregarding the numbers themselves. In order to define finite fields, we first need to define a \textit{group} and a \textit{finite field} (\autoref{def:group} and \autoref{def:field}, respectively).

\begin{definition}
  \label{def:group}
  A group $\mathbb{G}$ is a tuple $(\mathbb{G}, \times, 1)$ where, $\mathbb{G}$ is the set of elements in the group, $\times$ is a binary operator, and $1$ is the multiplicative identity. The group $\mathbb{G}$ must satisfy the following properties:
  \begin{enumerate}
    \item $\mathbb{G}$ is \textbf{closed} under $\times$. For all $a,b \in \mathbb{G}$, $a \times b \in \mathbb{G}$.
    \item $\mathbb{G}$ is \textbf{associative} under $\times$. For all $a,b,c \in \mathbb{G}$, $(a \times b) \times c = a \times (b \times c)$.
    \item $\mathbb{G}$ has an \textbf{identity} element $1$ under $\times$. For all $a \in \mathbb{G}$, $a \times 1 = a$.
    \item $\mathbb{G}$ has an \textbf{inverse} element for each element under $\times$. For all $a \in \mathbb{G}$, there exists an element $a^{-1}$ such that $a \times a^{-1} = 1$.
  \end{enumerate}
\end{definition}

\begin{definition}
  \label{def:field}
  A finite field $\mathbb{F}$ is a tuple $(\mathbb{F}, +, \times, 0, 1)$ where, $\mathbb{F}$ is the set of elements in the field, $+$ and $\times$ are the addition and multiplication operators, and $0$ and $1$ are the additive and multiplicative identities. The field $\mathbb{F}$ must satisfy the following properties:
  \begin{enumerate}
    \item  $+$ and $\times$ are \textbf{commutative}, \textbf{associative} and $\times$ is \textbf{distributive} over $+$.
          \begin{enumerate}
            \item $\forall a,b \in \mathbb{F}: a + b = b + a$ and $a \times b = b \times a$ (commutative)
            \item $\forall a,b,c \in \mathbb{F}: (a + b) + c = a + (b + c)$ and $(a \times b) \times c = a \times (b \times c)$ (associative)
            \item $\forall a,b,c \in \mathbb{F}: a \times (b + c) = a \times b + a \times c$ (distributive)
          \end{enumerate}
    \item $(\mathbb{F}, +, 0)$ forms an additive group
    \item $(\mathbb{F} \setminus \{0\}, \times, 1)$ forms a multiplicative group
  \end{enumerate}
\end{definition}

\noindent
The most commonly known finite fields are the prime fields $\mathbb{F}p$, widely used in protocols like RSA. These fields can be extended into $\mathbb{F}{p^n}$, containing $p^n$ elements. However, prime fields are not efficient for iterating over large numbers of digits, a common requirement in protocols such as SD-in-the-Head. For better efficiency, we require a field that can be represented using bits or bytes. One method to achieve this is by constructing fields commonly known as Galois fields. Consider the simplest prime field $\mathbb{F}_2 = ({0,1}, \texttt{XOR}, \texttt{AND}, 0, 1)$. It is straightforward to verify that this field satisfies the properties outlined in \autoref{def:field}.

The Galois field $\mathbb{F}{2^n}$ is an extension field of $\mathbb{F}2$, defined as $\mathbb{F}{2^n}$. This field is constructed by selecting an irreducible polynomial $p(x)$ of degree $n$ with coefficients in $\mathbb{F}2$. The elements of $\mathbb{F}{2^n}$ are represented as polynomials of degree at most $n-1$ with coefficients in $\mathbb{F}2$. Consequently, $\mathbb{F}{2^n}$ contains exactly $2^n$ elements. The polynomial $p(x)$ ensures that $\mathbb{F}_{2^n}$ forms a valid field as it defines the modular reduction in multiplication.

Each element in $\mathbb{F}_{2^n}$ is represented as a polynomial of degree at most $n-1$ with coefficients in $\mathbb{F}_2$.

\begin{align*}
  a(x) = a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \dots + a_1x + a_0
\end{align*}

where $a_i \in \mathbb{F}_2$ This corresponds directly into the 8-bit binary representation $a_{n-1}a_{n-2}\dots a_1a_0$. The addition and multiplication operations within $\mathbb{F}{2^n}$ are defined as follows:

\begin{itemize}
  \item \textbf{Addition}: The addition of two elements $a,b \in \mathbb{F}_{2^n}$ is defined as the bitwise XOR of the two elements.
  \item \textbf{Multiplication}: The multiplication of two elements $a,b \in \mathbb{F}_{2^n}$ is defined as the multiplication of the two polynomials modulo the irreducible polynomial $p(x)$.
\end{itemize}

A common choice for the irreducible polynomial is the Rijndael polynomial $p(x) = x^8 + x^4 + x^3 + x + 1$ used in the AES protocol \cite{brownadvanced}. This field is commonly known as $\mathbb{F}_{2^8}$. The size of the field allows us to represent elements as bytes, which is efficient for iterating over large numbers of digits and it contains all the elements of a byte.

\subsubsection{Field extension}

An advantageous property of Galois fields is that they can easily be extended, like $\mathbb{F}_{2}$ to $\mathbb{F}_{2^8}$. To ensure security in the SD-in-the-Head protocol, the field $\mathbb{F}_{2^8}$, which we shall denote as $\mathbb{F}_q$ is extended to encompass the field 32-bit unsigned integers. This can be done as a \textit{tower extension} by first building a degree-2 extension to $\mathbb{F}_{q^2}$ using the irreducible polynomial $F_{q^2} = F_q[X] / (X^2 + X + 32)$ and finally extending this field to $\mathbb{F}_{q^4}$ using the irreducible polynomial $F_{q^4} = F_q[Z] / (Z^2 + Z + 32(X))$. While you could create the final field independently, the tower extension allows us to reuse the construction of the intermediate field. Addition and multiplication in the extension fields are represented as the following

\begin{align*}
  a     & = a_0 + a_1x                                               &  & a \in \mathbb{F}_{q^\eta}, a_0, a_1 \in \mathbb{F}_q \\
  b     & = b_0 + b_1x                                               &  & b \in \mathbb{F}_{q^\eta}, b_0, b_1 \in \mathbb{F}_q \\\\
  a + b & = (a_0 + a_1x) + (b_0 + b_1x) = (a_0 + b_0) + (a_1 + b_1)x                                                           \\
  a * b & = (a_0 + a_1x) * (b_0 + b_1x)                                                                                        \\
        & = a_0b_0 + (a_0b_1 + a_1b_0)x + a_1b_1x^2                                                                            \\
        & = a_0b_0 + (a_0b_1 + a_1b_0)x + a_1b_1(x + 32)                                                                       \\
        & = a_0b_0 + 32a_1b_1 + (a_0b_1 + a_1b_0 + a_1b_1)x
\end{align*}

\section{Cryptographic Primitives and protocols}
We assume the reader has basic knowledge of cryptographic primitives but will provide brief introductions to the primitives used in the SD-in-the-Head protocol. If you know the basics feel free to skip this section.

\subsection{Collision Resistant Hash Functions}
We denote a hash function by a generator $\mathcal{H}$ which on input of a security parameter $k$ outputs a function $h : \{0,1\}^* \rightarrow \{0,1\}^k$. We denote a hash function $h$ to be cryptographically secure if it satisfies the following properties

\begin{lemma}[Preimage Resistance]
  \label{lem:preimage}
  Given a hash function $h$ and a hashed message $c$, there exist no PPT algorithm that can find a message $m$ such that $h(m) = c$ with non-negligible probability.
\end{lemma}

\begin{lemma}[Collision Resistance]\label{lem:collision}
  Given a hash function $h$, there exist no PPT algorithm that can find two distinct messages $m, m'$ such that $h(m) = h(m')$ with non-negligible probability.
\end{lemma}

The consensus among researchers indicates that for combinatorial problems, such as those underlying SHA2 and the more recent SHA3, quantum computing is expected to reduce the security level from $k$ to $k/2$ for preimage resistance through approaches like Grover's algorithm~\cite{nielsen2010quantumgrover}. In contrast, no known methods have been proposed to reduce the security level for collision resistance. Consequently, protocols that depend on collision-resistant hash functions, such as Merkle Signature or SD-in-the-Head, remain a viable choice for post-quantum secure protocols.

In proving the security of hash functions, researchers often resort to models like the Quantum Random Oracle Model (QROM), while there is significant work being done to further prove the future security of hash functions~\cite{dtuPostquantumSecurity}.

\subsection{Merkle Tree Commitment Scheme}
A commitment scheme allows a \textit{prover} to commit to a value $v$ and later prove that the revealed value is identical to the one initially committed to. The scheme satisfies the following two properties:

\begin{lemma}[Binding]
  \label{lem:binding}
  Given a commitment $c$ to a value $v$, a dishonest prover can only change the committed value and have the verifier accept the new value with negligible probability.
\end{lemma}

\begin{lemma}[Hiding]
  \label{lem:hiding}
  Given a commitment $c$ to a value $v$, the verifier can only recover the value $v$ with negligible probability.
\end{lemma}

A variant of such a scheme which allows for partial opening, is the \textit{Merkle Tree Commitment scheme}.

Let $v_1, \dots, v_n$ be the values to be committed, and let $h$ denote a cryptographically secure hash function. The leaves of the Merkle tree, $a_{0,1}, \dots, a_{0,n}$, are initialized as the hash values of the input values:
\[
  a_{0,i} = h(v_i), \quad \text{for } i = 1, \dots, n.
\]

Each parent node $a_{i,j}$, where $i$ represents the level of the tree and $j$ denotes the parent index at that level, is computed as the hash of its two child nodes:
\[
  a_{i,j} = h(a_{i-1,2j} \, | \, a_{i-1,2j+1}),
\]
where $|$ indicates concatenation of the child nodes.

The root node, $a_{h,0}$, where $h$ is the height of the tree, serves as the commitment value and is sent to the verifier.

To reveal a committed value $v$, the prover computes the authentication path for the value's leaf node $a_{0,p}$, where $p$ is the index of the value. The authentication path $A$ consists of the sequence of sibling nodes required to reconstruct the root. The first element in the authentication path, $A_0$, is the sibling of the leaf node:
\[
  A_0 =
  \begin{cases}
    a_{0,p-1}, & \text{if } p \text{ is odd,}  \\
    a_{0,p+1}, & \text{if } p \text{ is even.}
  \end{cases}
\]
This element is recorded as $auth_0$.

For higher levels, each subsequent element in the path is determined as:
\[
  A_i = h(A_{i-1} \, | \, auth_{i-1}),
\]
where $auth_i$ is the sibling node of $A_{i-1}$ at the same level.

The prover transmits the authentication path $A$ and the index $p$ of the selected value to the verifier. The verifier checks the validity of the commitment by using the provided leaf value and authentication path to recompute the root of the Merkle tree.

\subsubsection{Security properties}

The hiding property of the protocol is ensured by the use of a secure hash function, as the prover only shares the root of the tree. If the adversary were able to recover any value $v$, this would break the preimage resistance \autoref{lem:preimage} of the hash function.

Conversely, the binding property \autoref{lem:binding} is upheld by the fact that the adversary would have to find a collision for the hash function to change the commitment value.

A notable property of the Merkle tree is its ability to open a partial subset of the committed values by providing only the necessary authentication paths for the values in the subset. The verifier can still validate these values using a single global root value.

This property is particularly advantageous in protocols like the SD-in-the-Head threshold protocol, where only a subset of the committed values needs to be opened while keeping the rest concealed. By leveraging this feature, the prover can efficiently prove the validity of the subset without revealing the entire set of commitments.

\subsection{Secure Multi-Party Computation}\label{sec:mpc}

Secure Multi-Party Computation (MPC) refers to a cryptographic protocol enabling multiple parties to jointly compute a function $f$ represented by a circuit $C$, ensuring that no information about the individual inputs is revealed beyond what can be inferred from the output of $C$. In the following, sections we will denote a MPC protocol by $\Pi_f$ for an $n$-party functionality $f(x, w_1, \dots, w_n)$, a public input $x$ and secret inputs $w_i$ of the party $P_i$. The goal is to compute the function $f$ on the inputs of the $n$ parties while upholding the following properties~\cite{cramer2015secure}

\begin{itemize}
  \item \textbf{Correctness}: The output of the protocol is the correct evaluation of the function $f$ on the inputs of the parties $w_i$ and the public input $x$
  \item \textbf{Privacy}: The protocol ensures that no partylearns anything about the inputs of the other parties beyond what can be inferred from the output of the function.
\end{itemize}

In terms of security needed for the SD-in-the-Head protocol, we define the following security requirements for the MPC protocol $\Pi_f$

\begin{itemize}
  \item \textbf{Semi-honest security}: The protocol is secure against semi-honest adversaries, where parties follow the protocol but may attempt to learn information from the messages they receive.
  \item \textbf{Low-threshold security}: The protocol is secure against a coalition of up to $\ell$ parties, where $\ell$ is the threshold. This is also known as a $\ell$-private MPC protocol.
\end{itemize}

We denote the view $V_i$ of a party $P_i$ in the protocol as $(i, x, r_i, w_i, (m_1, \dots, m_j))$ where $r_i$ is the randomness used by $P_i$, $w_i$ is the secret share, and $(m_1, \dots, m_j)$ is the messages received by $P_i$ in the first $j+1$ rounds of the protocol. We say that two views $V_i, V_j$ are \textit{consistent} according to the public input $x$ if the views are identical given the public input $x$.

\subsection{Zero-Knowledge Proofs}\label{sec:zk}
This section will only give a brief introduction to \textit{two-party interactive} Zero-Knowledge (ZK) schemes for a \textit{prover} and a \textit{verifier}. We denote a ZK $\Pi_{\mathcal{R}}$ for some NP relation $\mathcal{R}(x, w)$. Let $x$ be a public statement in \textbf{NP} and $w$ be a witness such that $(x, w) \in \mathcal{R}$~\cite{feneuil2023threshold}.

\begin{definition}
  Let $x$ be a statement of language $L$ in \textbf{NP}, and $W(x)$ the set of witnesses for $x$ such that the following relation holds:
  \begin{align*}
    \mathcal{R} = \{(x, w)\; x \in L, w \in W(x)\}
  \end{align*}
\end{definition}

\section{Syndrome Decoding Problem}\label{sec:syndrome}

The SD-in-the-Head protocol is built on the computational hardness of the Syndrome Decoding (SD) problem for random linear codes over a finite field (see \autoref{sec:gf256}). This protocol uses a variant of the SD problem, referred to as the \textit{coset weights} problem, first introduced by Berlekamp and McEliece in 1978~\cite{berlekamp1978inherent}. The problem is defined as follows:
\begin{definition}\label{def:syndrome}
  Given $H \in \mathbb{F}^{(m-k)\times m}_q$ and $y \in \mathbb{F}^{m-k}_q$. The problem is to find $x \in \mathbb{F}^m_q$ s.t.\ wt$(x) \leq w$ and $Hx = y$.
\end{definition}
Generating such an instance is straightforward: one can construct a uniformly random parity-check matrix $H$ and a codeword $x$ (with $wt(x) \leq w$), and then compute the syndrome $y = Hx$. In the SD-in-the-Head protocol, the values of the matrix $H$ and the syndrome $y$ are elements of the finite field $\mathbb{F}_q$ which we will explain further in \autoref{sec:gf256}. The SD problem is well-known to be NP-complete for random instances \cite{berlekamp1978inherent} also referred to as the general decoding problem.To illustrate the computational difficulty, solving the problem using brute force would require $O(\binom{m}{w} q^w)$ operations, which is computationally infeasible for large $m$ and $k$.

\subsubsection{Standard form of the parity-check matrix}\label{sec:standard_form_of_the_parity_check_matrix}
To improve the performance and reduce the key size of the protocol, its possible to utilize the fact that the matrix $H$ can be in standard form. $H = (H'|I_{m-k}) $ Where $H' \in \mathbb{F}^{(m-k)\times k}_q$. This allows for the following representation of the syndrome:
\begin{equation}
  y = Hx = H'x_a + x_b
  \label{eq:standard_form_of_the_parity_check_matrix}
\end{equation}
with $x = (x_a | x_b)$. This improves the performance of the algorithms used in the SD-in-the-Head protocol in the following ways
\begin{itemize}
  \item At the MPC layer, we only need to reveal one share $x_a$. Due to the fact that the other share $x_b$ can simply be recomputed by $x_b = y - H'x_a$.
  \item By linearity of the above relation one only needs to send $x_a$ in order to recover the SD instance. So from a sharing of $x_a$ one can check the correctness of the SD instance.
\end{itemize}

\subsubsection{Polynomial representation of SD}\label{sec:polynomial_representation}
In order to convert the SD problem into a form that allows for \textit{Multi Party Computation in the Head} (MPCitH, which we will exlpain further in \autoref{sec:mpcinth}) the SD-in-the-Head protocol is based on three (witness-dependent) polynomials $S, Q$ and $P$, and one public polynomial $F$.
These are used for checking the correctness of the SD solution by verifying the following relation:
\begin{equation}
  \centering
  S\cdot Q = P\cdot F
  \label{eq:polynomial_representation}
\end{equation}
Let $f_1,\dots, f_q$ denote elements of $\mathbb{F}_q$, then the polynomials are defined as:
\begin{itemize}
  \item $S\in \mathbb{F}_q[X]$ is the Lagrange interpolation of the coordinates of $x$, such that it matches $S(f_i) = x_i$ for $i\in [1:m]$ and has degree $\text{deg}(S) \leq m-1$
  \item $Q\in \mathbb{F}_q[X]$ is defined by $Q(X) = \prod_{i\in E}(X - f_i)$. $E \subset [1:m]$ with order $|E| = w$, such that $E$ contains the non-zero coordinates of $x$. $Q$ has degree $\text{deg}(Q) = w$.
  \item $P\in \mathbb{F}_q[X]$ is defined as $P = S\cdot Q/F$ and has degree $\text{deg}(P) \leq w-1$. By definition the polynomial $F$ divides $S\cdot Q$.
  \item $F\in \mathbb{F}_q[X]$ is the \textit{vanishing polynomial} of the set ${f_1, \dots, f_m}$ also defined as $F(X) = \prod_{i\in [1:m]}(X - f_i)$ and has degree $\text{deg}(F) = m$.
\end{itemize}
We can now look at the relation in \autoref{eq:polynomial_representation}. If we look at the left-hand side, which has the following property by design $S\cdot Q(f_i) = 0 \;\forall\; f_i \in [1:m]$. This comes from the fact that the polynomial $S(f_i) = 0$ whenever $x_i = 0$, as it is the lagrange interpolation of $x$. Furthermore, the polynomial $Q(f_i)$ is zero whenever $f_i$ is a non-zero coordinate of $x$, which follows from the definition of $Q$.

For the right-hand side, the polynomial $F$ is the vanishing polynomial for the set ${f_1, \dots, f_m}$, so $F(f_i) = 0 \;\forall\; f_i \in [1:m]$. The polynomial $P$ is needed to match the degree of $S \cdot Q$. As the degree of $F$ is $m \leq \text{deg}(S\cdot Q) \leq m + w - 1$.

It is now apparent that if the prover can convince the verifier that they know of polynomials $P,Q$ such that $S\cdot Q = F \cdot P = 0$ at all points $f_i \in [1:m]$. The following must hold, either $S(f_i) = x_i = 0$ or $Q(f_i) = 0$. However, the polynomial $Q$ can be zero in at most $w$ points based on the degree, this means that S is non-zero in at most $w$ points, based on the construction, which in turn implies that $x$ has weight as most $w$.

With this, we can define the soundness of the MPC protocol as follows:
\begin{equation}
  wt(x) \leq w \Leftrightarrow \exists P,Q \text{  with  }\text{deg}(P)\leq w-1\text{  and  }\text{deg}(Q) = w\text{ s.t. \autoref{eq:polynomial_representation} holds}
  \label{eq:soundness}
\end{equation}

We can now share a witness $(x_a, Q, P)$. Now based on the relation \autoref{eq:standard_form_of_the_parity_check_matrix}, $x$ can be locally computed along with the polynomial $S$, this can then be used to run an equality test for the relation $S \cdot Q = P \cdot F$.

\subsubsection{False positive probability}\label{sub:equality_test}
The equality test from \autoref{eq:polynomial_representation} has a small probability of false positives, denoted $p$. To reduce this probability, the relation is evaluated at random points $\{r_k \in \mathbb{F}_q\}_{k\in[t]}$. By the Schwartz-Zippel lemma, the probability of a false positive is bounded by $p \leq \frac{t}{q}$, where $q$ is the size of the finite field $\mathbb{F}_q$. In short, this makes it unlikely that the relation will hold for all points $r_k$ if the relation is not sound according to \autoref{eq:soundness}. Furthermore, we can tweak the parameters $t$ and $q$ to reduce $p$. We will explain the choice of field size $q$ in \autoref{sec:gf256}.

\section{MPC-in-the-Head}\label{sec:mpcinth}

The SD-in-the-Head protocol construction is based on the \textit{Multi-Party Computation in the Head} (MPCitH) framework. In this section we will give an introduction to the framework and how it can be used to construct ZK proofs, which in turn can be combined with the Fiat-Shamir heuristic to create a signature scheme.

The MPC-in-the-Head (MPCitH) framework, introduced by~\cite{ishai2007zero}, builds upon these techniques to construct generic zero-knowledge protocols (ZK). A ZK protocol allows a \textit{prover} to convince a \textit{verifier} of the validity of a statement without revealing any information about the inputs to the statement. The framework provides a versatile method of constructing protocols that are quantum safe as its security relies on assumptions that are still believed to be quantum secure. Namely, commitment schemes and hash functions~\cite{feneuil2023threshold} which has no known quantum algorithms that break their security.

We will give insight into the basic construction suggested by Ishai et al \cite{ishai2007zero}.

\begin{definition}
  \label{def:mpcinth_basic}
  Given a semi-honest $\ell$-private MPC protocol $\Pi_f$ with perfect correctness, a relation $\mathcal{R}$ for some public statement $x$, a witness $w$. Let $w_i$ be an additive secret share of $[[w]]$ for the party $P_i$. Let $f$ be a $n$-party functionality $f(x, w_1, \dots, w_n) = \mathcal{R}(x, w)$, i.e. $f(x,w)$ accepts if $(x,w) \in \mathcal{R}$.
  \begin{enumerate}
    \item The prover builds a random sharing of $[[w]] = w_1, \dots, w_n$. Then
          \begin{itemize}
            \item Simulates the outputs of the MPC protocol $\Pi_f$ on the inputs $(x, w_1, \dots, w_n)$ and the randomness $r_1, \dots, r_n$.
            \item Prepares views $V_1, \dots, V_n$ of the parties in the protocol $\Pi_f$.\footnote{Remember that the views include the inputs, randomness and messages received by the parties.}
            \item Commits to each view $V_i$ using a secure commitment scheme, and sends ($\text{Commit}(V_1), \dots, \text{Commit}(V_n)$) to the verifier.
          \end{itemize}
    \item The verifier picks $\ell$ random distinct indices $i \in [n]$ and sends them to the prover.
    \item The prover opens the commitments into the views $V_i$ and sends the openings to the verifier.
    \item The verifier accepts if and only if:
          \begin{enumerate}
            \item\label{prop:mpcinth_commit} The views are valid according to the commitment scheme.
            \item\label{prop:mpcinth_consistent} The views are consistent according to the public input $x$.
            \item\label{prop:mpcinth_knowledge} The views output $1$ according to $\mathcal{R}$ meaning that $\mathcal{R}(x,w) = 1$.
          \end{enumerate}
  \end{enumerate}
\end{definition}

We see the following properties of the protocol:

\begin{lemma}[Completeness \cite{ishai2007zero}]
  \label{def:mpcinth_completeness}
  A boolean function is said to be complete if it depends on all inputs of the function. Given an honest prover, $\mathcal{R}(x,w) = 1$ and the correctness of $\Pi_f$, all outputs of $P_i$ are one and all views are consistent.
\end{lemma}

\begin{lemma}[Soundness \cite{ishai2007zero}]
  If the statement $x$ is false, i.e. $x \notin L$, then $\mathcal{R}(x,w) = 0$ for all $w$. By the correctness of $\Pi_f$, the output of all parties must be $0$. If the verifier accepts, then the prover must have created $\ell$ inconsistent views. This happens with probability at most $p = 1 / \binom{n}{\ell}$. The error probability can be reduced to $2^{-k}$ by repeating the protocol $O(kn^2)$ times.
\end{lemma}

\begin{lemma}[Zero-Knowledge \cite{ishai2007zero}]
  The verifier only sees $\ell$ views, and therefore from the definition of $\Pi_f$ learns nothing of the secret witness $w$.
\end{lemma}

The basic MPC-in-the-Head protocol provides a foundation for constructing ZK proofs for any NP relation and has been shown to produce relatively efficient ZK protocols~\cite{feneuil2023threshold,baum2020concretely,katz2018improved}. Variants of the protocol have demonstrated reductions in communication complexity, though at the cost of decreased computational efficiency on the prover's side, as these constructions require a substantial increase in the number of parties $N$~\cite{ishai2007zero, feneuil2023threshold}, which subsequently leads to heightened computational complexity.

Recent advancements, however, offer promising improvements on both fronts. First, the integration of \textit{MPC preprocessing}\cite{katz2018improved} enables the instantiation of an MPC-in-the-Head instance where communication complexity is independent of $N$, thereby facilitating increased security while maintaining low communication costs. Additionally, employing \textit{Linear Secret Sharing Schemes} (LSSS)\cite{feneuil2023threshold} in place of additive sharing significantly enhances computational efficiency, as the computations for both prover and verifier become bounded by the threshold $\ell$. The incorporation of these techniques ultimately paves the way for the development of the SD-in-the-Head protocol.

\subsection{Proving the SD relation using Beaver triples}\label{sec:verify}
In the following section we introduce the specific MPC protocol employed to verify the SD polynomial relation $P \cdot F = S \cdot Q$ (\autoref{sec:polynomial_representation}).

Consider the multiplication MPC protocol by \cite{baum2020concretely} which verifies the relation $z = x\cdot y$ for $x,y,z \in \mathbb{F}_q$.

\begin{definition}
  \label{def:beaver}
  Given an input triple $(x,y,z) \in \mathbb{F}$ random shared triple $([[a]], [[b]], [[c]]) \in \mathbb{F}$, it is possible to verify the correctness of the statement $z = x \cdot y$ without revealing any information on either of the input.
  \begin{enumerate}
    \item The parties generate a random $\epsilon \in \mathbb{F}$.
    \item The parties locally set $[[\alpha]] = \epsilon[[x]] + [[a]], [[\beta]] = [[y]] + [[b]]$.
    \item The parties run \texttt{open}$([[\alpha]])$ and \texttt{open}$([[\beta]])$ to obtain $\alpha$ and $\beta$.
    \item The parties locally set $[[v]] = \epsilon[[z]] - [[c]] + \alpha  \cdot [[b]] + \beta  \cdot [[a]] - \alpha  \cdot \beta$.
    \item The parties run \texttt{open}$([[v]])$ to obtain $v$ and accept iff $v = 0$.
  \end{enumerate}
\end{definition}

Observe that if both triples are correct multiplication triples (i.e., $z = xy$ and $c = ab$) then the parties will always accept since
\begin{align}
  v & = \epsilon \cdot z - c + \alpha \cdot b + \beta \cdot a - \alpha \cdot \beta                                            \\
    & = \epsilon \cdot xy - ab + (\epsilon \cdot x + a)b + (y + b)a - (\epsilon \cdot x + a)(y + b)                           \\
    & = \epsilon \cdot xy - ab + \epsilon \cdot xb + ab + ya + ba - \epsilon \cdot xy - \epsilon \cdot xb - ay - ab           \\
    & = (\epsilon \cdot xy - \epsilon \cdot xy) + (ab - ab) + (\epsilon \cdot xb - \epsilon \cdot xb) + (ya - ay) + (ba - ab) \\
    & = 0
\end{align}

\begin{lemma}
  If $([[a]], [[b]], [[c]])$ or $([[x]], [[y]], [[z]])$ is an incorrect multiplication triple then the parties output \texttt{Accept} in the sub-protocol above with probability $\frac{1}{|\mathbb{F}|}$.\footnote{A full proof of this can be found in \cite{baum2020concretely}.}
\end{lemma}

\subsection{MPC preprocessing model}

\todo{This part can quickly become tedious, so we need to make sure that we are not going too deep into the details.}

We need to read into \cite{katz2018improved} and how they use preprocessing to improve the efficiency of MPCitH from \cite{ishai2007zero}. Se page 3 in \cite{katz2018improved} for preprocessing phase.

\begin{quote}
  As a consequence of being able to rely on preprocessing, the
  space of possible protocols Î  we can use is greatly expanded. In
  particular, we find that we obtain much shorter proofs by using
  an n-party protocol (secure against semi-honest corruption of allbut-one of the parties) with n as high as 64. The ability to rely
  on preprocessing is critical here: the communication complexity
  of traditional MPC protocols (that do not rely on preprocessing)
  with security against all-but-one corruption is quadratic in the
  number of parties, but by relying on preprocessing we can obtain
  communication complexity independent of n.
  Further optimizations and specific parameter choices for the
  above proof are discussed in the remainder of the paper.
\end{quote}

\section{Linear Secret Sharing Schemes (LSSS)}\label{sec:lsss}

\textit{Secret sharing schemes} (SSS) are a type of cryptographic protocol that allows for the distribution of a secret amongst a group of participants. The secret can only be reconstructed when a sufficient number of shares are combined together. The threshold variant of the SD-in-the-Head protocol relies on a low-threshold linear secret sharing scheme (\textit{LSSS}). Threshold secret sharing schemes allow for the reconstruction of a secret from a subset of shares of length $\ell$, where $\ell$ is the threshold. The threshold allows for the SD-in-the-Head protocol to be more communication efficient, as the amount of shares needed to reconstruct the secret is low.

\begin{definition}
  \label{def:sss}
  \textit{$S$ is a $(\ell,n)$ threshold SSS if it satisfies the following properties}:

  \begin{itemize}
    \item \textbf{Share generation}: Given a secret $s$, the scheme generates $n$ shares $\texttt{share}(s) = [[s]] = [[s_1, s_2, \dots, s_n]]$.
    \item \textbf{Reconstruction}: Given a subset of shares $[[s']]$ of size $\ell$, the scheme can reconstruct the secret $s = \texttt{open}([[s']])$.
  \end{itemize}

\end{definition}

\textit{Linear secret charing schemes}, or $(+,+)$-homomorphic schemes, refers to secret sharing schemes that are linearly homomorphic over some field $\mathbb{F}$ (say the galois field \texttt{GF256} described in \autoref{sec:gf256}). This means that given shares $[[a]]$ and $[[b]]$ we have that
\begin{definition}
  A $(\ell,n)$ threshold SSS is $(+,+)$-homomorphic if for any two secrets $s_1$ and $s_2$ and their shares $[[s_1]]$ and $[[s_2]]$, the sum of the shares $[[s_1]] + [[s_2]] = [[s_{11} + s_{21}, s_{12} + s_{22}, \dots, s_{1n} + s_{2n}]]$ is equal to the share of the sum of the secrets $[[s_1 + s_2]]$ for the same subset of shares.
\end{definition}

\subsection{Shamir's Secret Sharing}\label{sec:shamir}

Shamir's Secret Sharing scheme \cite{shamir1979share}. Shamir's Secret Sharing is a method for distributing a secret amongst a group of participants, each of which is given a share of the secret. The secret can only be reconstructed when a sufficient number of shares are combined together.

For a secret $s$ and a given security threshold $\ell$, the share $[[s]]$ is generated by sampling a random polynomial of degree $t-1$ with $s$ as the free coefficient. Each participant is given a share of the polynomial evaluated at a point. The secret can be reconstructed by interpolating the polynomial from a sufficient number of shares.

The $(t,n)$ Shamir's polynomial based secret sharing scheme is $(+,+)$-homomorphic in which the addition of two polynomials secrets equals the Lagrange's interpolation of the sum-of-shares for the same subset of shares.

\todo{Proof of lemma and solution}

\section{Fiat-Shamir Heuristic}\label{sec:fiatshamir}
To transform any zero-knowledge protocol into a signature scheme, one can use the approach described in \cite{fiat1986prove}. Here, we outline the general concept.

Zero-knowledge protocols typically rely on the verifier to issue a challenge to the prover. This challenge serves as a source of randomness that the prover is not supposed to control. In the Fiat-Shamir framework, the original protocol is based on the problem of factoring integers -- a problem that is now vulnerable to quantum attacks, specifically Shor's algorithm. However, the fundamental method of converting a zero-knowledge protocol into a signature scheme remains unaffected.

To adapt a zero-knowledge protocol into a signature scheme, the prover eliminates the need for a verifier to provide randomness. Instead, the prover generates the randomness using a pseudo-random function. This function takes as input the message to be signed along with some random values generated by the prover. The output of this function serves as the challenge. With this, the prover can compute the required values to prove authenticity without external interaction.

In the context of the SD-in-the-Head protocol, the prover uses the pseudo-randomly generated challenge to locally compute the witness values required for the proof. These witness values are derived through the MPC protocol, where the prover simulates the necessary computations internally, generating and processing the shares "in their head." This eliminates the need for the verifier's active participation in the MPC protocol, streamlining the signature generation process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Specification}\label{ch:spec}

\todo{more detailed description of the algorithm
  e.g. how we sampled I[e] witness challenge
  table of spec params (with our code naming and different categories)}

For our implementation, we selected the \textit{threshold} variant of the SD-in-the-Head protocol~\cite{aguilarsyndrome11,feneuil2023threshold}, as this variant offers the most significant performance improvements compared to the initial protocol~\cite{feneuil2022syndrome} and the \textit{hypercube} variant~\cite{aguilarsyndrome11,aguilar2023return}, albeit at the cost of a slightly larger signature size.

In this chapter, we provide a specification of the initial SD-in-the-Head protocol, along with the details specific to the threshold variant. We include pseudo-code for the major subroutines and algorithms, as well as a comprehensive description of the parameters used in the protocol. Finally, we present an overview of the associated security guarantees.


\section{SD-in-the-Head Protocol}
The original SD-in-the-Head protocol is based on the computational hardness of the Syndrome Decoding (SD) problem for random linear codes over a finite field. As a reminder, problem is expressed as follows (see \autoref{sec:syndrome})
\begin{quote}
  For a parity check matrix $H \in \mathbb{F}_q^{(m-k)\times m}$ in standard form and a syndrome $y$. The challenge is to find a vector $x$ s.t. $Hx = y$ and $wt(x) \leq w$.
  \begin{align*}
    y = Hx = H'x_a + x_b \text{ \ \ \ where \ \ \ } H' \in \mathbb{F}_q^{(m-k)\times k}
  \end{align*}
  for $x = (x_a | x_b)$. We have the polynomial representation of the problem (see \autoref{sec:polynomial_representation})
  \begin{align*}
    S\cdot Q = P\cdot F \text{ \ \ \ where \ \ \ } S, Q, P, F \in \mathbb{F}_q[X]
  \end{align*}
\end{quote}
\noindent Testing that the relation holds has a small probability of a \textit{false positive}. We denote this as $p$. The protocol takes several steps to reduce $p$. First, the relation is evaluated at random points $\{r_k \in \mathbb{F}_q\}_{k\in[t]}$. Second, the field of the random points is extended from $\mathbb{F}_q$ to $\mathbb{F}_{q^4}$.

\begin{lemma}[Schwartz-Zippel]\label{lem:schwartz}
  For a non-zero polynomial $P \in \mathbb{S}[X]$ of degree $d \geq 0$. Let $\mathbb{R}$ be a finite subset of $\mathbb{S}$ and set of random points $[r_1, \dots, r_n] \in \mathbb{R}$, the probability that $\textnormal{Pr}[P(r_1, \dots, r_n) = 0] \leq d/|\mathbb{R}|$.
\end{lemma}

security

The SD-in-the-Head protocol is based on the \textit{Multi-Party Computation in the Head} (MPCitH) framework (see \autoref{sec:mpcinth}). \todo{Explain why we then use MPC and the linearity of polynomial evaluation with sacrificing random beaver triples}

\section{Field implementation}

\subsection{Field extension}

First extend $F_q$ to $F_{q^4} = F_q[Z] / (Z^2 + Z + 32(X))$. This is done by representing elements in the field as polynomials of degree at most 1 with coefficients in $F_q$. \todo{Define addition and multiplication}

\subsubsection{Polynomial evaluation}
\todo{Should this be moved to the specification section?}
For polynomial evaluation in the multiparty computation algorithm, seen in \autoref{sec:mpc}, we need to evaluate a polynomial $P(x)$ at a point $r \in F_q^\eta$. This is done by evaluating the polynomial at each coefficient and summing the results. The polynomial $P(x)$ is defined as
\begin{align}
  \textstyle\bigcup_{|Q|}(F_q)^{|Q|} \times F_q^\eta & \rightarrow F_q^\eta                 \nonumber                                  \\
  Q(r)                                               & = \textstyle\sum_{i=0}^{|Q|} Q_i \cdot r^{i-1} &  & Q_i \in F_q, r \in F_q^\eta
  \label{eq:mpcpoly}
\end{align}

\todo{See gf256\_ext.rs}

\section{Merkle Tree Commitment}
\todo{Can we use Haraka v2}

\section{Hashing and XOF}

\todo{Write about Shake, Keccak} and how we initiate the hash function and XOF

\section{MPC computation}

The computation is based on HVZKAoK Protocol using imperfect preprocessing and sacrificing. Section 3.3 \cite{baum2020concretely}.

A toy example of the computation protocol computations, can be seen in~\autoref{fig:mpc}. We have the two computation methods. Here for 1 split and 1 evaluation point. This means that we have only value for each challenge and beaver triple. Note that any arithmetic is run in GF256, so addition and subtraction are both \texttt{XOR} and multiplication is modulus $x^8 + x^4 + x^3 + x + 1$. Furthermore, for negation we have that $-a = a$.

\begin{figure}
  \makebox[\textwidth]{
    \fbox{\begin{minipage}[t]{.45\textwidth}
        \noindent \texttt{PartyComputation}
        \begin{flalign*}
           & \textit{Input: }                                                               \\
           & (s_a, Q', P, a, b, c), (\overline{\alpha}, \overline{\beta}), (H', y)          \\
           & (\epsilon, r), \texttt{with\_offset}                                           \\
           & \textit{Output: }                                                              \\
           & (\alpha, \beta, v)                                                             \\\\
           & Q = Q'_1\text{ if \texttt{with\_offset} else }Q'_0                             \\
           & S = (s_a | y + H's_a) \text{ if \texttt{with\_offset} else } (s_a | H's_a)     \\
           & v = -c                                                                         \\
           & \alpha = \epsilon \cdot Q(r) + a                                               \\
           & \beta = S(r) + b                                                               \\
           & v \mathrel{{+}{=}} \epsilon \cdot F(r) \cdot P(r)                              \\
           & v \mathrel{{+}{=}} \overline{\alpha} \cdot b + \overline{\beta} \cdot a        \\
           & v \mathrel{{+}{=}} - \alpha \cdot \beta \text{ \ \ \ if \texttt{with\_offset}}
        \end{flalign*}
      \end{minipage}}
    \hfill
    \noindent
    \fbox{\begin{minipage}[t]{.45\textwidth}
        \noindent \texttt{InverseComputation}
        \begin{flalign*}
           & \textit{Input: }                                                                 \\
           & (s_a, Q', P), (\alpha, \beta, v), (\overline{\alpha}, \overline{\beta}), (H', y) \\
           & (\epsilon, r), \texttt{with\_offset}                                             \\
           & \textit{Output: }                                                                \\
           & (a, b, c)                                                                        \\\\
           & Q = Q_1\text{ if \texttt{with\_offset} else }Q_0                                 \\
           & S = (s_a | y + H's_a) \text{ if \texttt{with\_offset} else } (s_a | H's_a)       \\
           & c = -v                                                                           \\
           & a = \alpha - \epsilon \cdot Q(r)                                                 \\
           & b = \beta - S(r)                                                                 \\
           & c \mathrel{{+}{=}} \epsilon \cdot F(r) \cdot P(r)                                \\
           & c \mathrel{{+}{=}} \overline{\alpha} \cdot b + \overline{\beta} \cdot a          \\
           & c \mathrel{{+}{=}} - \alpha \cdot \beta \text{ \ \ \ if \texttt{with\_offset}}
        \end{flalign*}
      \end{minipage}}}
  \caption{Simplified version of the MPC party computation and inverse computation. $Q_0$ means that $Q$ is completed with a $0$ for leading coefficient. Furthermore, $F$ is precomputed. Note that all arithmetic is done in $\mathbb{F}_q = GF256$. All elements are in $\mathbb{F}_q^\eta$ except for the coefficients of $Q$, $S$ and $P$ which are in $\mathbb{F}_q$.}
  \label{fig:mpc}
\end{figure}
\bigskip

Note that the

If we first instantiate an input $i$ and one random input $i^*$ (like the \texttt{input\_coef}).Then the input share is generated by adding the two. Similar, but simpler, to the input share generation of Algorithm 12, line 13 of the specification.
\begin{align*}
  i             & = (s_a, Q, P, a,b,c)                                             \\
  i^*           & = ({s_a}^*, Q^*, P^*, a^*,b^*,c^*)                               \\
  [i] = i + i^* & = ({s_a} + {s_a}^*, Q + Q^*, P + P^*, a + a^*, b + b^*, c + c^*) \\
                & = ([s_a], [Q], [P], [a], [b], [c])                               \\
  \texttt{chal} & = (\epsilon, r)                                                  \\
  \texttt{pk}   & = (H', y)
\end{align*}
We also compute the plain broadcast share of the input as per Algorithm 12, line 18. Note that $\overline{v}$ is computed to zero and therefore removed from the computation in the implementation.
\begin{align*}
  (\overline{\alpha},\ \overline{\beta})         & =
  \texttt{PartyComputation}(i,\ (\overline{\alpha}, \overline{\beta}),\ \texttt{chal},\ \texttt{pk},\ \texttt{true})                                                                                   \\\\
  \overline{\alpha}                         =    & \epsilon \cdot Q_1(r) + a                                                                                                                           \\
  \overline{\beta}                          =    & S_y(r)  + b                                                                                                                                         \\
  \overline{v}                            =      & -c + \epsilon \cdot F(r) \cdot P(r) + \overline{\alpha} \cdot b + \overline{\beta} \cdot a  - \overline{\alpha} \cdot \overline{\beta}              \\
  \overline{v}                            =      & -c + \epsilon \cdot F(r) \cdot P(r) + (\epsilon \cdot Q_1(r) + a) \cdot b + (S_y(r)  + b) \cdot a  - (\epsilon \cdot Q_1(r) + a) \cdot (S_y(r) + b) \\
  \overline{v}                            =      & -c + \epsilon \cdot F(r) \cdot P(r)                                                                                                                 \\
                                                 & + \epsilon \cdot Q_1(r) \cdot b + c + S_y(r) \cdot a + c                                                                                            \\
                                                 & - \epsilon \cdot Q_1(r) \cdot S_y(r) - \epsilon \cdot Q_1(r) \cdot b - a \cdot S_y(r) - c                                                           \\
  \overline{v}                                 = & \ 0
\end{align*}
We then compute a broadcast share from the randomness and the broadcast, as per Algorithm 12, line 21.
\begin{align*}
  (\alpha^*, \beta^*, v^*) & = \texttt{PartyComputation}(i^*, (\overline{\alpha},
  \overline{\beta}), \texttt{chal}, \texttt{pk}, \texttt{false})                  \\\\
  \alpha^*                 & = \epsilon \cdot {Q^*}_0(r) + a^*                    \\
  \beta^*                  & =  {S^*}_0(r) + b^*
\end{align*}
This broadcast share is sent to the verifier along with the truncated input share (removing the beaver triples). The verifier then needs to recompute the input share beaver triples using the \texttt{InverseComputation} function. First we add the input share to the broadcast share as per Algorithm 13, line 8.
\begin{align*}
  (\alpha', \beta', v') & = (\alpha^*, \beta^*, v^*) + (\overline{\alpha},\ \overline{\beta}, 0)
  = (\alpha^* + \overline{\alpha}, \beta^* + \overline{\beta}, v^* + 0)                          \\\\
  \alpha'               & = \epsilon \cdot {Q^*}_0(r) + a^* + \overline{\alpha}                  \\
                        & = \epsilon \cdot {Q^*}_0(r) + a^* + \epsilon \cdot Q_1(r) + a          \\
  \beta'                & = {S^*}_0(r) + b^* + \overline{\beta}                                  \\
                        & = {S^*}_0(r) + b^* + S_y(r) + b                                        \\
\end{align*}
Next, the verifier computes the inverse of the broadcast share to recompute $([a], [b], [c])$ using the \texttt{InverseComputation} function. This is done as per Algorithm 13, line 10.
\begin{align*}
  (a', b', c') & = \texttt{InverseComputation}([i], (\alpha', \beta', v'),
  (\overline{\alpha}, \overline{\beta}), \texttt{chal}, \texttt{pk}, \texttt{true})                                                  \\\\
  a'           & = \alpha' - \epsilon \cdot {[Q]}_1(r)                                                                               \\
               & = \epsilon \cdot {Q^*}_0(r) + a^* + \epsilon \cdot Q_1(r) + a - \epsilon \cdot {[Q]}_1(r)                           \\
               & = \epsilon \cdot {Q^*}_0(r) - \epsilon \cdot {[Q]}_1(r) + \epsilon \cdot Q_1(r) + [a]                               \\
               & = \epsilon \cdot ({Q^*}_0(r) - {[Q]}_1(r) +  Q_1(r)) + [a]                                                          \\
               & = \epsilon \cdot (Q^*_0(r) +  Q^*_0(r)) + [a]                                             &  & \autoref{eq:mpcpoly} \\
               & = [a]                                                                                     &  & \autoref{eq:mpcpoly} \\
  b'           & = \beta' - {[S]}_y(r)                                                                                               \\
               & = {S^*}_0(r) + b^* + S_y(r) + b - {[S]}_y(r)                                                                        \\
               & = {S^*}_0(r) - {[S]}_y(r) + S_y(r)  + [b]                                                                           \\
               & = {S^*}_0(r) - S^*_0(r)  + [b]                                                            &  & \autoref{eq:mpcpoly} \\
               & = [b]                                                                                     &  & \autoref{eq:mpcpoly} \\
\end{align*}
\todo{Want to explain the above in a more detailed manner? Specifically ${Q^*}_0(r) - {[Q]}_1(r) = Q_1(r)$}

\section{Security}

The security analysis of the SD-in-the-Head signature scheme is based on the proposed standardization requirements from the 2022 NIST call for proposals of non-lattice based signature schemes \cite{nistcall}. Therefore, as a preliminary, we will describe the reasoning and requirements of the NIST standardization process.

With the development of new quantum algorithms and unknowns in the capacities of the future quantum computers, there remain large uncertainties in estimating the security of the algorithms. To combat these uncertainties, NIST proposed for the 2022 stadardization effort, to define the security of submissions in a range of five categories. Each, with an easy-to-analyze cryptographic primitive providing the lower bound for a variety of metrics deemed relevant to practical security. The SD-in-the-Head specification provides security parameters adhering to categories one, three and five.

\begin{definition}
  \label{def:nistsec}
  Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for key search on a block cipher with a 128-bit (e.g. AES-128), 192-bit (e.g. AES-192) and 256-bit (e.g. AES-256) for categories one, three and five respectively.
\end{definition}

In terms of quantum security, the complexity and capability of quantum algorithms are measured in terms of quantum circuit size, i.e. the number of quantum gates in the quantum circuit. In order to estimate the quantum security of the signature protocols, circuit size can be compared to the resources required to break the security of \autoref{def:nistsec}. Therefore,
according to the proposal by NIST, the SD-in-the-Head specification provides security metrics in terms of quantum circuit depth to optimal key recovery for AES-128, AES-192 and AES-256 for categories one, three and five respectively. These are estimated to be $2^{143}$, $2^{207}$ and $2^{272}$ classical gates~\cite{nistcall}.

\subsection{Security Definition}

The SD-in-the-Head signature scheme upholds the \textbf{E}xistential \textbf{U}n\textbf{f}orgeability under \textbf{C}hosen \textbf{M}essage \textbf{A}ttack (EUF-CMA) security property for digital signature schemes as this is the type of attack that NIST will evaluate signature proposals~\cite{nistcall,aguilarsyndrome11}. EUF-CMA works as the following game:

\begin{enumerate}
  \item The challenger generates a key pair $(pk, sk)$ and sends $pk$ to the adversary.
  \item The adversary is then allowed to query a signing oracle for signatures of chosen messages $(m_1, ..., m_r)$ and receives valid signatures $(\sigma_1, ..., \sigma_r)$. For the NIST evaluation it is assumed that the adversary can query the signing oracle for up to $2^{64}$ chosen messages according to the adversary's running time. However, there is no requirement on the timing of the queries.
  \item The adversary then outputs a pair $(m^*, \sigma^*)$. The adversary wins if the following holds
        \begin{enumerate}
          \item $m^*$ has not been queried to the signing oracle.
          \item The pair $(m^*, \sigma^*)$ is a valid signature for $m^*$ under $pk$.
        \end{enumerate}
\end{enumerate}

\begin{table}[h]
  \label{tab:secparam}
  \centering
  \def\arraystretch{1.5}%  1 is the default, change whatever you need
  \begin{tabular}{cccccccccccccc}
    \specialrule{.1em}{.05em}{.05em}
    \multicolumn{2}{c}{\textbf{NIST security}} &      & \multicolumn{5}{c}{\textbf{SD parameters}} &     & \multicolumn{5}{c}{\textbf{MPCitH Parameters}}                                                             \\ \cline{1-2} \cline{4-8} \cline{10-14}
    Category                                   & Bits &                                            & $q$ & $m$                                            & $k$ & $w$ & $d$ &  & N   & $\ell$ & $\tau$ & $\eta$ & $t$ \\ \hline
    \textbf{I}                                 & 143  & \textit{}                                  & 256 & 242                                            & 126 & 87  & 1   &  & $q$ & 3      & 17     & 4      & 7   \\
    \textbf{III}                               & 207  &                                            & 256 & 376                                            & 220 & 114 & 2   &  & $q$ & 3      & 26     & 4      & 10  \\
    \textbf{V}                                 & 272  &                                            & 256 & 494                                            & 282 & 156 & 2   &  & $q$ & 3      & 34     & 4      & 13  \\ \specialrule{.1em}{.05em}{.05em}
  \end{tabular}
  \caption{Security parameters for the SDitH protocol for categories one, three and five~\cite{aguilarsyndrome11}.}
\end{table}

\begin{table}[h]
  \label{tab:hashparam}
  \centering
  \def\arraystretch{1.5}%  1 is the default, change whatever you need
  \begin{tabular}{clll}
    \specialrule{.1em}{.05em}{.05em}
         & \multicolumn{1}{c}{\textbf{I}} & \multicolumn{1}{c}{\textbf{III}} & \multicolumn{1}{c}{\textbf{V}} \\ \cline{2-4}
    Hash & SHA3-256                       & SHA3-384                         & SHA3-512                       \\
    XOF  & SHAKE-128                      & SHAKE-256                        & SHAKE-256                      \\ \specialrule{.1em}{.05em}{.05em}
  \end{tabular}
  \caption{Hash and XOF functions used in the SDitH protocol for categories one, three and five~\cite{aguilarsyndrome11}.}
\end{table}

\subsection{Assumptions}\label{sec:assumptions}
The SD-in-the-Head protocol is secure under the following assumptions:

Syndrome Decoding instances cannot be solved in complexity lower than $2^\kappa$ corresponding to the complexity of breaking AES by exhaustive search (see \autoref{def:nistsec}) in terms of quantum circuit size. For this, $\kappa$ is defined as $143$, $207$ and $272$ for the categories \cite{nistcall}. Furthermore, the XOF primitive used is secure with 128-bit, 192-bit, 256-bit security levels for each of the categories respectively. Finally, the Hash function used \textit{behaves as random oracle}. Specifically, security holds in Random Oracle Model (ROM) and Quantum Random Oracle Model (QROM).

\subsection{Security of the Syndrome Detection Problem}\label{sec:sdsec}

Recall the definition of the syndrome detection problem from \autoref{def:syndrome}. The hardness of the SD problem is well established and the coset variant used in the SD-in-the-Head signature scheme, has been shown to be \textit{NP-complete}~\cite{berlekamp1978inherent,aguilarsyndrome11}. Furthermore, a brute force attack of guessing $x$ would require finding a unique correct solution in $\binom{m}{w} q^w$ which is infeasible for the parameters outlined in the specification. As an example, for category one, the number of possible solutions are $\approx 7.7 \times 10^{276}$. There exists more sophisticated algorithms for solving the SD problem, such as the Generalized Birthday Algorithms (GBA) and Information Set Decoding (ISD)~\cite{prange1962use}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}\label{ch:impl}

We set out to manually develop as many of the subroutines and primitives ourselves, including GF256 and merkle trees. However, hashing and XOF where made using pre-existing Rust libraries.

Tooling and language feaures (rust, criterion, rayon)
code sections
code re-usability with traits for categories.

const generics vs inline mutability (benchmarking?, nightly?)
compiling constants for categories

can we use other hashes that still provide the same security assumption from \autoref{sec:assumptions} as post-quantum security (Xoodyak, KangarooTwelve, Haraka v2)


\section{Rust}
\cite{nistsaferlanguages}

\section{Subroutines}
\label{sub:subroutines}
Here we implemented hashing as the first part, along with commitment. (cd1acdb0b6e9)

\section{Merkle Tree impl}
For the merkle tree implementation, we have followed the specification. But added a function that calculates bottom up the revealed nodes based on the view opening challenges. This is used to know the length of the authentication path that is added to the signature and is needed when parsing the signature for verification. The algorithm uses the queue structure which is also mentioned in the spec, where the tree structure is a 1D array with the last elements being the leaf. First it checks for empty input. Then we initiate an empty list to hold the revealed nodes. Now in order to traverse the tree we will get the $heightIndex$ and $lastIndex$, which are the current layer of the tree, and the index of the 1D array of where the current layer stops. Lastly we need a queue to hold the indexes. Now in order to find the revealed nodes it starts by adding the indexes of the selected leaves to the queue. Then we go through the queue until the next index is $1$ which indicates that we are at the root. In the while loop we first get the current index, and then check if we have gone up a layer in the tree, and if the current node is a left child. If the current node is a left child and it is the last index in the layer, then we can just skip to the next node because we know it has no sibling. If it is not the last node we get the next node as a candidate, if the queue is not empty. If the current node is still the left child, and the next node in the queue is the other child of the same parent we do not need to add this as a revealed node as it is part of the known values. But if the next node is not the right child of the current node we add it as a revealed node. If the current node is not the left child we add the left child to the revealed nodes. And then we add the parent index to the queue. When we exit the while loop we have traversed the tree bottom up adding for each layer the neighbours of the selected leaves. It works as follows:

\begin{algorithm}
\caption{Get Revealed Nodes for Selected Leaves in Merkle Tree}
\begin{algorithmic}[1]
\Require $selectedLeaves$: List of selected leaf indices
\Ensure List of revealed node indices

\Function{getRevealedNodes}{$selectedLeaves$}
    \If{$selectedLeaves$ is empty}
        \State \Return empty list
    \EndIf

    \State $revealedNodes \gets \text{empty list}$
    \State $(heightIndex,\; lastIndex) \gets (1 \ll treeHeight,\; treeNodes - 1)$
    \State $queue \gets \text{empty queue}$

    
    \ForAll{$leaf \in selectedLeaves$} \Comment{Add commitments to the queue}
        \State $val \gets (1 \ll treeHeight) + leaf$
        \If{$\text{queue.add}(val)$ fails}
            \State \textbf{panic} ``Could not add element to queue''
        \EndIf
    \EndFor

    \While{$queue.peek() \neq 1$} \Comment{Do until we reach the root}
        \State $index \gets queue.remove()$
        
        \If{$index < heightIndex$}
            \State $heightIndex \gets heightIndex \gg 1$
            \State $lastIndex \gets lastIndex \gg 1$
        \EndIf

        \State $isLeftChild \gets (index \; \% \; 2 == 0)$

        \If{$isLeftChild \land index == lastIndex$}
            \Comment{Node has no sibling}
            \State \textbf{continue}
        \Else
            \State $candidateIndex \gets 0$
            \If{\textbf{not} $queue.isEmpty()$}
                \State $candidateIndex \gets queue.peek()$
            \EndIf

            \If{$isLeftChild \land (candidateIndex == index + 1)$}
                \State $queue.remove()$
            \ElsIf{$isLeftChild$}
                \State $revealedNodes.append(index + 1)$
            \Else
                \State $revealedNodes.append(index - 1)$
            \EndIf
        \EndIf

        \State $parentIndex \gets index \gg 1$
        \If{$queue.add(parentIndex)$ fails}
            \State \textbf{panic} ``Could not add element to queue''
        \EndIf
    \EndWhile

    \State \Return $revealedNodes$
\EndFunction

\end{algorithmic}
\end{algorithm}

\subsection{Addition of parameters}
Then we introduced the parameters from the spec. f4739166d684

\subsection{Hashing, XOF, and Commitments}

\subsection{Galois field arithmetic}
\label{}
Implemented the galois field. 94c3d9889709

\section{Key generation}

\section{MPC}\label{sub:mpc_algo}
We started on implementing the mpc computation algo along with the inverse

\section{Comparison with spec impl}\label{sub:comparison_with_spec_impl}
In order to verify and debug our own code we tried to setup our implementation to fit with the specification implementation in C, meaning we had to run their code in order to get the intermediate data in order to compare the internal states.
Here we found a bug in the way the view-opening challenges was calculated, they had forgotten to finalize the Shake before squeezing, which was a problem because in the tiny keccak rust library this was always done when initializing a hash or xof.
Furthermore we found that in order to generate the same output from the xof we needed to first rotate the permutation once by supplying an empty vector before actually using the function.
We also found that the merkle tree was not using the salt.

\section{Benchmarking}\label{sub:benchmarking} % (fold)
We setup Criterion in order to do initial benchmarking

% subsection Benchmarking (end)

\section{Categories}\label{sub:categories} % (fold)

We tried reworking to use dynamically sized vectors instead of compile time sized arrays. Big rework and refactor. Benching showed slower running times. Instead we went with compile time script to set constants.

% subsection Array structure (end)

\section{Optimizations}
In order to find optimizations we used the samply CPU profiler \cite{Stange2024mstange}, to locate long running functions and loops. This was done by building the binary with cargo and supplying it to the samply CPU profiler. The profiler then generates a call stack and a flamegraph from the binary, this shows how many cycles of the cpu were used in different sections of the code.
\todo{Maybe a picture of how the base program looked in the call stack}

\subsection{Feature flags}\label{sub:feature_flags} % (fold)
To maintain a baseline implementation while enabling flexibility to switch between different NIST categories, we utilized Rust's feature flags. These flags support conditional compilation, allowing multiple implementations of the same function to coexist in the codebase. Only the specific implementation required is compiled, based on the flag provided to the compiler. This approach made it possible to test individual optimizations in isolation, rather than applying all optimizations simultaneously.

% subsection Feature flags (end)

\subsection{Parallelisation}\label{sub:rayon} % (fold)

% subsection Rayon (end)

\subsection{SIMD}\label{sub:simd} % (fold)

% subsection SIMD (end)

\subsection{Hash functions}
Blake3, Haraka v2, KangarooTwelve, Xoodyak

\section{Kat for NIST}\label{sub:kat_for_nist} % (fold)

% subsection Kat for NIST (end)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Benchmarks}\label{ch:bench}
diaries of benchmarks.
discussion of results

\todo{old benchmarks \url{https://asecuritysite.com/openssl/openssl3_b2}}

Test on both mac and linux.
nightly vs stable rust
different hashes
benching at different tags
parallelisation, test for amount of cores, 2, 4, 8, 16
no turbo boost (max 2.6 GHz)
cycles per bytes
compare ours to the optimised reference

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}\label{ch:conclusion}

wrap up and pose future work
what should people continue with
point to round 2 NIST
work in context of timeline

\todo{conclude on the problem statement from the introduction}

future work: Verkle trees optimisation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\appendix
\chapter{The Technical Details}

\todo{\dots}

\end{document}
