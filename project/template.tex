\documentclass[twoside,11pt]{report}

\usepackage[latin1]{inputenc}
\usepackage[american]{babel}
\usepackage{a4}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[outputdir=out]{minted}
\setminted[rust]{breaklines,autogobble}
\usepackage[]{amsmath}
\usepackage{epsfig}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[useregional]{datetime2}
\DTMlangsetup[en-US]{showdayofmonth=false}
\usepackage{lipsum}
\usepackage{ctable} % for \specialrule command
\usepackage{enumitem}
\usepackage{stmaryrd}
\usepackage{wasysym}
\usepackage{dirtree}

\usepackage{listings}
\lstdefinestyle{tree}{
  literate=
  {├}{{\smash{\raisebox{-1ex}{\rule{1pt}{\baselineskip}}}\raisebox{0.5ex}{\rule{1ex}{1pt}}}}1 
  {─}{{\raisebox{0.5ex}{\rule{1.5ex}{1pt}}}}1 
  { }{~}1
  {└}{{\smash{\raisebox{0.5ex}{\rule{1pt}{\dimexpr\baselineskip-1.5ex}}}\raisebox{0.5ex}{\rule{1ex}{1pt}}}}1 
  {│}{{\smash{\raisebox{-1ex}{\rule{1pt}{\baselineskip}}}\raisebox{0.5ex}{\rule{1ex}{0pt}}}}1 
}
\setlength{\parindent}{10pt} % indentation

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newcommand{\definitionautorefname}{Definition}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]

\addto\extrasamerican{%
  \def\chapterautorefname{Chapter}%
  \def\sectionautorefname{Section}%
  \def\subsectionautorefname{Section}%
  \def\lemmaautorefname{Lemma}%
  \def\figureautorefname{Figure}%
  \def\algorithmautorefname{Algorithm}%
}

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother



\newtheoremstyle{lemma}% <name>
{3pt}% <Space above>
{3pt}% <Space below>
{\normalfont}% <Body font>
{}% <Indent amount>
{\upshape}% <Theorem head font>
{:}% <Punctuation after theorem head>
{.5em}% <Space after theorem headi>
{}% <Theorem head spec (can be left empty, meaning `normal')>

\renewcommand*\sfdefault{lmss}
\renewcommand*\ttdefault{txtt}

\newcommand{\todo}[1]{{\color[rgb]{.5,0,0}\textbf{$\blacktriangleright$#1$\blacktriangleleft$}}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\begin{document}
\newcommand{\rust}[1]{{\mintinline{rust}{#1}}}\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{empty}
\pagenumbering{roman}
\vspace*{\fill}\noindent{\rule{\linewidth}{1mm}\\[4ex]
{\Huge\sf SD-in-the-Head rust implementation and optimization}\\[2ex]
{\huge\sf Hugh Benjamin Zachariae, 201508592 \\ Magnus Jensen,
201708626}\\[2ex]
\noindent\rule{\linewidth}{1mm}\\[4ex]
\noindent{\Large\sf Master's Thesis, Computer Science\\[1ex]
  \today \\[1ex] Advisor: Diego F. Aranha\\[15ex]}\\[\fill]}
\epsfig{file=images/logo.eps}\clearpage

% A math share macro
\newcommand{\sh}[1] {\ensuremath{\llbracket #1 \rrbracket}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{plain}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

\todo{in English\dots}

\chapter*{Resum\'e}
\addcontentsline{toc}{chapter}{Resum\'e}

\todo{in Danish\dots}

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

\todo{\dots}

\vspace{2ex}
\begin{flushright}
  \emph{Hugh Benjamin Zachariae and Magnus Jensen}\\
  \emph{Aarhus, \today.}
\end{flushright}

\tableofcontents
\cleardoublepage
\pagenumbering{arabic}
\setcounter{secnumdepth}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}\label{ch:intro}

few pages. Introduce what we have done and how the paper is structured

\section{Post-Quantum Cryptography}\label{sec:quantum}

Digital signatures are essential for ensuring a secure internet. As our collective reliance on the internet grows, they play a critical role in identifying and verifying the authenticity of the sources we interact with over the network. Digital signatures ensure that software or web pages come from trusted sources and have not been tampered with, forming a cornerstone of IT security. They also enable the verification of the signer's identity and help detect unauthorized changes to data. Additionally, digital signatures support non-repudiation, meaning they provide proof to a third party that a signature was genuinely created by the signer.

The most widely used digital signature schemes rely on public-key cryptography. Each signer has a public and a private key. The private key allows the signer to sign messages, while the public key is used to verify the signature. The security of a signature is grounded in the computational complexity of solving specific mathematical problems. Standardization efforts in the early 2000s~\cite{pub2000digital} led to the adoption of algorithms such as RSA, DSA, and ECDSA, which leverage intractability of factoring large integers and solving discrete logarithms to ensure robust protection.

\subsection{NIST call to action}

In 1994, the discovery of Shor's algorithm~\cite{shor1997} introduced a polynomial-time method for solving these problems using quantum computing. At that time, quantum computers were theoretical constructs. However, in 2001, Chuang et al.~\cite{vandersypen2001experimental,buchmann2004post} successfully implemented Shor's algorithm on a 7-qubit quantum computer. Two decades later, significant advancements in quantum technology, have brought us closer to the point where quantum computers could be a reality.
\todo{Add some examples of the significant advancements in quantum technology.}

A necessary pre-requisite for a post-quantum secure signature scheme is the existence of a problem that is intractable for quantum computers. Several candidates from the family of NP-hard problems within complexity theory have been proposed. These are problems that can not be solved in polynomial-time by any classical or quantum computer.

In 2016, the National Institute of Standards and Technology (NIST) initiated a call for quantum-resistant public-key cryptosystems~\cite{nistcall}. This effort led to the standardization of three signature schemes: Dilithium~\cite{ducas2018crystals}, Falcon~\cite{fouque2018falcon}, and SPHINCS+~\cite{bernstein2019sphincs+}. Dilithium and Falcon are lattice-based schemes that rely on the hardness of structured lattice problems. They offer efficient key generation, signing, and verification, making them practical for many applications. However, their shared dependence on the same mathematical assumption poses a risk if this assumption is broken.

In contrast, SPHINCS+ is a hash-based signature scheme, relying solely on the security of cryptographic hash functions rather than lattice-based problems. This makes it an important alternative, providing diversity in case lattice assumptions are compromised. Despite its robustness, SPHINCS+ is significantly slower than Dilithium and Falcon, with high computational costs and larger signature sizes, which limits its suitability for real-world applications.

To mitigate the risks of relying heavily on lattice-based schemes, NIST issued a second call in 2022 for quantum-resistant signature schemes based on alternative assumptions, ensuring a more diversified and resilient set of standards for the future.


\subsection{Code-based problems}

With the introduction of public-key cryptosystems by Diffie and Hellman in 1976, we saw the introduction to schemes like RSA, that is based on factoring of large integers and ElGamal which is based on the discrete logarithm problem. While less known, there exists schemes based on coding theory -- its own discipline within information theory which predates the introduction of public-key cryptography. In 1978, McEliece introduced a PK scheme based on coding theory~\cite{mceliece1978public} -- only two years later -- based on the hardness of decoding random linear codes~\cite{berlekamp1978inherent}.

While the scheme never saw widespread adoption due to its large key sizes, the interest in code-based crypto-schemes has seen a resurgence as it is not vulnerable to known quantum attacks like Shor's algorithm. Recent developments in code-based cryptosystems have shown that the performance needed for code based cryptographic schemes practically is within reach. Introductions of linear based codes on finite fields and MPCitH (Multi-Party Computation in the Head) threshold schemes~\cite{baum2020concretely} show promising results. With the SD-in-the-Head scheme~\cite{aguilarsyndrome11,feneuil2023threshold} presented in this report, such techniques are combined to define an efficient post-quantum secure signature scheme.

\section{Our contributions}

\todo{describe how we progressed through the project in an interesting way. What did we set out to do, what were the hurdles.}

In this report, we present an implementation of the SD-in-the-Head protocol in Rust. While the NIST standardization process initially prioritizes selecting the appropriate protocols, the ultimate goal is to produce a well-documented specification that enables implementors to safely and accurately reproduce the chosen protocols.

To this end, it is crucial for both NIST and the authors to provide an in-depth specification accompanied by clear, well-documented examples to facilitate a smooth transition to the new standard.

Our objectives include delivering a high-performance, thoroughly documented implementation, complemented by extensive test coverage to ensure correctness and security. Additionally, we aim to identify and address potential pitfalls and challenges encountered during the implementation process in order to provide this feedback to the authors. Furthermore, we will attempt to condense the literature behind the \emph{SD-in-the-Head} protocol and present a comprehensive report on the protocol and its prerequisites.

Finally, this project serves as an opportunity to test our capabilities by implementing a cutting-edge cryptographic protocol in a modern, memory-safe language like Rust. As the final product, we will deliver this report alongside a Rust library and a client implementation for the SD-in-the-Head protocol.


\subsection{Choosing the SD-in-the-Head protocol}
The SD-in-the-Head protocol offers a highly modular design, enabling extensive configurability and support for various subroutines. This flexibility is particularly evident in the two variants included in the specification~\cite{aguilarsyndrome11}: the \textit{hypercube} and \textit{threshold} variants, both of which demonstrate significant performance improvements over the original protocol~\cite{feneuil2022syndrome,aguilar2023return,feneuil2023threshold}.

The threshold variant, in particular, provides an opportunity to explore and implement several cryptographic primitives, including Merkle Trees~\cite{becker2008merkle}, Linear Secret Sharing (Shamir), and Multi-Party Computation in the Head (MPCitH)~\cite{baum2020concretely}. Along with Galois Fields~\cite{brownadvanced}.

Additionally, the authors supply a detailed and well-crafted specification, along with a reference implementation in C++, which serves as a robust starting point for our work.


\subsection{Choosing Rust}
We chose to implement the SD-in-the-Head protocol in Rust for several compelling reasons. Rust, as a modern systems programming language, offers inherent memory and thread safety without compromising performance. Its strong safety guarantees\cite{jung2017rustbelt} are a key reason why Rust is included in the NIST list of safer programming languages~\cite{nistsaferlanguages}. These properties are particularly critical when implementing cryptographic primitives, where memory safety and robustness against concurrency issues are foundational to secure software development. Additionally, Rust provides fine-grained control over hardware resources, enabling the optimization of performance -- an essential consideration in cryptographic implementations

Furthermore, our choice of Rust was driven by a desire to investigate its strengths and limitations in the context of implementing and optimizing cryptographic protocols. To this end, our report will include an overview of the features that Rust provides (\autoref{sec:rust}).

We prioritized code readability and maintainability in which Rust offers a string advantage. We incorporated its included testing framework and benchmarking tools to validate the correctness and efficiency of the implementation. These measures also facilitate meaningful comparisons with alternative signature schemes.

For readability, we emphasized clear, well-documented, and logically structured code supported by Rust's documentation and modular structure.

Given the inherent flexibility and variants of the SD-in-the-Head protocol, our implementation leverages Rust's feature flags and build time scripts to enhance modularity and interchangeability. This approach not only supports a high degree of configurability but also ensures that the implementation remains adaptable to future changes and extensions.

\todo{Other signature schemes. How many signatures can be generated per second.}

\section{Structure of the report}
The report is structured as follows: \autoref{ch:prelim} provides the necessary background information on the cryptographic primitives and sub-protocols used in the SD-in-the-Head protocol. \autoref{ch:spec} presents the specification of the SD-in-the-Head protocol, focusing on the threshold variant. \autoref{ch:impl} details our implementation of the protocol in Rust, including the development process, design choices, and optimizations. \autoref{ch:bench} evaluates the performance of our implementation and compares it to the reference implementation and other alternatives. Finally, \autoref{ch:conclusion} summarizes our contributions.

\todo{Keep this up to date}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preliminaries}\label{ch:prelim}

In this section, we outline the mathematical and cryptographic foundations necessary to understand the SD-in-the-Head scheme and its components, providing a step-by-step progression from the basics to the construction of the protocol.

We begin by introducing the essential subroutines that underpin the SD-in-the-Head protocol. This includes \textbf{Galois Finite Fields}~\cite{martinez2023syndromes, reed1960polynomial, brownadvanced} along with cryptographic primitives; \textbf{collision-resistant hash functions} and \textbf{Merkle tree commitment scheme}~\cite{becker2008merkle}. In addition, we provide short introductions to the foundational protocols of \textbf{Secure Multi-Party Computation} (MPC) and \textbf{Zero-Knowledge} (ZK) proofs, which serve as the underlying cryptographic protocols.

Next, we delve into the Syndrome Decoding (SD) problem, which forms the backbone of the SD-in-the-Head protocol. This includes an overview of the problem definition~\cite{aguilarsyndrome11, mceliece1978public, berlekamp1978inherent, baldi2013optimization}, and its polynomial representation. This will support an understanding of how the protocol operates on hard instances of the SD problem.

Building upon these foundations, we lay the groundwork for the construction of the SD-in-the-Head protocol. First, we discuss the \textbf{Multi-Party-Computation-in-the-Head} (MPCitH) paradigm, explaining how it is leveraged to construct efficient ZK proofs~\cite{ishai2007zero}. Additionally we detail the role of the MPC preprocessing~\cite{baum2020concretely} and linear secret-sharing scheme~\cite{feneuil2023threshold}, which dramatically improve the efficiency of the MPCitH construction, and verification. Next, we introduce the \textbf{Fiat-Shamir heuristic}, a powerful tool that transforms an interactive ZK protocol into a non-interactive signature scheme~\cite{fiat1986prove}.

\section{Galois Finite Field}\label{sec:gf256}
Finite fields lays the basis for many cryptographic protocols and primitives. In essence, finite field theory describes the relation and properties between numbers, while disregarding the numbers themselves. In order to define finite fields, we first need to define a \textit{group} and a \textit{finite field} (\autoref{def:group} and \autoref{def:field}, respectively).

\begin{definition}\label{def:group}
  A group $\mathbb{G}$ is a tuple $(\mathbb{G}, \times, 1)$ where, $\mathbb{G}$ is the set of elements in the group, $\times$ is a binary operator, and $1$ is the multiplicative identity. The group $\mathbb{G}$ must satisfy the following properties:
  \begin{enumerate}
    \item $\mathbb{G}$ is \textbf{closed} under $\times$. For all $a,b \in \mathbb{G}$, $a \times b \in \mathbb{G}$.
    \item $\mathbb{G}$ is \textbf{associative} under $\times$. For all $a,b,c \in \mathbb{G}$, $(a \times b) \times c = a \times (b \times c)$.
    \item $\mathbb{G}$ has an \textbf{identity} element $1$ under $\times$. For all $a \in \mathbb{G}$, $a \times 1 = a$.
    \item $\mathbb{G}$ has an \textbf{inverse} element for each element under $\times$. For all $a \in \mathbb{G}$, there exists an element $a^{-1}$ such that $a \times a^{-1} = 1$.
  \end{enumerate}
\end{definition}

\begin{definition}\label{def:field}
  A finite field $\mathbb{F}$ is a tuple $(\mathbb{F}, +, \times, 0, 1)$ where, $\mathbb{F}$ is the set of elements in the field, $+$ and $\times$ are the addition and multiplication operators, and $0$ and $1$ are the additive and multiplicative identities. The field $\mathbb{F}$ must satisfy the following properties:
  \begin{enumerate}
    \item  $+$ and $\times$ are \textbf{commutative}, \textbf{associative} and $\times$ is \textbf{distributive} over $+$.
          \begin{enumerate}
            \item $\forall a,b \in \mathbb{F}: a + b = b + a$ and $a \times b = b \times a$ (commutative)
            \item $\forall a,b,c \in \mathbb{F}: (a + b) + c = a + (b + c)$ and $(a \times b) \times c = a \times (b \times c)$ (associative)
            \item $\forall a,b,c \in \mathbb{F}: a \times (b + c) = a \times b + a \times c$ (distributive)
          \end{enumerate}
    \item $(\mathbb{F}, +, 0)$ forms an additive group
    \item $(\mathbb{F} \setminus \{0\}, \times, 1)$ forms a multiplicative group
  \end{enumerate}
\end{definition}

\noindent
The most commonly known finite fields are the prime fields $\mathbb{F}_p$, widely used in discrete logarithm cryptosystems, such as ECDSA and ElGamal. These fields can be extended into $\mathbb{F}_{p^n}$, containing $p^n$ elements which we will describe in \autoref{sub:field_extension}. However, prime fields are not efficient for iterating over large numbers, due to the modular reduction overhead, a common requirement in protocols such as SD-in-the-Head. For better efficiency, we require a field that can be represented using bits or bytes. One method to achieve this is by constructing fields commonly known as Galois fields. Consider the simplest prime field, also called binary field, $\mathbb{F}_2 = ({0,1}, \texttt{XOR}, \texttt{AND}, 0, 1)$. It is straightforward to verify that this field satisfies the properties outlined in \autoref{def:field}.

The Galois field $\mathbb{F}{2^n}$ is an extension field of $\mathbb{F}2$, defined as $\mathbb{F}{2^n}$. This field is constructed by selecting an irreducible polynomial $f(x)$ of degree $n$ with coefficients in $\mathbb{F}2$. The elements of $\mathbb{F}{2^n}$ are represented as polynomials of degree at most $n-1$ with coefficients in $\mathbb{F}2$. Consequently, $\mathbb{F}{2^n}$ contains exactly $2^n$ elements. The polynomial $f(x)$ ensures that $\mathbb{F}_{2^n}$ forms a valid field as it defines the modular reduction in multiplication.

Each element in $\mathbb{F}_{2^n}$ is represented as a polynomial of degree at most $n-1$ with coefficients in $\mathbb{F}_2$.

\begin{align*}
  a(x) = a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \cdots + a_1x + a_0
\end{align*}

\noindent
where $a_i \in \mathbb{F}_2$ This corresponds directly into the 8-bit binary representation $a_{n-1}a_{n-2}\dots a_1a_0$. The addition and multiplication operations within $\mathbb{F}{2^n}$ are defined as follows:

\begin{itemize}
  \item \textbf{Addition}: The addition of two elements $a,b \in \mathbb{F}_{2^n}$ is defined as the bitwise XOR of the two elements. Due to the fact that it corresponds to addition modulo $2$.
  \item \textbf{Multiplication}: The multiplication of two elements $a,b \in \mathbb{F}_{2^n}$ is defined as the multiplication of the two polynomials modulo the irreducible polynomial $p(x)$.
\end{itemize}

A common choice for the irreducible polynomial is the Rijndael polynomial $p(x) = x^8 + x^4 + x^3 + x + 1$ used in the AES protocol~\cite{brownadvanced}. This field is commonly known as $\mathbb{F}_{2^8}$. The size of the field allows us to represent elements as bytes, which is efficient for iterating over large numbers of digits and it contains all the elements of a byte.

\subsubsection{Field extension}\label{sub:field_extension}

An advantageous property of Galois fields is that they can easily be extended, like $\mathbb{F}_{2}$ to $\mathbb{F}_{2^8}$. To ensure security in the SD-in-the-Head protocol, the field $\mathbb{F}_{2^8}$, which we shall denote as $\mathbb{F}_q$ is extended to encompass the field 32-bit unsigned integers. This can be done as a \textit{tower extension} by first building a degree-2 extension to $\mathbb{F}_{q^2}$ using the irreducible polynomial $F_{q^2} = F_q[X] / (X^2 + X + 32)$ and finally extending this field to $\mathbb{F}_{q^4}$ using the irreducible polynomial $F_{q^4} = F_q[Z] / (Z^2 + Z + 32(X))$. While you could create the final field independently, the tower extension allows us to reuse the construction of the intermediate field. Addition and multiplication in the extension fields are represented as the following

\begin{align*}
  a         & = a_0 + a_1x                                               &  & a \in \mathbb{F}_{q^\eta}, a_0, a_1 \in \mathbb{F}_q \\
  b         & = b_0 + b_1x                                               &  & b \in \mathbb{F}_{q^\eta}, b_0, b_1 \in \mathbb{F}_q \\\\
  a + b     & = (a_0 + a_1x) + (b_0 + b_1x) = (a_0 + b_0) + (a_1 + b_1)x                                                           \\
  a \cdot b & = (a_0 + a_1x) \cdot (b_0 + b_1x)                                                                                    \\
            & = a_0b_0 + (a_0b_1 + a_1b_0)x + a_1b_1x^2                                                                            \\
            & = a_0b_0 + (a_0b_1 + a_1b_0)x + a_1b_1(x + 32)                                                                       \\
            & = a_0b_0 + 32a_1b_1 + (a_0b_1 + a_1b_0 + a_1b_1)x
\end{align*}

\section{Cryptographic Primitives and protocols}
We assume the reader has basic knowledge of cryptographic primitives but will provide brief introductions to the primitives used in the SD-in-the-Head protocol. If you know the basics feel free to skip this section.

\subsection{Collision Resistant Hash Functions}\label{sec:prelim_hash}
We denote a hash function by a generator $\mathcal{H}$ which on input of a security parameter $k$ outputs a function $h : \{0,1{\}}^* \rightarrow \{0,1{\}}^{2k}$. We denote a hash function $h$ to be cryptographically secure if it satisfies the following properties

\begin{lemma}[Preimage Resistance]\label{lem:preimage}
  Given a hash function $h$ and a hashed message $c$, there exist no PPT algorithm that can find a message $m$ such that $h(m) = c$ with non-negligible probability.
\end{lemma}

\begin{lemma}[Collision Resistance]\label{lem:collision}
  Given a hash function $h$, there exist no PPT algorithm that can find two distinct messages $m, m'$ such that $h(m) = h(m')$ with non-negligible probability.
\end{lemma}

The consensus among researchers indicates that for collision and preimage resistance, quantum computing is expected to reduce the security level from $k$ to $k/2$ through approaches like Grover's algorithm~\cite{nielsen2010quantumgrover}. In contrast, no known methods have been proposed to reduce the security level for collision resistance. Consequently, protocols that depend on collision-resistant hash functions, such as Merkle Signature or SD-in-the-Head, remain a viable choice for post-quantum secure protocols.

In proving the security of hash functions, researchers often resort to models like the Quantum Random Oracle Model (QROM), while there is significant work being done to further prove the future security of hash functions~\cite{dtuPostquantumSecurity}.

\subsubsection{Extendable-output (hash) functions (XOF)}\label{sec:xof}
\todo{Give a short introduction to XOF}

\subsection{Commitment Schemes}
A commitment scheme allows a \textit{prover} to \textit{commit} to a value and later \textit{open} or reveal the value to a \textit{verifier}. It satisfies the following two properties:

\begin{lemma}[Binding]\label{lem:binding}
  Given a commitment $c$ to a value $v$, a dishonest prover can only change the committed value and have the verifier accept the new value with negligible probability.
\end{lemma}
\begin{lemma}[Hiding]\label{lem:hiding}
  Given a commitment $c$ to a value $v$, the verifier can only recover the value $v$ with negligible probability.
\end{lemma}

Consider a simple scenario. Alice and Bob are playing a game of \textit{heads or tails}. However each does not trust the other not to cheat
\begin{enumerate}[parsep=0pt, itemsep=0pt]
  \item Bob does not trust Alice not to correct her guess after learning his result
  \item Alice does not trust Bob not to use the knowledge of her guess to cheat
\end{enumerate}
Therefore, consider the following protocol. Given a public secure hash function $h$ \autoref{sec:prelim_hash}:
\begin{enumerate}[parsep=0pt, itemsep=0pt]
  \item Alice chooses a guess $g \in \{0,1\}$ for \textit{tails} and \textit{heads} respectively.
  \item Alice computes her commitment $c = h(g | w)$ for some random string $w$ and sends $c$ to Bob.
  \item Bob flips his coin to get the result $r \in \{0,1\}$ for \textit{tails} and \textit{heads} respectively.
  \item Bob announces $r$ to Alice.
  \item Alice announces $g', w$ to Bob.
  \item Bob accepts the result if $h(g' | w) = c$.
\end{enumerate}

From \autoref{lem:preimage}, we can see that Bob is not able to learn $g$ before he flips his coin without knowing $g \mid w$.\footnote{We can adjust the length of $w$ to enhance the security of the protocol.} This ensures \autoref{lem:hiding}.

Furthermore, Alice cannot change her result $g$ without being able to find a different $w'$ such that:
\[
  h(g' \mid w') = c.
\]
which she can only do with negligible probability per \autoref{lem:collision}, ensuring \autoref{lem:binding}.

\subsubsection{Merkle Tree Commitment Scheme}
A variant of such a scheme which allows for partial opening, meaning that the prover does not have to open the entire commitment, but can instead open a subset of the commitment, is the \textit{Merkle Tree Commitment scheme}~\cite{becker2008merkle}.

Let $v_1, \dots, v_n$ be the values to be committed, and let $h$ denote a cryptographically secure hash function. The leaves of the Merkle tree, $a_{0,1}, \dots, a_{0,n}$, are initialized as the hash values of the input values:
\[
  a_{0,i} = h(v_i), \quad \text{for } i = 1, \dots, n.
\]

Each parent node $a_{i,j}$, where $i$ represents the level of the tree and $j$ denotes the parent index at that level, is computed as the hash of its two child nodes:
\[
  a_{i,j} = h(a_{i-1,2j} \, | \, a_{i-1,2j+1}),
\]
where $|$ indicates concatenation of the child nodes.

The root node, $a_{h,0}$, where $h$ is the height of the tree, serves as the commitment value and is sent to the verifier.

To reveal a committed value $v$, the prover computes the authentication path for the value's leaf node $a_{0,l}$, where $l$ is the index of the value. The authentication path $A$ consists of the sequence of sibling nodes required to reconstruct the root. The first element in the authentication path, $A_0$, is the sibling of the leaf node:
\[
  A_0 =
  \begin{cases}
    a_{0,l-1}, & \text{if } l \text{ is odd,}  \\
    a_{0,l+1}, & \text{if } l \text{ is even.}
  \end{cases}
\]
This element is recorded as $auth_0$.

For higher levels, each subsequent element in the path is determined as:
\[
  A_i = h(A_{i-1} \, | \, auth_{i-1}),
\]
where $auth_i$ is the sibling node of $A_{i-1}$ at the same level.

The prover transmits the authentication path $A$ and the index $p$ of the selected value to the verifier. The verifier checks the validity of the commitment by using the provided leaf value and authentication path to recompute the root of the Merkle tree.

\subsubsection{Security properties}

The hiding property of the protocol is ensured by the use of a secure hash function, as the prover only shares the root of the tree. If the adversary were able to recover any value $v$, this would break the preimage resistance \autoref{lem:preimage} of the hash function.

Conversely, the binding property \autoref{lem:binding} is upheld by the fact that the adversary would have to find a collision for the hash function to change the commitment value.

A notable property of the Merkle tree is its ability to open a partial subset of the committed values by providing only the necessary authentication paths for the values in the subset. The verifier can still validate these values using a single global root value.

This property is particularly advantageous in protocols like the SD-in-the-Head threshold protocol, where only a subset of the committed values needs to be opened while keeping the rest concealed. By leveraging this feature, the prover can efficiently prove the validity of the subset without revealing the entire set of commitments.

\subsection{Secret Sharing Schemes (SSS)}

Secret Sharing Schemes (SSS) are a class of cryptographic primitives that allow a secret to be dispersed into multiple \textit{shares}, each of which can be distributed to different parties. These shares can then be recombined to reconstruct the original secret. SSS was initially invented~\cite{shamir1979share} to alleviate the single-point-of-failure problem for storage. Below we supply a definition of a secret sharing scheme~\cite{cramer2015secure} and the notation used in this report.

\begin{definition}[Secret Sharing Scheme (SSS)]\label{def:ss-share}
  We denote a SSS be a system of ``dispersing'' a secret $s$ into a sequence of $N$ pieces of data $\sh{s} = [s_1, \dots, s_n]$, called \emph{shares}, such that we have the following properties:
  \begin{itemize}[parsep=0pt, itemsep=0pt]
    \item \textit{$(N-1)$-privacy}: An adversary holding $l < N$ shares learns nothing about the secret $s$.
    \item \textit{Reconstruction}: Given a subset of shares $s_i$ of size $\ell$, the scheme can reconstruct the secret $s = \texttt{open}(s_i)$.
  \end{itemize}
  We denote the following notation:
  \begin{itemize}
    \item \texttt{share}: We denote a the function \texttt{share}$_n(s) \rightarrow \sh{s}$ to be a function that takes a secret $s$ and returns a sharing $\sh{s}$ consisting of $n$ shares $s_i$ such that \texttt{open}$(\sh{s}) = s$.
    \item \texttt{open}: We denote a function \texttt{open}$(\sh{s}) \rightarrow s$ to be the opening of the shared value $\sh{s}$ where each party $P_i$ broadcasts its share $s_i$ allowing all participants to recover the secret value $s$.
  \end{itemize}
\end{definition}

\subsubsection{Additive SSS and additive homomorphism}\label{sec:additive-sss}

In its simple form, a SSS can be based on the same ideas as the \textit{one-time pad} encryption. Assume a Abelian group $F$ and a share $\sh{s} \in F^n$ of a secret $s \in F$. Sample $n-1$ uniformly random elements $[s_1, \dots, s_{n-1}] \in \mathbb{F}^{n-1}$ and set $s_n = s - \sum_{i=1}^{n-1} s_i$. Then we have that
\begin{align*}
  s = \sum_{i=1}^{n} s_i
\end{align*}
A notable feature of this construction is that if the parties possess two shares, $\sh{x}$ and $\sh{y}$, they can easily compute $\sh{x + y}$ by simply adding their individual shares. This property is referred to as \textit{partial homomorphic} or \textit{additively homomorphic}. The correctness of $\sh{x} + \sh{y} = \sh{c}$ for $c = x + y$ can be demonstrated as follows:
\begin{align*}
  c & = \texttt{open}(\sh{c})                         \\
    & = \texttt{open}(\sh{x} + \sh{y})                \\
    & = \texttt{open}([x_i + y_i])                    \\
    & = \sum_{i=1}^{n} x_i + y_i                      \\
    & = \sum_{i=1}^{n} x_i + \sum_{i=1}^{n} y_i       \\
    & = \texttt{open}(\sh{x}) + \texttt{open}(\sh{y}) \\
    & = x + y
\end{align*}
This property enables us to easily produce Multi-Party Computation (MPC) (see \autoref{sec:mpc}) on top of such a SSS\@. However, to allow full arithmetic circuits, we need more elaborate protocols, which we will discuss in \autoref{sec:zk-arithmetic-mpc-circuits}.
\subsubsection{Threshold SSS}\label{sec:threshold-sss}

In many situations such schemes are not sufficient. Imagine wanting to solve the the distribution of a secret (e.g. a password) among a set of locations to ensure that you do not have a single-point-of-failure. To solve this, we require a SSS that can reconstruct the secret from a a subset of shares up to a threshold $\ell$. This is called a $(\ell, n)$-threshold SSS\@.

\begin{definition}[$(\ell, n)$-private (threshold) SSS]\label{def:mpc-ss-threshold}
  A $(\ell, n)$-threshold SSS~\cite{cramer2015secure}, where $\ell, n$ are integers with $0 \leq \ell < n$, denotes a SSS such that:
  \begin{itemize}[parsep=0pt, itemsep=0pt]
    \item \textit{$\ell$-privacy}: Any $\leq t$ shares jointly give no information on the secret $s$. Namely,
    \item \textit{Reconstruction}: Any $\geq t + 1$ of these shares jointly determine the secret $s$ uniquely (\textit{$(\ell + 1)$-opening}).
  \end{itemize}
\end{definition}

In this case, the SSS described above would not suffice as loosing even one share would result in the loss of the secret. There are several ways to solve this problem. We could distribute more than one share to each location. If distributed correctly, we can ensure that only $t \leq N$ parties need to be available. However, there are also other more sophisticated schemes that can be used to ensure that the secret is not lost.

\subsubsection{Shamir's Secret Sharing}\label{sec:shamir}

One particular example, and the one utilized by the SD-in-the-Head protocol, is Shamir's Secret Sharing scheme~\cite{shamir1979share,cramer2015secure}. This $(\ell, n)$-private threshold SSS is based on polynomials over a finite field $\mathbb{F}$. A secret $s$ is shared by choosing a random polynomial $P_s$ of degree at most $\ell \geq 2$ with $s = P_s(0)$. For each participant $i \in [n]$, $P_i$, a share $s_i$ is generated by evaluating $P(i)$. The secret can then be reconstructed by interpolating the polynomial from a $\ell + 1$ shares and with $\ell$ or less shares we gain no information about the secret -- i.e. we have $\ell$-privacy. We can prove reconstruction, correctness and privacy using \textit{lagrange interpolation}.

\begin{definition}[Lagrange interpolation]\label{def:lagrange}
  If $h(X)$ is a polynomial over $\mathbb{F}$ of degree at most $\ell$ and if $C$ is a subset of $\mathbb{F}$ with $|C| = \ell + 1$, then
  \begin{align}\label{eq:lagrange1}
    h(X) = \sum_{i\in C}h(i)\delta_i(X)
  \end{align}
  where $\delta_i(X)$ is a degree $l$ polynomial s.t. for all $i,j \in C$, $\delta_i(j) = 0$ if $i \neq j$ and $\delta_i(i) = 1$ if $i = j$.
  \begin{align}\label{eq:lagrange2}
    \delta_i(X) = \prod_{j \in C,j\neq i} \frac{X-j}{i-j}
  \end{align}
\end{definition}

\begin{lemma}\label{lem:lagrange}
  Given any set of pairs $S = \{x_i, y_i \in F| i \in C\}$, $|C| = \ell + 1$, we can construct exactly one polynomial $h$ with $h(i) = y_i$ of degree at most $\ell$ as
  \begin{align*}
    h(X) = \sum_{i \in C} y_i \delta_i(X)
  \end{align*}
  \noindent and that all coefficients of this polynomial can be efficiently computed from $S$.
\end{lemma}

It from \autoref{def:lagrange} and \autoref{eq:lagrange1} that the secret can be easily \textit{reconstructed}. Furthermore, if there existed two solution polynomials $h, h'$ then $h(x) - h'(x)$ would be a non-zero polynomial of degree at most $\ell$ and would have at least $\ell + 1$ roots, which cannot exist. This gives use \textit{correctness}

For \textit{$(\ell)$-privacy}, consider an index set $I$ with $|I| \leq \ell$. For a sharing of a secret $s$ and the resulting polynomial $h(x)$, where $h(0) = s$, it evaluates to a set of shares $\{h(i) \mid i \in I\}$.
Conversely, consider a random set of shares $A = \{s_i \mid i \in I\}$. It follows from \autoref{lem:lagrange} that there exists exactly one polynomial $h_A(x)$ such that
\begin{align*}
  h_A(0) & = s,                                 \\
  h_A(i) & = s_i \quad \text{for all } i \in I.
\end{align*}
This implies that there are $|\mathbb{F}|^\ell$ possible polynomials which in turn implies that for any set of $\ell$ shares, the possible reconstructed secret $s$ is uniform in $\mathbb{F}$. See~\autoref{fig:shamir} for a visual example.

Furthermore, the $(\ell, n)$ the scheme is $(+, +)$-homomorphic. This follows from the fact that polynomials over $\mathbb{F}$ are \textit{distributive over addition}. In other words, given two secrets $a, b$ and their corresponding polynomials $h_a(x)$ and $h_b(x)$, we have
\begin{align*}
                & (h_a + h_b)(i) = h_a(i) + h_b(i)          \\
  \Rightarrow\  & (h_a + h_b)(0) = h_a(0) + h_b(0) = a + b.
\end{align*}

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{images/shamir.png}
  \caption{Visualization of the Shamir's polynomial based secret sharing scheme. The dark line shows the specific sharing instance for a secret $s$ and a threshold $\ell=2$. With three shares, the secret can be reconstructed by interpolating the polynomial from the shares. However, with one or two shares, the reconstructed polynomial is not unique as we can see from the dotted lines.}\label{fig:shamir}
\end{figure}

\subsection{Secure Multi-Party Computation}\label{sec:mpc}

Secure Multi-Party Computation (MPC) refers to a cryptographic protocol enabling multiple parties to jointly compute a function $f$ represented by a circuit $C$, ensuring that no information about the individual inputs is revealed beyond what can be inferred from the output of $C$. In the following, sections we will denote a MPC protocol by $\Pi_f$ for an $n$-party functionality $f(x, w_1, \dots, w_n)$, a public input $x$ and secret inputs $w_i$ of the party $P_i$. The goal is to compute the function $f$ on the inputs of the $n$ parties while upholding the following properties~\cite{cramer2015secure}

\begin{definition}[Correctness]\label{def:mpc-correctness}
  We say that $\Pi$ realizes a deterministic $n$-party functionality $f(x, w_1, \dots, w_n)$ with perfect (resp., statistical) correctness if for all inputs $x, w_1, \dots, w_n$, the probability that the output of some player is different from the output of $f$ is $0$ (resp., negligible in $k$), where the probability is over the independent choices of the random inputs $r_1, \dots, r_n$.
\end{definition}

\begin{definition}[Privacy]\label{def:mpc-privacy}
  The protocol ensures that no party learns anything about the inputs of the other parties beyond what can be inferred from the output of the function.
\end{definition}

\begin{definition}[$\ell$-privacy]\label{def:mpc-ell-privacy}
  The MPC protocol has privacy up to $\ell$ malicious parties.\footnote{Most often achieved by using a $(\ell, n)$-private secret sharing scheme \autoref{def:mpc-ss-threshold}}
\end{definition}

In terms of security needed for the SD-in-the-Head protocol, we define the following security requirements for the MPC protocol $\Pi_f$

\begin{itemize}
  \item \textbf{Semi-honest security}: The protocol is secure against semi-honest adversaries, where parties follow the protocol but may attempt to learn information from the messages they receive.
  \item \textbf{Low-threshold security}: The protocol is secure against a coalition of up to $\ell$ parties, where $\ell$ is the threshold. This is also known as a $\ell$-private MPC protocol.
\end{itemize}

In order to prove the validity of a MPC protocol that have been run in the head, we define the following notion of \textit{consistency}~\cite{ishai2007zero}.

\begin{definition}[MPC view]\label{def:mpc-view}
  We denote the view $V_i$ of a party $P_i$ in the protocol as $(i, x, r_i, w_i, (m_1, \dots, m_j))$ where $r_i$ is the randomness used by $P_i$, $w_i$ is the secret share, and $(m_1, \dots, m_j)$ is the messages received by $P_i$ in the first $j$ rounds of the protocol. Note that the messages sent by the parties can be inferred from the $V_i$ by invoking $\Pi$.
\end{definition}

\begin{definition}[Consistent views]\label{def:mpc-consistent-view}
  We say that a pair of views $V_i, V_j$ are consistent (with respect to the protocol $\Pi$ and some public input $x$) if the outgoing messages implicit in $V_i, x$ are identical to the incoming messages reported in $V_j$ and vice versa.
\end{definition}

\begin{lemma}[Local vs. global consistency]\label{lem:consistency}
  Let $\Pi$ be an $n$-party protocol as above and $x$ be a public input.
  Let $V_1, \dots, V_n$ be an $n$-tuple of (possibly incorrect) views. Then all pairs of views $V_i, V_j$ are consistent with respect to $\Pi$ and $x$ if and only if there exists an honest execution of $\Pi$ with public input $x$ (and some choice of private inputs $w_i$ and random inputs $r_i$) in which $V_i$ is the view of $P_i$ for every $1 \leq i \leq n$.
\end{lemma}

\textbf{Proof of \autoref{lem:consistency}} The \textit{if} direction is trivial and follows from \autoref{def:mpc-view} and \autoref{def:mpc-consistent-view}.
The \textit{only if} direction is shown by the following. Consider $n$ pairwise consistent views $V_1, \dots, V_n$ \autoref{def:mpc-view}. Let us define an MPC protocol $\Pi$ by its \textit{next sent message} function $\Pi_j(i,x,w_i,r_i, (m_1, \dots, m_j)) = m_{j+1}$. The pairwise consistency, \autoref{def:mpc-consistent-view} and $\Pi_j$ implies, by induction that after $d$ rounds, that the actual view of all parties $P_i$ is the same as the view of $P_i$ in the first $d$ rounds. It follows that the views $V_i, \dots, V_n$ are consistent with the full execution of $\Pi$. \qed

\subsection{Zero-Knowledge Proofs}\label{sec:zk}
This section will give a brief introduction to \textit{two-party interactive} Zero-Knowledge (ZK) schemes for a \textit{prover} and a \textit{verifier}. The intuition behind a Zero-Knowledge proof of knowledge is that the prover can convince the verifier that they know a secret $w$, such that $x$ is true, without revealing the secret $w$ to the verifier. This could be someone wanting to prove that they are above the age of 18 without revealing their age. We denote a ZK $\Pi_{\mathcal{R}}$ for some NP relation $\mathcal{R}(x, w)$. Let $x$ be a public statement in \textbf{NP} and $w$ be a witness such that $(x, w) \in \mathcal{R}$~\cite{feneuil2023threshold}.
\todo{Use the ivan reference}

\begin{definition}
  Let $x$ be a statement of language $L$ in \textbf{NP}, and $W(x)$ the set of witnesses for $x$ such that the following relation holds:
  \begin{align*}
    \mathcal{R} = \{(x, w)\; x \in L, w \in W(x)\}
  \end{align*}
\end{definition}

\section{Syndrome Decoding Problem}\label{sec:syndrome}

The SD-in-the-Head protocol is built on the computational hardness of the Syndrome Decoding (SD) problem for random linear codes over a finite field (see \autoref{sec:gf256}). This protocol uses a variant of the SD problem, referred to as the \textit{coset weights} problem, first introduced by Berlekamp and McEliece in 1978~\cite{berlekamp1978inherent}. The problem is defined as follows:
\begin{definition}\label{def:syndrome}
  Given $H \in \mathbb{F}^{(m-k)\times m}_q$ and $y \in \mathbb{F}^{m-k}_q$. The problem is to find $x \in \mathbb{F}^m_q$ s.t.\ wt$(x) \leq w$ such that $Hx = y$.
\end{definition}
Generating such an instance is straightforward: one can construct a uniformly random parity-check matrix $H$ and a codeword $x$ (with $wt(x) \leq w$), and then compute the syndrome $y = Hx$. In the SD-in-the-Head protocol, the values of the matrix $H$ and the syndrome $y$ are elements of the finite field $\mathbb{F}_q$ explained previously in \autoref{sec:gf256}. The SD problem is well-known to be NP-complete for random instances~\cite{berlekamp1978inherent} also referred to as the general decoding problem.To illustrate the computational difficulty, solving the problem using brute force would require $O(\binom{m}{w} q^w)$ operations, which is computationally infeasible for large $m$ and $k$.

\subsubsection{Standard form of the parity-check matrix}\label{sec:standard_form_of_the_parity_check_matrix}
To improve the performance and reduce the key size of the protocol, its possible to utilize the fact that the matrix $H$ can be in standard form. $H = (H'|I_{m-k}) $ Where $H' \in \mathbb{F}^{(m-k)\times k}_q$, and $I_{m-k}$ is the identity matrix of size $m-k$. This allows for the following representation of the syndrome:
\begin{equation}
  y = Hx = H'x_a + x_b\label{eq:standard_form_of_the_parity_check_matrix}
\end{equation}
with $x = (x_a | x_b)$. This improves the performance of the algorithms used in the SD-in-the-Head protocol in the following ways
\begin{itemize}
  \item At the MPC layer, we only need to reveal one share $x_a$. Due to the fact that the other share $x_b$ can simply be recomputed by $x_b = y - H'x_a$.
  \item By linearity of the above relation one only needs to send $x_a$ in order to recover $Hx = H'x_a + x_b$ the Syndrome decoding instance. So from a sharing of $x_a$ one can check the correctness of the SD instance.
\end{itemize}

\subsubsection{Polynomial representation of SD}\label{sec:polynomial_representation}
In order to convert the SD problem into a form that allows for \textit{Multi Party Computation in the Head} (MPCitH, which we will exlpain further in \autoref{sec:mpcinth}) the SD-in-the-Head protocol is based on three (witness-dependent) polynomials $S, Q$ and $P$, and one public polynomial $F$.
These are used for checking the correctness of the SD solution by verifying the following relation:
\begin{equation}
  \centering
  S\cdot Q = P\cdot F\label{eq:polynomial_representation}
\end{equation}
Let $f_1,\dots, f_q$ denote all the elements of $\mathbb{F}_q$ and $x\in \mathbb{F}^m_q$ is a binary vector with hamming weight $wt(x) /leq w$, then the polynomials are defined as:
\begin{itemize}
  \item $S\in \mathbb{F}_q[X]$ is the Lagrange interpolation of the coordinates of $x$, such that it matches $S(f_i) = x_i$ for $i\in [1:m]$ and has degree $\text{deg}(S) \leq m-1$
  \item $Q\in \mathbb{F}_q[X]$ is defined by $Q(X) = \prod_{i\in E}(X - f_i)$. $E \subset [1:m]$ with order $|E| = w$, such that $E$ contains the non-zero coordinates of $x$. $Q$ has degree $\text{deg}(Q) = w$.
  \item $P\in \mathbb{F}_q[X]$ is defined as $P = S\cdot Q/F$ and has degree $\text{deg}(P) \leq w-1$. By definition the polynomial $F$ divides $S\cdot Q$.
  \item $F\in \mathbb{F}_q[X]$ is the \textit{vanishing polynomial} of the set ${f_1, \dots, f_m}$ also defined as $F(X) = \prod_{i\in [1:m]}(X - f_i)$ and has degree $\text{deg}(F) = m$.
\end{itemize}
We can now look at the relation in \autoref{eq:polynomial_representation}. If we look at the left-hand side, which has the following property by design $S\cdot Q(f_i) = 0 \ \forall\ f_i \in [1:m]$. This comes from the fact that the polynomial $S(f_i) = 0$ whenever $x_i = 0$, as it is the lagrange interpolation of $x$. Furthermore, the polynomial $Q(f_i)$ is zero whenever $f_i$ is a non-zero coordinate of $x$, which follows from the definition of $Q$.

For the right-hand side, the polynomial $F$ is the vanishing polynomial for the set ${f_1, \dots, f_m}$, so $F(f_i) = 0\ \forall\ f_i \in [1:m]$. The polynomial $P$ is needed to match the degree of $S \cdot Q$. As the degree of $F$ is $m \leq \text{deg}(S\cdot Q) \leq m + w - 1$.

It is now apparent that if the prover can convince the verifier that they know of polynomials $P,Q$ such that $S\cdot Q = F \cdot P = 0$ at all points $f_i \in [1:m]$. The following must hold, either $S(f_i) = x_i = 0$ or $Q(f_i) = 0$. However, the polynomial $Q$ can be zero in at most $w$ points based on the degree, this means that S is non-zero in at most $w$ points, based on the construction, which in turn implies that $x$ has weight at most $w$.

With this, we can define the soundness of the language for ZKP as follows:
\begin{equation}
  wt(x) \leq w \Leftrightarrow \exists P,Q \text{  with  }\text{deg}(P)\leq w-1\text{  and  }\text{deg}(Q) = w\text{ s.t. \autoref{eq:polynomial_representation} holds}\label{eq:soundness}
\end{equation}

We can now share a witness $(x_a, Q, P)$. Now based on the relation from \autoref{eq:standard_form_of_the_parity_check_matrix}, $x$ can be locally computed along with the polynomial $S$, this can then be used to run an equality test for the relation $S \cdot Q = P \cdot F$.

\subsubsection{False positive probability}\label{sub:equality_test}
The equality test from \autoref{eq:polynomial_representation} has a small probability of false positives, denoted $p$. To reduce this probability, the relation is evaluated at random points $\{r_k \in \mathbb{F}_q\}_{k\in[t]}$. By Schwartz-Zippel \autoref{lem:schwartz}, the probability of a false positive is bounded by $p \leq \frac{t}{q}$, where $q$ is the size of the finite field $\mathbb{F}_q$. In short, this makes it unlikely that the relation will hold for all points $r_k$ if the relation is not sound according to \autoref{eq:soundness}. Furthermore, we can tweak the parameters $t$ and $q$ to reduce $p$.

\begin{lemma}[Schwartz-Zippel]\label{lem:schwartz}
  For a non-zero polynomial $P \in \mathbb{S}[X]$ of degree $d \geq 0$. Let $\mathbb{R}$ be a finite subset of $\mathbb{S}$ and set of random points $[r_1, \dots, r_n] \in \mathbb{R}$, the probability that $\Pr[P(r_1, \dots, r_n) = 0] \leq d/|\mathbb{R}|$.
\end{lemma}

\section{MPC-in-the-Head}\label{sec:mpcinth}

The SD-in-the-Head protocol construction is based on the \textit{Multi-Party Computation in the Head} (MPCitH) framework. In this section we will give an introduction to the framework and how it can be used to construct ZK proofs, which in turn can be combined with the Fiat-Shamir heuristic to create a signature scheme.

The MPC-in-the-Head (MPCitH) framework, introduced by~\cite{ishai2007zero}, builds upon these techniques to construct generic zero-knowledge protocols (ZK). A ZK protocol allows a \textit{prover} to convince a \textit{verifier} of the validity of a statement without revealing any information about the inputs to the statement. The framework provides a versatile method of constructing protocols that are quantum safe as its security relies on assumptions that are still believed to be quantum secure. Namely, commitment schemes and hash functions~\cite{feneuil2023threshold} which have no known quantum algorithms that break their security.

We will give insight into the basic construction suggested by Ishai et al~\cite{ishai2007zero}.

\begin{definition}\label{def:mpcinth_basic}
  Given a semi-honest $\ell$-private MPC protocol $\Pi_f$ with perfect correctness, a relation $\mathcal{R}$ for some public statement $x$, a witness $w$. Let $w_i$ be an additive secret share of $\sh{w}$ for the party $P_i$. Let $f$ be a $n$-party functionality $f(x, w_1, \dots, w_n) = \mathcal{R}(x, w)$, i.e. $f(x,w)$ accepts if $(x,w) \in \mathcal{R}$.

  \begin{enumerate}[parsep=2pt, itemsep=0pt]
    \item The prover builds a random sharing of $\sh{w} = w_1, \dots, w_n$. Then
          \begin{enumerate}[nolistsep]
            \item Simulates the outputs of the MPC protocol $\Pi_f$ on the inputs $(x, w_1, \dots, w_n)$ and the randomness $r_1, \dots, r_n$.
            \item Prepares views $V_1, \dots, V_n$ of the parties in the protocol $\Pi_f$.\footnote{Remember that the views include the inputs, randomness and messages received by the parties.}
            \item Commits to each view $V_i$ using a secure commitment scheme, and sends ($\text{Commit}(V_1), \dots, \text{Commit}(V_n)$) to the verifier.
          \end{enumerate}
    \item The verifier picks $\ell$ random distinct indices $i \in [n]$ and sends them to the prover.
    \item The prover opens the corresponding $\ell$ commitments into the views $V_i$ and sends the openings to the verifier.
    \item The verifier accepts if and only if:
          \begin{enumerate}[nolistsep]
            \item\label{prop:mpcinth_commit} The views are valid according to the commitment scheme.
            \item\label{prop:mpcinth_consistent} The views are consistent according to the public input $x$.
            \item\label{prop:mpcinth_knowledge} The views output $1$ according to $\mathcal{R}$ meaning that $\mathcal{R}(x,w) = 1$.
          \end{enumerate}
  \end{enumerate}
\end{definition}

We see the following properties of the protocol:

\begin{lemma}[Completeness~\cite{ishai2007zero}]\label{def:mpcinth_completeness}
  A boolean function is said to be complete if it depends on all inputs of the function. Given an honest prover, $\mathcal{R}(x,w) = 1$ and the correctness of $\Pi_f$, all outputs of $P_i$ are one and all views are consistent.
\end{lemma}

\begin{lemma}[Soundness~\cite{ishai2007zero}]
  If the statement $x$ is false, i.e. $x \notin L$, then $\mathcal{R}(x,w) = 0$ for all $w$. By the correctness of $\Pi_f$, the output of all parties must be $0$. If the verifier accepts, then the prover must have created $\ell$ inconsistent views. This happens with probability at most $p = 1 / \binom{n}{\ell}$. The error probability can be reduced to $2^{-k}$ by repeating the protocol $O(kn^2)$ times.
\end{lemma}

\begin{lemma}[Zero-Knowledge~\cite{ishai2007zero}]
  The verifier only sees $\ell$ views, and therefore from the definition of $\Pi_f$ learns nothing of the secret witness $w$.
\end{lemma}

The basic MPC-in-the-Head protocol serves as a foundation for constructing zero-knowledge (ZK) proofs for any NP relation and has been demonstrated to yield relatively efficient ZK protocols~\cite{feneuil2023threshold,baum2020concretely,katz2018improved}. Giacomelli et al. and Chase et al.~\cite{katz2018improved,giacomelli2016zkboo,chase2017post} provided concrete implementations of the \emph{MPC-in-the-Head} approach and observed that employing a 3-party protocol $\Pi$ achieved optimal performance within the space of protocols they analyzed. However, due to the small number of parties, the soundness of the resulting honest-verifier zero-knowledge (HVZK) proof is relatively weak. As a consequence, a large number of parallel repetitions is required to achieve negligible soundness error. This increases the size of the proofs and the communication overhead of the protocols. However, recent work by Baum and Nof~\cite{baum2020concretely} and Feneuil and Rivain~\cite{feneuil2023threshold} propose promising approaches to mitigate these issues.

\subsection{MPC preprocessing}

Pre-processing in MPC is an optimization technique that leverages shares of correlated randomness, such as randomness derived from a shared seed, which remains independent of the inputs to the protocol. This independence allows the parties to accelerate computation and perform pre-computation in advance. Consequently, this approach enables the design of MPCitH protocols that can utilize any $N$-party MPC protocol during pre-processing with minimal impact on the proof size. This method was first introduced by~\cite{katz2018improved} as part of their \textit{cut-and-choose} approach.

\subsubsection{MPC over arithmetic circuits}\label{sec:zk-arithmetic-mpc-circuits}

In order to form a ZK proof for the SD polynomial relation in \autoref{sec:polynomial_representation}, we need a MPC protocol that allows us to securely compute an arithmetic circuit. For this we use \textit{beaver triples}.

\begin{definition}[Beaver triple~\cite{beaver1992efficient}]\label{def:beaver}
  A beaver triple is a tuple $(a, b, c)$ where $a \cdot b = c$ and $a, b, c \in \mathbb{F}_q$.
\end{definition}

Consider the protocol for a single multiplication gate below by~\cite{baum2020concretely}, based on the MPC protocol described in~\cite{damgaard2012multiparty}. Note, that a similar technique can be applied to square relations. However, we focus here solely on multiplication, as it suffices for the SD relation.

\begin{definition}[Verification of a multiplication triple by sacrificing another]\label{def:sacrifice}
  Given an input triple $(x,y,z) \in \mathbb{F}$ random shared triple $(\sh{a}, \sh{b}, \sh{c}) \in \mathbb{F}$, it is possible to verify the correctness of the statement $z = x \cdot y$ without revealing any information on either of the input. Define \texttt{open} according to \autoref{def:ss-opening}.
  \begin{enumerate}
    \item The parties generate a random $\epsilon \in \mathbb{F}$.
    \item The parties locally set $\sh{\alpha} = \epsilon\sh{x} + \sh{a}, \sh{\beta} = \sh{y} + \sh{b}$.
    \item The parties run \texttt{open}$(\sh{\alpha})$ and \texttt{open}$(\sh{\beta})$ to obtain $\alpha$ and $\beta$.
    \item The parties locally set $\sh{v} = \epsilon\sh{z} - \sh{c} + \alpha  \cdot \sh{b} + \beta  \cdot \sh{a} - \alpha  \cdot \beta$.
    \item The parties run \texttt{open}$(\sh{v})$ to obtain $v$ and accept iff $v = 0$.
  \end{enumerate}
\end{definition}
Observe that if both triples are correct multiplication triples (i.e., $z = xy$ and $c = ab$) then the parties will always accept since
\begin{align}
  v & = \epsilon \cdot z - c + \alpha \cdot b + \beta \cdot a - \alpha \cdot \beta                                            \\
    & = \epsilon \cdot xy - ab + (\epsilon \cdot x + a)b + (y + b)a - (\epsilon \cdot x + a)(y + b)                           \\
    & = \epsilon \cdot xy - ab + \epsilon \cdot xb + ab + ya + ba - \epsilon \cdot xy - \epsilon \cdot xb - ay - ab           \\
    & = (\epsilon \cdot xy - \epsilon \cdot xy) + (ab - ab) + (\epsilon \cdot xb - \epsilon \cdot xb) + (ya - ay) + (ba - ab) \\
    & = 0
\end{align}
Otherwise, the parties will accept with probability $\frac{1}{|\mathbb{F}|}$ following from \autoref{lem:mpc-multiplication}.

\begin{lemma}\label{lem:mpc-multiplication}
  If $(\sh{a}, \sh{b}, \sh{c})$ or $(\sh{x}, \sh{y}, \sh{z})$ is an incorrect multiplication triple then the parties output \texttt{Accept} in the sub-protocol above with probability $\frac{1}{|\mathbb{F}|}$.\footnote{A full proof of this can be found in~\cite{baum2020concretely}.}
\end{lemma}

\subsubsection{ZK from Cut-and-Choose}\label{sec:zk-cut-and-choose}

Building upon the MPC protocol above, \autoref{def:cut-and-choose} outlines the fundamental concept of the cut-and-choose technique. For a more comprehensive protocol, refer to the \textit{HVZKAoK Protocol using Cut and Choose} discussed in~\cite{baum2020concretely}. Note

\begin{definition}[ZK from Cut-and-Choose (Multiplication)]\label{def:cut-and-choose}
  Denote the security parameters $M$ the number of pre-processing executions, $N$ the number of parties, $\tau$ the security parameter, a secure hash function \autoref{sec:prelim_hash} and a \textit{Random Oracle-based commitment scheme}. Given a $(N-1)$-private MPC protocol $\Pi_C$ for a circuit $C$ and an ideal functionality $\mathcal{F}$. Furthermore, we also denote $\mathcal{P}$ to be the prover and $\mathcal{V}$ as the verifier. \textit{Note} that we assume that there is only one multiplication gate in the circuit, to align with our purposes.
  \begin{enumerate}[itemsep=0pt, parsep=0pt]
    \item\label{itm:cut-choose-r1} For each $e \in [M]$ and for each party $P_i \in [N]$:
    \begin{enumerate}[nolistsep]
      \item $\mathcal{P}$ chooses a random $\texttt{salt} \leftarrow \{0,1\}^\lambda$ used for commitments.
      \item $\mathcal{P}$ chooses a master seed $sd_e$ and  expands $sd_e$ to $sd_{e,i}$ for each $i \in [N]$ using $\Pi_C$.\footnote{If the seed is expanded in a correlated way, you can send just $sd_e$ and have the verifer recreate the expanded seeds accordingly}
      \item $\mathcal{P}$ computes valid shares of beaver triples (\autoref{def:beaver}) $a_{e,i}$, $b_{e, i}$ and $c_{e,i}$ using $sd_{e,i}$.
            \begin{enumerate}
              \item To fix the initial randomness of $\sh{c_e}$, recompute $a_e$ and $b_e$ from the shares and set $c_e = a_e \cdot b_e$.
              \item Compute $\Delta_e = c_e - \sum_{i=1}^N c_{e,i}$.
              \item Set the triple of the multiplication gate to ($\sh{a_e}, \sh{b_e}, \sh{c_e} + \Delta_e$).
            \end{enumerate}
      \item $\mathcal{P}$ generates random sharings $w_i$ of the input $w$ using $sd_{e,i}$.
    \end{enumerate}
    \item $\mathcal{P}$ commits to the triples (including $\Delta_e$) and a sharing of the input $w$ along with the seeds $sd_{e,i}$\footnote{In order to further reduce communication all commitments can be hashed together to one hash.}
    \item $\mathcal{V}$ chooses a random challenge $E \subset [M]$ such that $|E| = \tau$ and sends it to $\mathcal{P}$. Set $\bar{E} = [M] \setminus E$
          \item\label{itm:cut-choose-r3} $\mathcal{P}$ sends the seeds $sd_e$ to $\mathcal{V}$ who verifies that the triples are correctly generated per step~\ref{itm:cut-choose-r1}
    \item $\mathcal{P}$ runs the MPC protocol $\Pi_C$ on the remaining $M-\tau$ pre-computed triples with different randomness $r_e$ resulting in shares $\alpha_e, \beta_e$.
    \item $\mathcal{P}$ commits to the views (\autoref{def:mpc-view}) of $\Pi_C$.
    \item For each $e \in \bar{E}$, $\mathcal{V}$ chooses a random $i \in [N]$ and sends it to $\mathcal{P}$.
    \item For each $e \in \bar{E}$: Let $I_e = [N] \setminus \{i_e\}$, $\mathcal{P}$ opens the commitments of views of the MPC protocol for $i \in I_e$ and sends the openings to $\mathcal{V}$ along with the seed $sd_{\bar{E}}$ and \texttt{salt} to recompute randomness.
    \item $\mathcal{V}$ outputs \texttt{Accept} if
          \begin{enumerate}[nolistsep]
            \item Step~\ref{itm:cut-choose-r3} was verified
            \item $\mathcal{V}$ verifies the commitments and consistency \autoref{def:mpc-consistent-view} of the views by running $\Pi_C$ as an honest participant using the pseudo-randomness from the seed and \texttt{salt}.
            \item $\mathcal{V}$ verifies for each $e \in \bar{E}$ that the recomputed output $\sh{y}$ of $\Pi_C$ is $y$
          \end{enumerate}
  \end{enumerate}
\end{definition}

Zero-Knowledge of the protocol is satisfied by the $N$-privacy of $\Pi_C$ (\autoref{def:mpc-ell-privacy}) as only $N-1$ views are shared, the witness $w$ is never revealed to the verifier.

We bound probability of $\mathcal{V}$ accepting a cheating outcome of \autoref{def:cut-and-choose}, i.e. that $C(w) \neq y$. First, we assume that $\mathcal{P}$ cheats in step~\ref{itm:cut-choose-r1} by incorrectly generating the precomputed values. The first challenge $E$ allows $\mathcal{V}$ to test $\tau$ triples. It's trivial to detect inconsistencies in the triples through the seed $sd_e$ by recomputing the triples and comparing them to the opened commitments. To reduce communication even more, the resulting computation from $\mathcal{P}$ can be hashed into a single hash which $\mathcal{V}$ tries to recompute. The probability that cheating is not detected is $\frac{\binom{M-c}{\tau}}{\binom{M}{\tau}}$. Next, we assume that $\mathcal{P}$ cheats by deviating from $\Pi_C$. In order to make the output of the protocol $y$, $\mathcal{P}$ must deviate in $M-\tau-c$ emulations. As $N-1$ views are opened and verified, $\mathcal{P}$ can only cheat in one view of \textbf{one} party. The probability that this is not detected is $\frac{1}{N^{M-\tau-c}}$. \todo{c Has not been introduced in this section, so maybe switch with next section}

\begin{lemma}[Soundness of the cut-and-choose approach]\label{lem:cut-and-choose-soundness}
  Let $c$ be the number of pre-processing emulations where $\mathcal{P}$ cheats. The probability that $\mathcal{V}$ accepts the outcome of \autoref{def:cut-and-choose} for $C(w) \neq y$ is bounded by
  \begin{align*}
    \xi_c(M, N, \tau) = \max_{0 \leq c \leq M-\tau} \left\{\frac{\binom{M-c}{\tau}}{\binom{M}{tau} \cdot N^{M-\tau-c}} \right\}
  \end{align*}
\end{lemma}

To re-iterate, we set out to make it so that the communication complexity of the ZK MPCitH protocol was not bounded by $N$.

\todo{Explain how C\&C gives us the smaller proof in MPCitH}

\subsubsection{ZK from sacrificing}\label{sec:zk-sacrifice}

\todo{Explain changes from C\&C to this from Baum how it increases soundness for smaller $n$}

\subsection{Linear Secret Sharing Schemes (LSSS)}\label{sec:lsss}

\todo{Go through Feneuilis threshhold paper and desribe their gains from using LSSS}
\todo{Move specification of LSSS to SSS section}

\textit{Secret sharing schemes} (SSS) are a type of cryptographic protocol that allows for the distribution of a secret amongst a group of participants. The secret can only be reconstructed when a sufficient number of shares are combined together. The threshold variant of the SD-in-the-Head protocol relies on a low-threshold linear secret sharing scheme (\textit{LSSS}). Threshold secret sharing schemes allow for the reconstruction of a secret from a subset of shares of length $\ell$, where $\ell$ is the threshold. The threshold allows for the SD-in-the-Head protocol to be more communication efficient, as the amount of shares needed to reconstruct the secret is low.

\begin{definition}\label{def:sss}
  \textit{$S$ is a $(\ell,n)$ threshold SSS if it satisfies the following properties}:

  \begin{itemize}
    \item \textbf{Share generation}: Given a secret $s$, the scheme generates seeds $sd_{e,i}$ for each $i \in [N]$ using $\Pi_C$. $n$ shares $\texttt{share}(s) = \sh{s} = [s_1, s_2, \dots, s_n]$.
    \item \textbf{Reconstruction}: Given a subset of shares $\sh{s'}$ of size $\ell$, the scheme can reconstruct the secret $s = \texttt{open}(\sh{s'})$.
  \end{itemize}

\end{definition}

\textit{Linear secret charing schemes}, or $(+,+)$-homomorphic schemes, refers to secret sharing schemes that are linearly homomorphic over some field $\mathbb{F}$ (say the galois field \texttt{GF256} described in \autoref{sec:gf256}). This means that given shares $\sh{a}$ and $\sh{b}$ we have that
\begin{definition}
  A $(\ell,n)$ threshold SSS is $(+,+)$-homomorphic if for any two secrets $s_1$ and $s_2$ and their shares $[[s_1]]$ and $[[s_2]]$, the sum of the shares $[[s_1]] + [[s_2]] = [[s_{11} + s_{21}, s_{12} + s_{22}, \dots, s_{1n} + s_{2n}]]$ is equal to the share of the sum of the secrets $[[s_1 + s_2]]$ for the same subset of shares.
\end{definition}

\section{Fiat-Shamir Heuristic}\label{sec:fiatshamir}
To transform any zero-knowledge protocol into a signature scheme, one can use the approach described in~\cite{fiat1986prove}. Here, we outline the general concept.

Zero-knowledge protocols typically rely on the verifier to issue a challenge to the prover. This challenge serves as a source of randomness that the prover is not supposed to control. In the Fiat-Shamir framework, the original protocol is based on the problem of factoring integers -- a problem that is now vulnerable to quantum attacks, specifically Shor's algorithm. However, the fundamental method of converting a zero-knowledge protocol into a signature scheme remains unaffected.

To adapt a zero-knowledge protocol into a signature scheme, the prover eliminates the need for a verifier to provide randomness. Instead, the prover generates seeds $sd_{e,i}$ for each $i \in [N]$ using $\Pi_C$.the randomness using a pseudo-random function. This function takes as input the message to be signed along with some random values generated by the prover. The output of this function serves as the challenge. With this, the prover can compute the required values to prove authenticity without external interaction.

In the context of the SD-in-the-Head protocol, the prover uses the pseudo-randomly generated challenge to locally compute the witness values required for the proof. These witness values are derived through the MPC protocol, where the prover simulates the necessary computations internally. This eliminates the need for the verifier's active participation in the MPC protocol, streamlining the signature generation process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Specification}\label{ch:spec}

\todo{more detailed description of the algorithm
  e.g. how we sampled I[e] witness challenge
  table of spec params (with our code naming and different categories)}

For our implementation, we selected the \textit{threshold} variant of the SD-in-the-Head protocol~\cite{aguilarsyndrome11,feneuil2023threshold}, as this variant offers the most significant performance improvements compared to the initial protocol~\cite{feneuil2022syndrome} and the \textit{hypercube} variant~\cite{aguilarsyndrome11,aguilar2023return}, albeit at the cost of a slightly larger signature size.

In this chapter, we provide a specification of the initial SD-in-the-Head protocol, along with the details specific to the threshold variant. We include pseudo-code for the major subroutines and algorithms, as well as a comprehensive description of the parameters used in the protocol. Finally, we present an overview of the associated security guarantees.


\section{SD-in-the-Head Protocol}
The original SD-in-the-Head protocol is based on the computational hardness of the Syndrome Decoding (SD) problem for random linear codes over a finite field. As a reminder, problem is expressed as follows (see \autoref{sec:syndrome})
\begin{quote}
  For a parity check matrix $H \in \mathbb{F}_q^{(m-k)\times m}$ in standard form and a syndrome $y$. The challenge is to find a vector $x$ s.t. $Hx = y$ and $wt(x) \leq w$.
  \begin{align*}
    y = Hx = H'x_a + x_b \text{ \ \ \ where \ \ \ } H' \in \mathbb{F}_q^{(m-k)\times k}
  \end{align*}
  for $x = (x_a | x_b)$. We have the polynomial representation of the problem (see \autoref{sec:polynomial_representation})
  \begin{align*}
    S\cdot Q = P\cdot F \text{ \ \ \ where \ \ \ } S, Q, P, F \in \mathbb{F}_q[X]
  \end{align*}
\end{quote}
\noindent Testing that the relation holds has a small probability of a \textit{false positive}. We denote this as $p$. The protocol takes several steps to reduce $p$. First, the relation is evaluated at random points $\{r_k \in \mathbb{F}_q\}_{k\in[t]}$. Second, the field of the random points is extended from $\mathbb{F}_q$ to $\mathbb{F}_{q^4}$.

security

The SD-in-the-Head protocol is based on the \textit{Multi-Party Computation in the Head} (MPCitH) framework (see \autoref{sec:mpcinth}). \todo{Explain why we then use MPC and the linearity of polynomial evaluation with sacrificing random beaver triples}

\section{Field implementation}

\subsection{Field extension}

First extend $F_q$ to $F_{q^4} = F_q[Z] / (Z^2 + Z + 32(X))$. This is done by representing elements in the field as polynomials of degree at most 1 with coefficients in $F_q$. \todo{Define addition and multiplication}

\subsubsection{Polynomial evaluation}
\todo{Should this be moved to the specification section?}
For polynomial evaluation in the algorithm, seen in \autoref{sec:mpc}, we need to evaluate a polynomial $P(x)$ at a point $r \in F_q^\eta$. This is done by evaluating the polynomial at each coefficient and summing the results. The polynomial $P(x)$ is defined as
\begin{align}
  \textstyle\bigcup_{|Q|}(F_q)^{|Q|} \times F_q^\eta & \rightarrow F_q^\eta                 \nonumber                                                    \\
  Q(r)                                               & = \textstyle\sum_{i=0}^{|Q|} Q_i \cdot r^{i-1} &  & Q_i \in F_q, r \in F_q^\eta\label{eq:mpcpoly}
\end{align}

\todo{See gf256\_ext.rs}

\section{Merkle Tree Commitment}

\todo{Right now this section sounds more like a section in "Implementation". Instead it should describe the spec like their paper does with the addition of the getRevealedNodes method.}

We chose to follow the definitions for the merkle tree provided in \cite{aguilarsyndrome11}. However, we added a separate method that calculates bottom up the revealed nodes based on the view opening challenges. This is used to know the length of the authentication path that is added to the signature and is needed when parsing the signature for verification. The algorithm uses the queue structure which is also mentioned in the spec, where the tree structure is a 1D array with the last elements being the leaf. First it checks for empty input. Then we initiate an empty list to hold the revealed nodes. Now in order to traverse the tree we will get the $heightIndex$ and $lastIndex$, which are the current layer of the tree, and the index of the 1D array of where the current layer stops. Lastly we need a queue to hold the indexes. Now in order to find the revealed nodes it starts by adding the indexes of the selected leaves to the queue. Then we go through the queue until the next index is $1$ which indicates that we are at the root. In the while loop we first get the current index, and then check if we have gone up a layer in the tree, and if the current node is a left child. If the current node is a left child and it is the last index in the layer, then we can just skip to the next node because we know it has no sibling. If it is not the last node we get the next node as a candidate, if the queue is not empty. If the current node is still the left child, and the next node in the queue is the other child of the same parent we do not need to add this as a revealed node as it is part of the known values. But if the next node is not the right child of the current node we add it as a revealed node. If the current node is not the left child we add the left child to the revealed nodes. And then we add the parent index to the queue. When we exit the while loop we have traversed the tree bottom up adding for each layer the neighbours of the selected leaves. The algorithm is described in pseudocode in \autoref{alg:get-revealed-nodes}.

\todo{change last sentence}

\begin{breakablealgorithm}
  \caption{Get Revealed Nodes for Selected Leaves in Merkle Tree}\label{alg:get-revealed-nodes}
  \begin{algorithmic}[1]
    \Require $selectedLeaves$: List of selected leaf indices
    \Ensure List of revealed node indices

    \Function{getRevealedNodes}{$selectedLeaves$}
    \If{$selectedLeaves$ is empty}
    \State \Return empty list
    \EndIf

    \State $revealedNodes \gets \text{empty list}$
    \State $(heightIndex,\; lastIndex) \gets (1 \ll treeHeight,\; treeNodes - 1)$
    \State $queue \gets \text{empty queue}$


    \ForAll{$leaf \in selectedLeaves$} \Comment{Add commitments to the queue}
    \State $val \gets (1 \ll treeHeight) + leaf$
    \If{$\text{queue.add}(val)$ fails}
    \State \textbf{panic} ``Could not add element to queue''
    \EndIf
    \EndFor

    \While{$queue.peek() \neq 1$} \Comment{Do until we reach the root}
    \State $index \gets queue.remove()$

    \If{$index < heightIndex$}
    \State $heightIndex \gets heightIndex \gg 1$
    \State $lastIndex \gets lastIndex \gg 1$
    \EndIf

    \State $isLeftChild \gets (index \; \% \; 2 == 0)$

    \If{$isLeftChild \land index == lastIndex$}
    \Comment{Node has no sibling}
    \State \textbf{continue}
    \Else
    \State $candidateIndex \gets 0$
    \If{\textbf{not} $queue.isEmpty()$}
    \State $candidateIndex \gets queue.peek()$
    \EndIf

    \If{$isLeftChild \land (candidateIndex == index + 1)$}
    \State $queue.remove()$
    \ElsIf{$isLeftChild$}
    \State $revealedNodes.append(index + 1)$
    \Else
    \State $revealedNodes.append(index - 1)$
    \EndIf
    \EndIf

    \State $parentIndex \gets index \gg 1$
    \If{$queue.add(parentIndex)$ fails}
    \State \textbf{panic} ``Could not add element to queue''
    \EndIf
    \EndWhile

    \State \Return $revealedNodes$
    \EndFunction

  \end{algorithmic}
\end{breakablealgorithm}


\todo{Can we use Haraka v2}

\section{Hashing and XOF}

\todo{Write about Shake, Keccak} and how we initiate the hash function and XOF

\section{MPC computation}

The computation is based on HVZKAoK Protocol using imperfect preprocessing and sacrificing. Section 3.3~\cite{baum2020concretely}.

A toy example of the computation protocol computations, can be seen in~\autoref{fig:mpc}. We have the two computation methods. Here for 1 split and 1 evaluation point. This means that we have only value for each challenge and beaver triple. Note that any arithmetic is run in GF256, so addition and subtraction are both \texttt{XOR} and multiplication is modulus $x^8 + x^4 + x^3 + x + 1$. Furthermore, for negation we have that $-a = a$.

\begin{figure}
  \makebox[\textwidth]{
    \fbox{\begin{minipage}[t]{.45\textwidth}
        \noindent \texttt{PartyComputation}
        \begin{flalign*}
           & \textit{Input: }                                                               \\
           & (s_a, Q', P, a, b, c), (\overline{\alpha}, \overline{\beta}), (H', y)          \\
           & (\epsilon, r), \texttt{with\_offset}                                           \\
           & \textit{Output: }                                                              \\
           & (\alpha, \beta, v)                                                             \\\\
           & Q = Q'_1\text{ if \texttt{with\_offset} else }Q'_0                             \\
           & S = (s_a | y + H's_a) \text{ if \texttt{with\_offset} else } (s_a | H's_a)     \\
           & v = -c                                                                         \\
           & \alpha = \epsilon \cdot Q(r) + a                                               \\
           & \beta = S(r) + b                                                               \\
           & v \mathrel{{+}{=}} \epsilon \cdot F(r) \cdot P(r)                              \\
           & v \mathrel{{+}{=}} \overline{\alpha} \cdot b + \overline{\beta} \cdot a        \\
           & v \mathrel{{+}{=}} - \alpha \cdot \beta \text{ \ \ \ if \texttt{with\_offset}}
        \end{flalign*}
      \end{minipage}}
    \hfill
    \noindent
    \fbox{\begin{minipage}[t]{.45\textwidth}
        \noindent \texttt{InverseComputation}
        \begin{flalign*}
           & \textit{Input: }                                                                 \\
           & (s_a, Q', P), (\alpha, \beta, v), (\overline{\alpha}, \overline{\beta}), (H', y) \\
           & (\epsilon, r), \texttt{with\_offset}                                             \\
           & \textit{Output: }                                                                \\
           & (a, b, c)                                                                        \\\\
           & Q = Q_1\text{ if \texttt{with\_offset} else }Q_0                                 \\
           & S = (s_a | y + H's_a) \text{ if \texttt{with\_offset} else } (s_a | H's_a)       \\
           & c = -v                                                                           \\
           & a = \alpha - \epsilon \cdot Q(r)                                                 \\
           & b = \beta - S(r)                                                                 \\
           & c \mathrel{{+}{=}} \epsilon \cdot F(r) \cdot P(r)                                \\
           & c \mathrel{{+}{=}} \overline{\alpha} \cdot b + \overline{\beta} \cdot a          \\
           & c \mathrel{{+}{=}} - \alpha \cdot \beta \text{ \ \ \ if \texttt{with\_offset}}
        \end{flalign*}
      \end{minipage}}}
  \caption{Simplified version of the MPC party computation and inverse computation. $Q_0$ means that $Q$ is completed with a $0$ for leading coefficient. Furthermore, $F$ is precomputed. Note that all arithmetic is done in $\mathbb{F}_q = GF256$. All elements are in $\mathbb{F}_q^\eta$ except for the coefficients of $Q$, $S$ and $P$ which are in $\mathbb{F}_q$.}\label{fig:mpc}
\end{figure}
\bigskip

Note that the

If we first instantiate an input $i$ and one random input $i^*$ (like the \texttt{input\_coef}).Then the input share is generated by adding the two. Similar, but simpler, to the input share generation of Algorithm 12, line 13 of the specification.
\begin{align*}
  i                & = (s_a, Q, P, a,b,c)                                             \\
  i^*              & = ({s_a}^*, Q^*, P^*, a^*,b^*,c^*)                               \\
  \sh{i} = i + i^* & = ({s_a} + {s_a}^*, Q + Q^*, P + P^*, a + a^*, b + b^*, c + c^*) \\
                   & = (\sh{s_a}, \sh{Q}, \sh{P}, \sh{a}, \sh{b}, \sh{c})             \\
  \texttt{chal}    & = (\epsilon, r)                                                  \\
  \texttt{pk}      & = (H', y)
\end{align*}
We also compute the plain broadcast share of the input as per Algorithm 12, line 18. Note that $\overline{v}$ is computed to zero and therefore removed from the computation in the implementation.
\begin{align*}
  (\overline{\alpha},\ \overline{\beta})         & =
  \texttt{PartyComputation}(i,\ (\overline{\alpha}, \overline{\beta}),\ \texttt{chal},\ \texttt{pk},\ \texttt{true})                                                                                   \\\\
  \overline{\alpha}                         =    & \epsilon \cdot Q_1(r) + a                                                                                                                           \\
  \overline{\beta}                          =    & S_y(r)  + b                                                                                                                                         \\
  \overline{v}                            =      & -c + \epsilon \cdot F(r) \cdot P(r) + \overline{\alpha} \cdot b + \overline{\beta} \cdot a  - \overline{\alpha} \cdot \overline{\beta}              \\
  \overline{v}                            =      & -c + \epsilon \cdot F(r) \cdot P(r) + (\epsilon \cdot Q_1(r) + a) \cdot b + (S_y(r)  + b) \cdot a  - (\epsilon \cdot Q_1(r) + a) \cdot (S_y(r) + b) \\
  \overline{v}                            =      & -c + \epsilon \cdot F(r) \cdot P(r)                                                                                                                 \\
                                                 & + \epsilon \cdot Q_1(r) \cdot b + c + S_y(r) \cdot a + c                                                                                            \\
                                                 & - \epsilon \cdot Q_1(r) \cdot S_y(r) - \epsilon \cdot Q_1(r) \cdot b - a \cdot S_y(r) - c                                                           \\
  \overline{v}                                 = & \ 0
\end{align*}
We then compute a broadcast share from the randomness and the broadcast, as per Algorithm 12, line 21.
\begin{align*}
  (\alpha^*, \beta^*, v^*) & = \texttt{PartyComputation}(i^*, (\overline{\alpha},
  \overline{\beta}), \texttt{chal}, \texttt{pk}, \texttt{false})                  \\\\
  \alpha^*                 & = \epsilon \cdot {Q^*}_0(r) + a^*                    \\
  \beta^*                  & =  {S^*}_0(r) + b^*
\end{align*}
This broadcast share is sent to the verifier along with the truncated input share (removing the beaver triples). The verifier then needs to recompute the input share beaver triples using the \texttt{InverseComputation} function. First we add the input share to the broadcast share as per Algorithm 13, line 8.
\begin{align*}
  (\alpha', \beta', v') & = (\alpha^*, \beta^*, v^*) + (\overline{\alpha},\ \overline{\beta}, 0)
  = (\alpha^* + \overline{\alpha}, \beta^* + \overline{\beta}, v^* + 0)                          \\\\
  \alpha'               & = \epsilon \cdot {Q^*}_0(r) + a^* + \overline{\alpha}                  \\
                        & = \epsilon \cdot {Q^*}_0(r) + a^* + \epsilon \cdot Q_1(r) + a          \\
  \beta'                & = {S^*}_0(r) + b^* + \overline{\beta}                                  \\
                        & = {S^*}_0(r) + b^* + S_y(r) + b                                        \\
\end{align*}
Next, the verifier computes the inverse of the broadcast share to recompute $(\sh{a}, \sh{b}, \sh{c})$ using the \texttt{InverseComputation} function. This is done as per Algorithm 13, line 10.
\begin{align*}
  (a', b', c') & = \texttt{InverseComputation}(\sh{i}, (\alpha', \beta', v'),
  (\overline{\alpha}, \overline{\beta}), \texttt{chal}, \texttt{pk}, \texttt{true})                                                     \\\\
  a'           & = \alpha' - \epsilon \cdot {\sh{Q}}_1(r)                                                                               \\
               & = \epsilon \cdot {Q^*}_0(r) + a^* + \epsilon \cdot Q_1(r) + a - \epsilon \cdot {\sh{Q}}_1(r)                           \\
               & = \epsilon \cdot {Q^*}_0(r) - \epsilon \cdot {\sh{Q}}_1(r) + \epsilon \cdot Q_1(r) + \sh{a}                            \\
               & = \epsilon \cdot ({Q^*}_0(r) - {\sh{Q}}_1(r) +  Q_1(r)) + \sh{a}                                                       \\
               & = \epsilon \cdot (Q^*_0(r) +  Q^*_0(r)) + \sh{a}                                             &  & \autoref{eq:mpcpoly} \\
               & = \sh{a}                                                                                     &  & \autoref{eq:mpcpoly} \\
  b'           & = \beta' - {\sh{S}}_y(r)                                                                                               \\
               & = {S^*}_0(r) + b^* + S_y(r) + b - {\sh{S}}_y(r)                                                                        \\
               & = {S^*}_0(r) - {\sh{S}}_y(r) + S_y(r)  + \sh{b}                                                                        \\
               & = {S^*}_0(r) - S^*_0(r)  + \sh{b}                                                            &  & \autoref{eq:mpcpoly} \\
               & = \sh{b}                                                                                     &  & \autoref{eq:mpcpoly} \\
\end{align*}
\todo{Want to explain the above in a more detailed manner? Specifically ${Q^*}_0(r) - {\sh{Q}}_1(r) = Q_1(r)$}

\section{Security}

The security analysis of the SD-in-the-Head signature scheme is based on the proposed standardization requirements from the 2022 NIST call for proposals of non-lattice based signature schemes~\cite{nistcall}. Therefore, as a preliminary, we will describe the reasoning and requirements of the NIST standardization process.

With the development of new quantum algorithms and unknowns in the capacities of the future quantum computers, there remain large uncertainties in estimating the security of the algorithms. To combat these uncertainties, NIST proposed for the 2022 stadardization effort, to define the security of submissions in a range of five categories. Each, with an easy-to-analyze cryptographic primitive providing the lower bound for a variety of metrics deemed relevant to practical security. The SD-in-the-Head specification provides security parameters adhering to categories one, three and five.

\begin{definition}\label{def:nistsec}
  Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for key search on a block cipher with a 128-bit (e.g. AES-128), 192-bit (e.g. AES-192) and 256-bit (e.g. AES-256) for categories one, three and five respectively.
\end{definition}

In terms of quantum security, the complexity and capability of quantum algorithms are measured in terms of quantum circuit size, i.e. the number of quantum gates in the quantum circuit. In order to estimate the quantum security of the signature protocols, circuit size can be compared to the resources required to break the security of \autoref{def:nistsec}. Therefore,
according to the proposal by NIST, the SD-in-the-Head specification provides security metrics in terms of quantum circuit depth to optimal key recovery for AES-128, AES-192 and AES-256 for categories one, three and five respectively. These are estimated to be $2^{143}$, $2^{207}$ and $2^{272}$ classical gates~\cite{nistcall}.

\subsection{Security Definition}

The SD-in-the-Head signature scheme upholds the \textbf{E}xistential \textbf{U}n\textbf{f}orgeability under \textbf{C}hosen \textbf{M}essage \textbf{A}ttack (EUF-CMA) security property for digital signature schemes as this is the type of attack that NIST will evaluate signature proposals~\cite{nistcall,aguilarsyndrome11}. EUF-CMA works as the following game:

\begin{enumerate}
  \item The challenger generates seeds $sd_{e,i}$ for each $i \in [N]$ using $\Pi_C$.a key pair $(pk, sk)$ and sends $pk$ to the adversary.
  \item The adversary is then allowed to query a signing oracle for signatures of chosen messages $(m_1, \dots, m_r)$ and receives valid signatures $(\sigma_1, \dots, \sigma_r)$. For the NIST evaluation it is assumed that the adversary can query the signing oracle for up to $2^{64}$ chosen messages according to the adversary's running time. However, there is no requirement on the timing of the queries.
  \item The adversary then outputs a pair $(m^*, \sigma^*)$. The adversary wins if the following holds
        \begin{enumerate}
          \item $m^*$ has not been queried to the signing oracle.
          \item The pair $(m^*, \sigma^*)$ is a valid signature for $m^*$ under $pk$.
        \end{enumerate}
\end{enumerate}

\begin{table}[ht]\label{tab:secparam}
  \centering
  \def\arraystretch{1.5}%  1 is the default, change whatever you need
  \begin{tabular}{cccccccccccccc}
    \specialrule{.1em}{.05em}{.05em}
    \multicolumn{2}{c}{\textbf{NIST security}} &      & \multicolumn{5}{c}{\textbf{SD parameters}} &     & \multicolumn{5}{c}{\textbf{MPCitH Parameters}}                                                             \\ \cline{1-2} \cline{4-8} \cline{10-14}
    Category                                   & Bits &                                            & $q$ & $m$                                            & $k$ & $w$ & $d$ &  & N   & $\ell$ & $\tau$ & $\eta$ & $t$ \\ \hline
    \textbf{I}                                 & 143  & \textit{}                                  & 256 & 242                                            & 126 & 87  & 1   &  & $q$ & 3      & 17     & 4      & 7   \\
    \textbf{III}                               & 207  &                                            & 256 & 376                                            & 220 & 114 & 2   &  & $q$ & 3      & 26     & 4      & 10  \\
    \textbf{V}                                 & 272  &                                            & 256 & 494                                            & 282 & 156 & 2   &  & $q$ & 3      & 34     & 4      & 13  \\ \specialrule{.1em}{.05em}{.05em}
  \end{tabular}
  \caption{Security parameters for the SDitH protocol for categories one, three and five~\cite{aguilarsyndrome11}.}
\end{table}

\begin{table}[ht]\label{tab:hashparam}
  \centering
  \def\arraystretch{1.5}%  1 is the default, change whatever you need
  \begin{tabular}{clll}
    \specialrule{.1em}{.05em}{.05em}
         & \multicolumn{1}{c}{\textbf{I}} & \multicolumn{1}{c}{\textbf{III}} & \multicolumn{1}{c}{\textbf{V}} \\ \cline{2-4}
    Hash & SHA3-256                       & SHA3-384                         & SHA3-512                       \\
    XOF  & SHAKE-128                      & SHAKE-256                        & SHAKE-256                      \\ \specialrule{.1em}{.05em}{.05em}
  \end{tabular}
  \caption{Hash and XOF functions used in the SDitH protocol for categories one, three and five~\cite{aguilarsyndrome11}.}
\end{table}

\subsection{Assumptions}\label{sec:assumptions}
The SD-in-the-Head protocol is secure under the following assumptions:

Syndrome Decoding instances cannot be solved in complexity lower than $2^\kappa$ corresponding to the complexity of breaking AES by exhaustive search (see \autoref{def:nistsec}) in terms of quantum circuit size. For this, $\kappa$ is defined as $143$, $207$ and $272$ for the categories~\cite{nistcall}. Furthermore, the XOF primitive used is secure with 128-bit, 192-bit, 256-bit security levels for each of the categories respectively. Finally, the Hash function used \textit{behaves as random oracle}. Specifically, security holds in Random Oracle Model (ROM) and Quantum Random Oracle Model (QROM).

\subsection{Security of the Syndrome Detection Problem}\label{sec:sdsec}

Recall the definition of the syndrome detection problem from \autoref{def:syndrome}. The hardness of the SD problem is well established and the coset variant used in the SD-in-the-Head signature scheme, has been shown to be \textit{NP-complete}~\cite{berlekamp1978inherent,aguilarsyndrome11}. Furthermore, a brute force attack of guessing $x$ would require finding a unique correct solution in $\binom{m}{w} q^w$ which is infeasible for the parameters outlined in the specification. As an example, for category one, the number of possible solutions are $\approx 7.7 \times 10^{276}$. There exists more sophisticated algorithms for solving the SD problem, such as the Generalized Birthday Algorithms (GBA) and Information Set Decoding (ISD)~\cite{prange1962use}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}\label{ch:impl}

We set out to implement the SD-in-the-Head Protocol using a modern, well-maintained language that provides the necessary performance and security for a post-quantum secure signature scheme. To achieve this, we selected Rust~\cite{rustlangRustProgramming,nistsaferlanguages,lurklurkEffectiveRust,rustlangPerformanceBook} as the programming language. As a principle and to give ourselves the ability to manage the protocol effectively, we aimed to develop as much of the project's code as possible independently, minimizing reliance on pre-existing packages. This approach included implementing subroutines for the Galois Finite Field and Merkle Tree Commitment Scheme. However, for cryptographic primitives such as hash functions~\cite{blakethree,tinykeccak}, we utilized external libraries. This reliance is common, as developing such primitives securely often requires an entire dedicated effort and a whole project in itself.

This section provides a detailed overview of our implementation. We begin with an introduction to the Rust programming language, explaining our rationale for choosing it and highlighting specific features that are relevant to the subsequent sections. Following this, we present a brief walkthrough of each module in our implementation, detailing how the code aligns with the SD-in-the-Head specification. Additionally, we describe the methods used to test and optimize these modules and discuss the challenges encountered during their development.

\todo{For each section of code add following: What is it for (reference to spec section. What issues did we run into. Which Rust specific patterns are we utilizing. How are we testing it. )}

code sections
code re-usability with traits for categories.

compiling constants for categories

can we use other hashes that still provide the same security assumption from \autoref{sec:assumptions} as post-quantum security (Xoodyak, KangarooTwelve, Haraka v2)


\section{The Rust Programming language}\label{sec:rust}

Before delving into the specifics of our implementation, we need to answer the question: ``Why is Rust a good choice for our implementation?''.

In 2024, the government of the United States of America took a stance on the future of cyber secure programming languages~\cite{whitehouse2024memorysafe}. In their report, they underline the need for secure building blocks when developing secure software. They point to the fact that \textit{Common Weakness Enumeration} (CWE) data highlights \textit{memory safety vulnerabilities} (MSV) as one of the most pervasive classes of vulnerabilities. MSV's exploit how memory can be accessed, written, allocated, or deallocated in ways that are beyond the scope of the program. Common examples of programming languages that are vulnerable include C and C++, which are widely used due to their high performance. To prevent such vulnerabilities, the report emphasizes the importance of using programming languages that inherently provide memory safety and does not require the developers to manually ensure security.

The Rust programming language addresses memory safety through its unique \textit{borrow checker}~\autoref{sec:rustborrow}, which enforces strict rules on how memory is accessed and managed. In the same year as the White House's report, NIST also released their recommendations on \textit{Safer Languages}~\cite{nistsaferlanguages}, which highlighted Rust as a leading choice. Together, these endorsements underscore that Rust is well-suited for implementing a protocol that adheres to modern security standards.

In addition to its focus on safety, Rust benefits from an extensive library of community-maintained documentation and resources~\cite{rustlangRustProgramming,rustlangPerformanceBook,lurklurkEffectiveRust}. These resources provide invaluable support for developers, enabling them to better understand the language's features and effectively utilize its capabilities.

In this section we will give an overview of the key features of the Rust programming language that make it an ideal choice for this project as we aimed to build it robust by leveraging Rust's safety mechanisms, performance optimization capabilities, dynamic language features, and strong testing infrastructure.

\subsection{Cargo - Rust package manager}\label{sec:cargo}
A Rust program, often referred to as a \textit{crate}, is typically compiled using the Rust Compiler with a command such as \texttt{rustc hello.rs}. However, manually managing versions and dependencies can be tedious and error-prone. To simplify this process, Rust provides a \textit{package manager} called Cargo~\cite{rustlangCargo}. 

Cargo is a command-line tool -- run through \texttt{cargo} -- that facilitates the management of dependencies, building, testing, and benchmarking of your crate, as well as generating documentation. It is the officially recommended method for managing and building Rust projects.


\subsection{Memory safety}\label{sec:rustborrow} % mut references
Rust guarantees both type safety (\autoref{sec:rusttypes}) and memory safety. While we will not delve into the details, significant progress has been made toward formal proofs of these guarantees~\cite{jung2017rustbelt}. Unlike many other languages that rely on sophisticated garbage collection mechanisms to ensure memory safety, Rust avoids the associated performance overhead through two key features: the \textit{ownership} and \textit{lifetimes}. Along with a compile time functionality called the \textit{borrow checker}, these features ensure the memory safety of Rust programs. While the borrow checker is often the biggest hurdle for new Rust developers, it is also the feature that makes Rust so powerful.

In this section, we will explore the fundamental aspects of how the borrow checker and ownership work. Only the most critical aspects will be discussed, as more comprehensive information is available in the official documentation~\cite{rustlangRustProgramming}.

\subsubsection{Data races vulnerability}
In the section, we consider the problem of \textit{data races}. These are similar to a race condition and cause undefined behavior that in the end can lead to security vulnerabilities. They occur when
\begin{enumerate}[parsep=0pt, itemsep=0pt]
  \item Two or more pointers access the same memory location concurrently.
  \item At least one of the pointers is being used to write to the memory location.
  \item The is no mechanism to synchronize the data access
\end{enumerate}
In Rust, the borrow checker ensures that no data races can occur and it diagnoses them at compile time.

\subsubsection{The Stack and Heap}
First, it is crucial to first grasp the distinction between the stack and the heap. These are two memory management systems available at runtime~\cite[ch.4]{rustlangRustProgramming}, each with unique characteristics and use cases.

\begin{definition}[The Stack]
  The stack stores values in a last-in-first-out (LIFO) order, meaning the last value added is the first to be removed. You can think of it like a stack of thermal detonators in a crate: the last detonator placed in the crate is the first one you grab when you need it. All data stored on the stack must have a fixed, known size at compile time. This gives the stack its \textit{fixed-sized} structure.
\end{definition}

\begin{definition}[The Heap]
  The heap, in contrast, is more flexible. When you allocate memory for a value on the heap, you request a specific amount of memory, and the heap manager finds a suitable location for it. A helpful analogy for the heap is docking a spaceship on Coruscant: you tell the spaceport how large your ship is, and they assign you to a landing pad that fits your ship's dimensions, noting where you've been docked.
\end{definition}

\subsubsection*{Performance versus Dynamism}

Between the two systems, there exists a \textit{performance versus dynamism} trade-off. Consider the following code example:

\begin{minted}{rust}
  let a: Vec<u8> = vec![1, 2, 3];
  let b: [u8; 3] = [1, 2, 3];
\end{minted}

This code demonstrates two data types being allocated. The first, \rust{a}, is a vector type, while the second, \rust{b}, is a fixed-size array type. The vector type is always allocated on the heap, whereas the fixed array is allocated on the stack. Pushing  and accessing the stack is faster than for on the heap. Therefore, keeping values on the stack tends to outperform heap allocations. This is because heap allocation involves searching for a suitable location in memory to store the data. Accessing data on the heap is also slower as you have to follow a pointer. Note that there are several optimisations you can do when allocating to the heap, like changing the allocator for specific use cases, instantiating the vector with a capacity or the \rust{SmallVec} that dynamically changes from the stack to the heap when allocation surpasses its defined capacity. See~\cite{rustlangPerformanceBook} for more.

However, the vector type is far more dynamic. For instance, using the \rust{push} method, you can add more elements to a vector, and the allocator will automatically find additional space on the heap if needed. Conversely, stack allocations in Rust require that the size of the data be known at compile time, which imposes significant restrictions on how dynamic your code can be. This limitation became particularly evident when designing around the different category variants in \autoref{sub:categories}. However, this also ensures at compile time that you cannot access memory outside of the stack, a common vulnerability exposed in C and C++.

Additionally, the stack, due to its fixed-size structure, can only hold a limited amount of data. Attempting to allocate more memory than the stack can accommodate results in a \textit{stack overflow} error. In contrast, the heap offers virtually unlimited storage, as it can dynamically allocate memory as required.

\subsubsection{Ownership and Variable scope}
Now that we have given a brief introduction to the memory allocation, we can delve into the \textit{ownership system}. This system is a crucial aspect of Rust's memory management, as it ensures that memory is allocated and deallocated correctly. Ownership in Rust follows three rules
\begin{enumerate}[parsep=0pt, itemsep=0pt]
  \item Each value has an \textit{owner}
  \item There can only be one owner at one time
  \item When a owner goes out of scope, the value will be \textit{dropped}.
\end{enumerate}
First, we consider the \textit{variable scope} in the following example
\begin{minted}{rust}
{
  // d can not be accessed yet because it has not been assigned to the current scope
  let d = "Peace is a lie";
  println!("{d}") // Now we can use d
} // d is dropped here
\end{minted}
In this example, the variable \rust{d} comes into the scope and remains valid until the scope is closed. This is similar to most other programming languages. This works because the string literal \rust{"Peace is a lie"} is of known size and is allocated on the stack, meaning that it can be easily copied on stack and popped when the current scope ends. It also means that while it is efficient and fast, the variable \rust{d} cannot be mutated after the fact.

\subsubsection*{Variable and data interaction with \textit{Move}}

To illustrate where Rust differs, we need a data type that is more complex and stored on the heap instead. We will change the example to use a \rust{String} type
\begin{minted}{rust}
  let mut d = String::from("Peace is a lie");
  d.pust_str(", there is only passion."); // We can mutate d
  println!("{d}"); // prints "Peace is a lie, there is only passion."
\end{minted}
A variable instantiated with the \rust{String} type is dynamically allocated on the heap, and a \textit{pointer} is instead saved on the stack. Now consider both types of assignments with a \textit{re-assignment}
\begin{minted}{rust}
  // Case 1: Stack allocated type
  let d = "Through passion, I gain strength"
  let v = d;

  // Case 2: Heap allocated type
  let d = String::from("Through strength, I gain power");
  let v = d;
\end{minted}

For Case 1, if the string literal \rust{d} implements the \textit{Copy} trait. This means that when we assign \rust{let v = d}, its value can copied onto the stack with a negligent performance hit. This means \rust{d} remains accessible after being used or assigned elsewhere because its data was duplicated. Due to the immutability and fixed size of the string literal, this operation is also memory safe.

However, for Case 2; The variable \rust{d} is instead a \textit{pointer} to a heap-allocated string, it resides on the stack but points to data stored on the heap. When \rust{d} is moved to another variable (e.g., \texttt{v}), the pointer itself is transferred to \texttt{v} and the heap-allocated data remains unchanged. Ownership of the pointer is moved to \texttt{v}. This is called a \textit{move}. After the move, \rust{d} is no longer valid, and any attempt to use it would result in a compile-time error.
\begin{minted}{rust}
  let d = String::from("Through strength, I gain power");
  //  ^ move occurs because `d` has type `String`, which does not implement the `Copy` trait
  let v = d;
  //      ^ value moved here
  println!("{d}"); 
  //        ^^^ value borrowed here after move
  // error[E0382]: borrow of moved value: `d`
\end{minted}

\subsubsection*{Why the Restriction?}

Allowing \rust{d} to be used after the move could lead to undefined behavior. For one, due to the variable scoping, the runtime would drop the data for both \rust{d} and \rust{v}. As these would be pointing to the same data, this would case a \textit{double free} memory safety bug. Furthermore, if multiple pointers were allowed to simultaneously access or modify the heap-allocated data, it could result in data races. To avoid such risks and ensure memory safety, Rust enforces its ownership rules. If you need to continue using \rust{d} after a move, you must explicitly \textit{clone} the heap data before transferring ownership. This approach encourages developers to carefully manage how data is accessed and modified, promoting safe and predictable memory usage.

\subsubsection{Functions and ownership}\label{sec:rustlifetimes} % (fold)
Now that we have covered the basics of ownership, we explore how it interacts with functions. When passing a value to a function, the function takes ownership of the value. This means that the function takes over managing the \textit{lifetime} of the value, ensuring that it is not used after the function call, unless it is copied.
\begin{minted}{rust}
  // Case 1: Stack allocated type
  fn makes_copy(some_integer: i32) { // some_integer comes into scope
    println!("{some_integer}");
  } // Here, some_integer goes out of scope. Nothing special happens.

  // Case 2: Heap allocated type
  fn takes_ownership(some_string: String) { // some_string comes into scope
    println!("{some_string}");
  } // Here, some_string goes out of scope and `drop` is called.
    // The memory on the heap is freed.
\end{minted}

In Case 1, again the structure of the stack and known compile time size, nothing special needs to happen. However, in Case 2, the function \rust{takes_ownership} takes ownership of the \rust{String} value \rust{some_string}. When the function ends, the \rust{String} value is dropped, and the memory on the heap is freed. This is because the function takes ownership of the value, and the value is no longer valid after the function call.

However, a function can also take ownership of a value. Here's an example:
\begin{minted}{rust}
  fn main() {
    let d = String::from("hello");
    let (v, len) = calculate_length(d);
    println!("The length of '{v}' is {len}."); // v is valid
    println!("The length of '{d}' is {len}."); // d is not valid
    //                        ^ value borrowed here after move
    // error[E0382]: borrow of moved value: `d`
  }

  fn calculate_length(s: String) -> (String, usize) {
    let length = s.len();
    (s, length)
  }
\end{minted}
The function \rust{fn calculate_length()} takes a pointer and the ownership of the \rust{String} value \rust{d}. It returns the same pointer and ownership of the value along with the computed value. Notice that the ownership of the value works the same as it did before with variable assignment. The function \rust{calculate_length} takes ownership of the value, and \rust{d} is no longer valid after the function call.


\subsubsection{Borrowing with references}
However, often it is tedious to keep transferring ownership of values, whenever they need to be used in a function. So how can we avoid this boilerplate of returning ownership? Rust provides a way of avoiding this transfer of ownership by using \textit{references}. References allow you to \textit{borrow} the value without taking ownership of it. This is done by using the \rust{&} symbol. Consider the example
\begin{minted}{rust}
  fn main() {
    let d = String::from("hello");
    let len = calculate_length(&d);
    println!("The length of '{d}' is {len}."); // d is valid
  }

  fn calculate_length(s: &String) -> (String, usize) {
    //                   ^ the function takes a reference to a String with the `&` symbol
    s.len()
  }
\end{minted}

In this way, the function variables \rust{s} never takes ownership of \rust{d}, meaning that it is not dropped when the function call ends. We call the action of creating a reference \textit{borrowing}.

\subsubsection{Mutable references}
However references have their limitations. The reference is immutable, meaning that we cannot modify \rust{s}. If we wanted to modify the value, we would need to use a \textit{mutable reference}.

\begin{minted}{rust}
  fn main() {
    let mut d = String::from("hello");
    modify(&mut d);
  }
  
  fn modify(s: &mut String) {
      s.push_str(", world");
  }
 \end{minted}

With the \rust{&mut} annotation we tell the compiler that we want to modify the value. This is a powerful feature, but it comes with restrictions. If you have a mutable reference to a value, you can have no other references to that value to prevent data races.
\begin{minted}{rust}
  let mut s = String::from("hello");

  let r1 = &s;
  //       ^^^^^^ immutable borrow occurs here
  let r2 = &mut s;
  //       ^^^^^^ mutable borrow occurs here

  println!("{}, {}", r1, r2);
  //                 ^^ immutable borrow later used here
  // error[E0502]: cannot borrow `s` as mutable because it is also borrowed as immutable
\end{minted}
The compiler eases these restrictions as it can determine the lifetime of the references. Consider the following example
\begin{minted}{rust}
  let mut s = String::from("hello");

  let r1 = &s; // no problem
  let r2 = &s; // no problem
  println!("{r1} and {r2}");
  // variables r1 and r2 will not be used after this point

  let r3 = &mut s; // no problem
  println!("{r3}");
\end{minted}
The last borrow is valid because the previous borrows are no longer in scope. The compiler can determine that the borrows will not be used after the last borrow.

\subsubsection{Lifetimes}
The \textit{lifetime} of a value on the stack refers to the period during which the value is guaranteed to remain valid within its scope. In most cases, developers do not need to actively manage lifetimes, as the compiler can infer them automatically. A value's lifetime begins when it is assigned ownership and ends either when it is dropped or when its ownership is transferred.
\begin{minted}{rust}
  let r: &Item;
  {
    let item = Item { contents: 42 };
    //  ^^^^ binding `item` is declared here
    r = &item;
    //  ^^^^^ borrowed value does not live long enough
  }
  println!("r.contents = {}", r.contents);
  //                          ^^^^^^^^^^ borrow later used here
  // error[E0597]: `item` does not live long enough
\end{minted}
While the Rust compiler, most often can infer the lifetime of a value it can sometimes be necessary to explicitly define the lifetime. This is done by using the \textit{lifetime annotation} \rust{'a}. This is most often used when working with references in structs or functions.
\begin{minted}{rust}
  pub fn find(haystack: &[u8], needle: &[u8]) -> Option<&[u8]> {
    //                  -----          -----            ^ expected named lifetime parameter
    // ...
  } // error[E0106]: missing lifetime specifier
\end{minted}
In this example, there are two choices for the output lifetime for the reference returned by the function. The compiler cannot infer which lifetime to use, so it requires the developer to specify it. This is done by adding the lifetime annotation to the function signature.
\begin{minted}{rust}
  pub fn find<'a, 'b>(haystack: &'a[u8], needle: &'b[u8]) -> Option<&'a[u8]> {
    // ...
  }
\end{minted}
In this way, the compiler can determine that the lifetime of the output reference is the same as the input reference \rust{haystack}.

\subsubsection{Conclusions on Memory Safety in Rust}
This overview only scratches the surface of the power and features that Rust offers. However, it provides a brief introduction to the core features that make Rust a powerful programming language for developing secure and high-performance software. As mentioned, Rust comes with a steep learning curve, but it is supported by a large and active community that offers extensive documentation and resources. We suggest that the reader explore the official Rust Book~\cite{rustlangRustProgramming} or its community made books, like \textit{Effective Rust}~\cite{lurklurkEffectiveRust} and \textit{The Rust Performance book}~\cite{rustlangPerformanceBook}.

\subsection{Correctness (Types, Testing and Errors)}
When creating and iterating on software, it is crucial that you develop in a way that you can continually deliver functional and correct code. There are many patterns that help ensuring this. Rust provides several. In this section we will describe two -- Types and Automated tests.

\subsubsection{The Type System}\label{sec:rusttypes}
Every value in Rust has a \textit{data type}. Programs written in Rust are \textit{statically typed}, meaning the type of each value must be determined at compile time. The type informs the compiler about what operations the value supports and helps catch errors before they reach runtime~\cite[ch.3.2]{rustlangRustProgramming}. Rust's compiler often infers types automatically, but if it cannot, you must explicitly annotate the type of the value. Consider the following example:

\begin{minted}{rust} 
  let guess: u32 = "42".parse().expect("Not a number!"); 
\end{minted}

Here, the \rust{parse} function requires the annotation of the variable \rust{guess} to determine how the string should be parsed. Additionally, if you annotate \rust{guess} with a type that does not implement the \rust{parse} function for strings, the compiler will produce an error. This strict type system ensures that Rust minimizes the possibility of writing incorrect code -- at least in terms of the bounds of the language. While Rust enforces correctness in type usage, ensuring that a program operates semantically as intended, such as adhering to a protocol specification, often requires additional validation methods.

\subsubsection{Automated Testing}
Say that you are implementing an arithmetic field -- \textit{funny enough, we needed to do exactly that!} -- you would want your impletation to follow the properties that define the field. For example \textit{associativity over addition}
\begin{align}
  a + (b + c) = (a + b) + c
\end{align}
Such a property is hard to enforce using the type system. However, it is possible to enforce it using \textit{automated testing}. Consider the following examples
\begin{minted}{rust}
  // gf256.rs

  fn gf256_add(a: u8, b: u8) -> u8 {
    a ^ b
  }

  #[cfg(test)]
  mod arithmetic_tests {
    use super::*
    
    #[test]
    fn test_add_associativity() {
      let a = 3;
      let b = 4;
      let c = 5;

      assert_eq!(
        gf256_add(gf256_add(a, b), c), 
        gf256_add(a, gf256_add(b, c))
      );
    }
  }
\end{minted}
This is a simple example of the features that Rust provides that to support automated testing. First we create an internal module \rust{mod arithmetic_tests}. Note the annotation, \mintinline{rust}|#[cfg(test)]|. This ensures that the module is only included when running tests and not in the final production code.

Individual tests are made using the \mintinline{rust}|#[test]| and a function. Then we can use the macro \rust{assert_eq!()} to test whether that our property is upheld.

To run the tests in our implementation you simply run the following
\begin{minted}{bash}
  $ cargo test
  running 1 test
  test gf256::arithmetic_tests::test_add_associativity ... ok
\end{minted}
Crucially, this test allows us to continue developing or optimising the finite field while rerunning the tests to verify that the property remains valid. The more rigourous we test our function, the easier we can debug issues or bugs that may arise.

Writing good tests is a subject extensive enough to warrant its own discussion, and delving into the theory behind it is beyond the scope of this report. From the outset, we designed our implementation with testing in mind. This approach ensures that we can iteratively develop each module and confirm that it adheres to theoretical expectations and the given specification.

\subsubsection{Error handling}\label{sec:rusterror} % https://www.lurklurk.org/effective-rust/panic.html
Testing that functions return the correct values based on \textit{correct} inputs are a crucial part of ensuring correctness. But what happens if you have unexpected inputs? For example on the inputs a user to a CLI\@? This can lead to errors. So how do Rust allow us to deal with errors? There are two ways:
\begin{itemize}
  \item Using a \rust{panic}: Stopping the execution of the program.
  \item Returning a \rust{Result<T, Err>}: A type that wraps the return value with an error value allowing the caller to handle the error.
\end{itemize}
So when is it appropriate to use a panic? The Rust book~\cite[ch.9.3]{rustlangRustProgramming} provides a good guideline for when to use panic.
The general rule is that, if your code would end up in a bad state from which it can not recover it should panic. Everywhere else, you should return a \rust{Result} type. In this way the caller can handle the error in the appropriate way for their use case.
\begin{minted}{rust}
  fn sign(message: &[u8], secret_key: &[u8]) -> Result<Vec<u8>, Error> {
    if secret_key.len() == 0 {
      return Err(Error::SigningError);
    }
    ... code that signs the message ...
    Ok(signature)
  }

  fn main() {
    let message = b"Hello, World!";
    let sk = b"...";
    let signature = sign(&message, &sk);

    // You can match on the result to handle the error
    match signature {
      Ok(signature) => println!("Signature: {:?}", signature),
      Err(e) => eprintln!("Error: {:?}", e),
    }

    // or you can use the methods supplied by the Result type
    if (signature.is_ok()) {
      println!("Signature: {:?}", signature.unwrap());
    } 
  }
\end{minted}
There might be a number of reasons why a signature could not be generated, but a simple one that we want to relay to a potential user is that the secret key is invalid. In this case, we would return an error with the \textit{SigningError} variant. If a panic was used the caller of the sign function would not be able to recover from the error.

\subsection{Maintainable and readable code}
One of the reasons for choosing Rust is that it provides several tools and features that make it easier to write code that is both maintainable and readable. In this section, we will discuss some of the features that we utilized in our implementation to achieve this goal.

\subsubsection{Code Documentation}\label{sec:code-documentation} 
First things first, good written should either written in a way so that it is self-explanatory or supply the necessary documentation in order for the reader to quickly grasp the concepts and functionality of the code. Often times you would want to supply documentation in the form of comments in the code, above functions and values to explain what they do and how they work. Furthermore, you would want to supply the user with a \textit{manual} where the user can read about the exposed library.

It can be hard to maintain both of these aspects separately. Luckily, Rust enables the developer to write both at the same time. Consider the following code in a module

\begin{minted}{rust}
  //! This module exposes two functions: add and double

  /// Adds two functions together
  pub fn add(a: u8, b: u8) -> u8 { 
    a + b
  }

  /// Doubles a number using [`add`]
  pub fn double(a: u8) -> u8 {
    add(a, a)
  }
\end{minted}

Adding a triple slash \rust{///} above a function or struct generates documentation for it. Hovering over the function in most Rust-compatible editors, such as \href{https://code.visualstudio.com/docs/languages/rust}{Visual Studio Code}, displays the documentation. Similarly, using \rust{//!} creates top-level documentation for the module.

While these comments enable developers to access documentation directly within the code, Rust also provides a powerful feature to automatically generate comprehensive documentation. By running \href{https://doc.rust-lang.org/cargo/commands/cargo-doc.html}{\texttt{cargo doc}}, you can create a webpage with detailed documentation for the project. The generated documentation for our implementation can be viewed here: \todo{link to the documentation}.

\subsubsection{Modules}
Rust provides a module system that allow you to organize your code into logical units. Making it it easier to understand and maintain the codebase. A module is a collection of related items, such as functions, structs, and constants, that are grouped together.

Modules can be nested, allowing for a hierarchical structure of code.
If utilized correctly, this can help organize the logic of a program effectively for readability and reuse. For example, encapsulating field arithmetic within a module and separating components into subfolders and subroutines related to the main signature algorithm.

Rust structures modules either directly in the code or through the file system. There are two files that provides the entry points to your code.
\begin{enumerate}
  \item \texttt{main.rs}: This is the main entry point of the program. It is the first file that is executed when the program is run. You can quickly run the program by running \texttt{cargo run}.
  \item \texttt{lib.rs}: In the case that you are building code that you want to provide for other developers to use in their own Rust code -- named a \textit{crate} -- this is the entry point for such a library.
\end{enumerate}
We do not utilize internal modules except for tests so we will only describe the file system module system. Consider a simple example structure below
\begin{minted}{rust}
  // main.rs
  use crate::arith::gf256::add;

  mod arith

  fn main() {
    println!("{}", add(3, 4));
  }

  // arith/mod.rs
  pub mod gf256;

  // arith/gf256.rs
  pub fn add(a: u8, b: u8) -> u8 {
    a ^ b
  }

  #[cfg(test)]
  mod tests {
    ...
  }
\end{minted}
The example shows a structure with the main entry file which first declares a module \rust{mod arith}. The compiler then looks for the code of the module in three places
\begin{itemize}[parsep=0pt, itemsep=0pt]
  \item Inline with curly braces (e.g. like the \rust{mod tests {}})
  \item In the file \texttt{arith.rs}
  \item In the file \texttt{arith/mod.rs}
\end{itemize}
In this case, the module \rust{arith} is declared in the file \texttt{arith/mod.rs}. A nested submodule \rust{gf256} is then declared and found in the same recursive way. This structure allows for a clear \textit{separation of concerns} and makes it easier to understand the codebase and reuse code as modules.

\subsubsection*{Visibility} %pub(crate)
Code from modules can then be accessed using the path from the crate root using the \rust{use crate::arith::gf256::add} statement. However, Rust has a strict visibility system that restricts access to code based on the visibility of the item. By default, items are private to the module they are declared in. It is then possible to control the visibility of the code using the \textit{visibility modifier} \rust{pub}.
\begin{itemize}[parsep=0pt, itemsep=0pt]
  \item \rust{pub}: The item is fully accessible from outside the module.
  \item \rust{pub(crate)}: The item is accessible only within the same crate.
  \item \rust{pub(super)}: The item is accessible only within the parent module.
  \item \rust{pub(in path)}: The item is accessible only within the specified path.
\end{itemize}
This system provides fine-grained control over the visibility of code, making the codebase easier to understand and maintain. Furthermore, it simplifies the process of restricting unwanted access to internal functionality. For instance, consider a scenario where a function conditionally selects between two different implementations based on a program configuration. In such cases, you would want to restrict access to the individual implementation functions, exposing only the function responsible for making the choice. This ensures that the internal functions cannot be accessed accidentally, thereby reducing the likelihood of bugs.
\begin{minted}{rust}
  pub fn functionality_handler() {
    if cfg!(feature = "functionality_one") {
      functionality_one();
    } else {
      functionality_two();
    }
  }

  fn functionality_one() // private function
  fn functionality_two() // private function
\end{minted}

\subsubsection{Generics and Traits}

When implementing a program, it is common to encounter functionality that you want to reuse. However, due to the type system or the program's structure, you may end up duplicating code. For example, the authors of the SD-in-the-Head protocol define their implementation with two field variants -- \rust{GF(256)} and \rust{GF(251)} -- as well as three NIST categories. In their NIST submission, they create separate codebases for each variant, resulting in multiple copies of the protocol. This duplication is compounded by an additional $2\times$ copies for the hypercube variant:

\begin{minted}{bash}
$ cd ~/Repos/datalogi/sdith/Reference_Implementation/Threshold_Variant 
$ ls
sdith_threshold_cat1_gf256/  sdith_threshold_cat3_p251/
sdith_threshold_cat1_p251/   sdith_threshold_cat5_gf256/
sdith_threshold_cat3_gf256/  sdith_threshold_cat5_p251/
\end{minted}

Each codebase contains significant shared code. However, to switch between the variants and arithmetic fields, copies of the code with necessary modifications are maintained. While it is unclear whether the authors utilize an automated system to ensure consistency across these instances, such duplication is a common issue in programming and indicates that the codebase may not be as maintainable as it could be.

Rust addresses this problem with two powerful features: \textit{Generics} and \textit{Traits}.

Consider the following example:

\begin{minted}{rust}
fn add(a: u8, b: u8) -> u8 {
    a + b
}

fn main() {
    let a = 3u8;
    let b = 4u8;
    println!("{}", add(a, b));

    let c = 3u16;
    let d = 4u16;
    println!("{}", add(c, d)); // We get an error here
}
\end{minted}

In this example, the type system prevents calling the same function with different types. To handle \rust{u16}, you would need to duplicate the function, creating \rust{add_u8} and \rust{add_u16}, leading to issues such as increased maintenance complexity. For instance, if one function is updated, all others must be updated manually.

Rust solves this problem with \textit{Generics}. Generics enable defining functionality that is agnostic to the input type while maintaining Rust's strong type safety. The function \rust{add} can be rewritten as:

\begin{minted}{rust}
fn add<T>(a: T, b: T) -> T {
    a + b
}
\end{minted}

Here, the function \rust{add<T>} includes the type parameter \rust{T}, allowing the compiler to infer the type based on the inputs. This enables calling the function with different types. However, the rewritten function would still produce an error, as the compiler cannot guarantee that the input types support the \rust{+} operator. This is where \textit{Traits} come into play.

\subsubsection*{Traits}
Traits provide a way to define shared behavior in an abstract manner, similar to interfaces in other programming languages -- e.g TypeScript. They allow you to specify a set of methods that a type must implement. For example, in Rust, we can restrict a generic type \rust{T} to only those types that implement the \rust{std::ops::Add} trait, which defines the \rust{+} operator. Consider the following function definition:

\begin{minted}{rust}
use std::ops::Add;

fn add<T: Add<Output = T>>(a: T, b: T) -> T {
    a + b
}
\end{minted}

This function can now be called with any type that implements the \rust{Add} trait. For instance, both \rust{u8} and \rust{u16} implement the \rust{Add} trait by default, enabling the creation of generic, reusable code.

Traits also allow you to define custom reusable behavior. For example, consider defining arithmetic operations over finite fields $GF(256)$ and $GF(251)$. Suppose we want a function that calculates a polynomial over a field and works dynamically for both fields, as well as others in the future. We can achieve this by defining a trait to encapsulate the behavior of a field and then implementing the trait for each specific field. Here's an example:

\begin{minted}{rust}
// arith.rs
pub trait FieldArith {
    fn add(a: Self, b: Self) -> Self;
    fn mul(a: Self, b: Self) -> Self;
    fn pow(mut a: Self, b: u8) -> Self { // default implementation
        for _ in 0..b {
            a = Self::mul(a, a);
        }
        a
    }
}

// arith/gf256.rs
struct GF256(u8);
impl FieldArith for GF256 {
    fn add(a: Self, b: Self) -> Self {
        GF256(a.0 ^ b.0)
    }
    fn mul(a: Self, b: Self) -> Self {
        // implementation ...
    }
}

// arith/gf251.rs
struct GF251(u8);
impl FieldArith for GF251 {
    // implementation ...
}

// main.rs
fn polynomial<T: FieldArith + Default>(coeffs: &[T]) -> T {
    let mut result = T::default(); // The `Default` trait
    for (i, coeff) in coeffs.iter().enumerate() {
        result = T::add(result, T::mul(*coeff, T::pow(T::default(), i as u8)));
    }
    result
}

fn main() {
    let coeffs256 = vec![GF256(3), GF256(4), GF256(5)];
    println!("Field 256: {:?}", polynomial(&coeffs256));
    let coeffs251 = vec![GF251(3), GF251(4), GF251(5)];
    println!("Field 251: {:?}", polynomial(&coeffs251));
}
\end{minted}

Note that with Traits we can define default implementations for functions. For example, the \rust{pow} function is defined in the trait directly. This makes it so that we only need to implement the function for the fields that have a different implementation. With the trait implemented, the \texttt{polynomial} function only has to be defined once and can be used for both fields. This approach promotes code reuse and simplifies future program expansions. To add support for a new field, you only need to implement the \rust{FieldArith} trait for that field, allowing the existing functions to handle it seamlessly.

\subsubsection*{Conclusions on Maintainability and Readability}
Rust offers a range of features that make it easier to write maintainable and readable code. In-built linting and documentation facilitate the creation of well-documented code. The module system enables developers to organize code into logical units while controlling visibility between them. Generics and Traits are powerful tools for writing more agnostic and reusable code across different parts of a program. 

However, while these features support good coding practices, they can be further enhanced by leveraging Rust's dynamic compilation features. In the next section, we will explore how these features were utilized to develop a more dynamic and optimizable implementation of the SD-in-the-Head protocol.

\subsection{Dynamic compilation}
In many applications, it is beneficial to have dynamic compilation of the code. This allows for code generation, linking and configuring the package before building the actual binary. As a good example, when creating a submission to NIST you need to have different security levels for each category. This poses a problem of having to change the code based on specific parameters. This is where Cargo build scripts and Rust's feature flags come in handy. 

\subsubsection{Build script}
Build scripts are Rust files executed before the package is compiled, offering significant flexibility in the build process. By putting a build.rs file in the root folder of the package cargo will automatically execute this file before building. Enabling you to generate code, link libraries, or configure the build based on environment variables. 

A good example is building and linking C code into a rust package:
\begin{minted}{rust}
  fn main() {
    // Tell cargo to rerun the build script if the file changes
    println!("cargo:rerun-if-changed=src/foo.c");
    // `cc` is a create to build and link C code
    cc::Build::new()
      .file("src/foo.c")
      .compile("foo");
  }
\end{minted}
This approach can be particularly useful when incorporating parts of a reference implementation in C or C++. In our case, due to time constraints, we opted to consider using portions of the reference implementation. Additionally, another important use case for our build script is verifying that the selected features during package compilation are correct. This need arose while benchmarking against another hash function with a lower security level.

\subsubsection{Feature flags}
Rust offers a robust feature flag system that enables conditional compilation, providing significant flexibility for writing code optimized or configured for various use cases. 
A practical example, as previously mentioned, involves handling different sets of arithmetic fields. To address the challenge of creating a struct to manage functionality for the same type across different fields, we can leverage feature flags. These allow us to conditionally compile code tailored to the specific field required. This is achieved by adding the following configuration to the Cargo.toml file:
\begin{minted}{toml}
[features]
  default = ["gf256"]
  gf256 = []
  gf251 = []
\end{minted}
Or by giving it when running cargo:
\begin{minted}{bash}
  $ cargo build --features "gf251"
\end{minted}
Now we can use the feature flags in the code:
\begin{minted}{rust}
// arith.rs
pub trait FieldArith {
    // Trait functions
}

// arith/gf256.rs
#[cfg(feature = "gf256")]
impl FieldArith for u8 {
    // implementation ...
}

// arith/gf251.rs
#[cfg(feature = "gf251")]
impl FieldArith for u8 {
    // implementation ...
}

// main.rs
fn polynomial<T: FieldArith + Default>(coeffs: &[T]) -> T {
    let mut result = T::default(); // The `Default` trait
    for (i, coeff) in coeffs.iter().enumerate() {
        result = T::add(result, T::mul(*coeff, T::pow(T::default(), i as u8)));
    }
    result
}

fn main() {
    let coeffs256 = vec![1u8, 2u8, 3u8];
    println!("Field 256: {:?}", polynomial(&coeffs256));
}
\end{minted}
In the example above we use the feature flag to change which trait is implemented for the \rust{u8} type. This means that when we build the binary it only compiles the code that has been specified. 
As our implementation is a reference implementation for the SD-in-the-Head protocol, we have used feature flags to switch between different categories and optimizations. This allows us to easily test and compare different configurations without having to manually change the code or have duplicates.

\subsection{Rust optimizations}
Blazingly fast code does not come without a cost. However, Rust offers several ways to optimize performance. In this section, we will introduce the techniques we employed. We will first explore simple methods to enhance performance, followed by more advanced techniques such as parallelization and SIMD.

\subsection{Build Configuration}
% https://nnethercote.github.io/perf-book/build-configuration.html#alternative-allocators
\subsection{Benchmarking}

\subsubsection{Inlining}
A subtle but effective way to improve performance is to inline functions that are called many times, this removes the overhead of the function call. The Rust compiler will automatically inline functions if they are small enough. But there are some cases where this is not enough and the compiler does not inline. In this case we can use the \texttt{inline} attribute to force the compiler to inline a function. Cachegrind is a good profiling tool to check if a function is inlined by the compiler.
\todo{Maybe revise this}

\subsubsection{Parallelisation}


\subsubsection{SIMD}

\section{Structure}
\todo{describe the structure in detail}
\begin{figure}[H]
  \dirtree{%
    .1 \color{orange}Cargo.toml.
    .1 \color{orange}build.rs.
    .1 ./benches/.
    .1 ./src/.
    .2 \color{orange}main.rs.
    .2 \color{orange}lib.rs.
    .2 \color{teal}api.rs.
    .2 \color{teal}cli.rs.
    .2 \color{teal}keygen.rs.
    .2 \color{teal}witness.rs.
    .2 arith/\dots.
    .3 gf256/\dots.
    .2 constants/.
    .3 \color{teal}params.rs.
    .3 \color{teal}precomputed.rs.
    .3 \color{teal}types.rs.
    .2 kat/\dots.
    .2 mpc/\dots.
    .2 signature/\dots.
    .2 subroutines/\dots.
    .3 prg/\dots.
    .3 \color{teal}commitments.rs.
    .3 \color{teal}hashing.rs.
    .3 \color{teal}merkle\_tree.rs.
    .2 utils/.
    .3 \color{teal}iterator.rs.
  }
  \caption{The directory structure of the project. \texttt{mod.rs} files are omitted for brevity.}
\end{figure}

\section{Subroutines}\label{sub:subroutines}

We started out by implementing the different subroutines that are used in the SD-in-the-Head protocol. We will describe the implementation of each subroutine in detail.

\subsection{PRG}
First we implemented a pseudo random generator (PRG) based on a extendable output hash function (XOF). We used a library called \textit{tiny-keccak}~\cite{tiny-keccak}, which contains implementations of Keccak derived functions specified in the NIST FIPS 202 standard, SP800-185 and KangarooTwelve.

Our initial implementation focused on the NIST category one, which requires a security level of 128 bits. For this we used the SHAKE-128 implementation. But when we later had to adhere to the NIST category tree and five, we had to be able to switch between different implementations of the underlying XOF. For this we implemented a generic trait for the XOF:
\inputminted[firstline=18, lastline=23]{rust}{../sdith/src/subroutines/prg/xof.rs}
This allows us to create a struct that will contain a generic XOF and allow for interchangeability between different XOF implementations:
\inputminted[firstline=25, lastline=27]{rust}{../sdith/src/subroutines/prg/xof.rs}
Now in order to use the correct XOF based on the security level needed for each category, we used a constant to swithc between the different implementations of the SHAKE XOF -- we will discuss how this constant was defined for each category in \autoref{sub:categories}:
\inputminted[firstline=37, lastline=43]{rust}{../sdith/src/subroutines/prg/xof.rs}
We could now call the squeeze funtion to get the random bytes necessary for the seed, salt and key generation.

\subsubsection{Hashing}\label{sec:hashing} % (fold)
As mentioned in the spec, we need a collision-resistant hash function. For this we also used the tiny-keccak library. And again due to the different security levels, we had to be able to switch between different implementations of the underlying hash function. These being the SHA3-256, SHA3-384 and SHA3-512.
This was done by creating a generic trait for the hash function:

\todo{link to the hashing code}

% subsubsection Hashing (end)
%
\subsection{commitments}

\subsubsection{Merkle Tree impl}\label{sub:merkle_tree_impl}
Our merkle tree implementation was done using a sized array as data structure. Meaning that we save all the leafs of the tree in the last 256 values of the array.

\todo{link to the merkle tree code}

We then build the tree buttom up by hashing the left and right children of the current node and storing the result in the next position in the array. We do this until we reach the root node, which we then return as our global commitment.


\subsubsection{Hashing, XOF, and Commitments}

\subsubsection{Galois field arithmetic}\label{sub:gf256_arith}
As stated earlier, we wanted to implement the underlying arithmetic our selves and not rely on there being a library for the arithmetic of the field. This led us to implement the galois field arithmetic.
We did this by creating a trait for field arithmetic:

\todo{link to FieldArith trait}

This allows us to implement the field arithmetic for different types:
\begin{minted}{rust}
pub type FPoint = [u8; 4];

impl FieldArith for FPoint {

    fn field_mul(&self, rhs: Self) -> Self {
        gf256_ext32_mul(*self, rhs)
    }

}
\end{minted}
And we can now use the field arithmetic in our code to for example generate beaver triples:
\todo{link to the beaver triple code}

Furthermore, all our arithmetic can be tested individually, as we can see in the following test:
\todo{link to the arith test}
\section{Key generation}

\section{MPC}\label{sub:mpc_algo}
After implementing all the subroutines, we could now do the MPC computations necessary for signing and verification.

\section{Comparison with spec impl}\label{sub:comparison_with_spec_impl}
In order to verify and debug our own code we tried to setup our implementation to fit with the specification implementation in C, meaning we had to run their code in order to get the intermediate data in order to compare the internal states.
Here we found a bug in the way the view-opening challenges was calculated, they had forgotten to finalize the Shake before squeezing, which was a problem because in the tiny keccak rust library this was always done when initializing a hash or xof.
Furthermore we found that in order to generate the same output from the xof we needed to first rotate the permutation once by supplying an empty vector before actually using the function.
We also found that the merkle tree was not using the salt.

\section{Benchmarking}\label{sub:benchmarking} % (fold)
We utilized Criterion.rs~\cite{criterion} for initial benchmarking. Criterion.rs is a statistics-driven benchmarking library for Rust, offering robust tools to measure and analyze performance reliably.

\subsection{Benchmark setup}
Benchmarking functions are organized in a separate folder named bench. Within this folder, we create files dedicated to specific benchmarking tasks. To enable benchmarking, the target functions must be declared as pub (public) and exported through a lib.rs file. This structure creates a package that can be accessed and invoked by the benchmarking files.

\subsection{Test Environment}
Benchmarks were conducted on two machines: a Dell XPS 15 9510 and a MacBook Pro with an M2 Pro chip. These machines represent different architectures, allowing us to observe subtle variations in runtime performance based on hardware differences. The specifications of the machines are as follows:

\begin{itemize}
  \item \textbf{Dell XPS 15 9510}:
        \begin{itemize}
          \item \textbf{CPU}: 11th Gen Intel Core i7-11800H @ 2.30GHz x 16
          \item \textbf{GPU}: GeForce RTX 3050 Ti Mobile
          \item \textbf{RAM}: 32GB DDR4 3200MHz
          \item \textbf{OS}: Ubuntu 22.04.5 LTS, 64-bit
        \end{itemize}
  \item \textbf{MacBook Pro (M2 Pro)}:
        \begin{itemize}
          \item \textbf{CPU}: Apple M2 Pro (10-core CPU: 6 performance cores, 4 efficiency cores)
          \item \textbf{RAM}: 32GB LPDDR5
          \item \textbf{OS}: macOS Sequoia 15.1.1 (24B91)
        \end{itemize}
\end{itemize}
This setup provided a diverse testing environment, ensuring our benchmarks accounted for both x86\_64 and ARM architectures.
% subsection Benchmarking (end)

\subsection{Feature flags}\label{sub:feature_flags} % (fold)
To maintain a baseline implementation while enabling flexibility to switch between different NIST categories, we utilized Rust's feature flags. These flags support conditional compilation, allowing multiple implementations of the same function to coexist in the codebase. Only the specific implementation required is compiled, based on the flag provided to the compiler. This approach made it possible to test individual optimizations in isolation, rather than applying all optimizations simultaneously.

The way we implemented this was by using the \rust{cfg} attribute. This attribute allows us to specify a feature flag, which can be used to conditionally compile code. For example, we can use the following code to conditionally compile a function:
\begin{minted}{rust}
#[cfg(not(feature = "simd"))]
pub fn gf256_add_vector(vz: &mut [u8], vx: &[u8]) {
... add vector the naive way ...
}

#[cfg(feature = "simd")]
pub fn gf256_add_vector(vz: &mut [u8], vx: &[u8]) {
... add vector the simd way ...
}
\end{minted}
This ensures that when the feature flag for $simd$ is set the compiled code will have the simd function only.

\section{Categories}\label{sub:categories} % (fold)
We tried reworking to use dynamically sized vectors instead of compile time sized arrays. Which proved to be a big rework and refactor. Benching showed slower running times. So instead we went with compile time script to set the constants.

This allows us to change between categories in the following ways:
\begin{itemize}
  \item By environment variable
  \item By Feature flag
\end{itemize}

\subsubsection{Environment variables}\label{sub:env_vars}
Environment variables can be set in the command line or given to the compiler when invoking cargo. The following code snippet shows how you would pass the category to the compiler. Due to the fact that we went with
\begin{minted}{bash}
~$ SDITH_CATEGORY=5 cargo bench
\end{minted}

\subsubsection{Feature flags categories}\label{sub:feature_flags_categories}
After implementing the environment variables, we realised that we could also use feature flags to change the category. This is done by adding the following to the Cargo.toml file.
\begin{minted}{rust}
[features]
default = ["category_one", "optimized"]

# Categories
category_one = []
category_three = []
\end{minted}
Where we state the default category to be one, and the optimized flag to be enabled.
Now in the build.rs file we can change the category based on the feature flag in the following way:
\begin{minted}{rust}
fn get_feature_flag_category() -> Result<Category, String> {
    if cfg!(feature = "category_one") {
        Ok(CATEGORY_ONE)
    } else if cfg!(feature = "category_three") {
        Ok(CATEGORY_THREE)
    } else if cfg!(feature = "category_five") {
        Ok(CATEGORY_FIVE)
    } else {
        Err("No category feature flag set".to_string())
    }
}
\end{minted}
One can now invoke the benchmarking for category one by running the following command:
\begin{minted}{bash}
  ~$ cargo bench --features category_one
\end{minted}

\section{Optimizations}
In order to find optimizations we used the samply CPU profiler~\cite{Stange2024mstange}, to locate long running functions and loops. This was done by building the binary with cargo and supplying it to the samply CPU profiler. The profiler then generates seeds $sd_{e,i}$ for each $i \in [N]$ using $\Pi_C$. a call stack and a flamegraph from the binary, this shows how many cycles of the cpu were used in different sections of the code.
\todo{Maybe a picture of how the base program looked in the call stack}


\subsection{Parallelisation}\label{sub:rayon} % (fold)

% subsection Rayon (end)

\subsection{SIMD}\label{sub:simd} % (fold)

% subsection SIMD (end)

\subsection{Hash functions}
Blake3, Haraka v2, KangarooTwelve, Xoodyak

\section{Kat for NIST}\label{sub:kat_for_nist} % (fold)

% subsection Kat for NIST (end)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Benchmarks}\label{ch:bench}
diaries of benchmarks.
discussion of results

\todo{old benchmarks \url{https://asecuritysite.com/openssl/openssl3_b2}}

Test on both mac and linux.
nightly vs stable rust
different hashes
benching at different tags
parallelisation, test for amount of cores, 2, 4, 8, 16
no turbo boost (max 2.6 GHz)
cycles per bytes
compare ours to the optimised reference

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}\label{ch:conclusion}

wrap up and pose future work
what should people continue with
point to round 2 NIST
work in context of timeline

Idea: We could add subroutines as parameters to the API. Then the developer could just add their own subroutines defined by our traits.

\todo{conclude on the problem statement from the introduction}

future work: Verkle trees optimisation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\appendix
\chapter{The Technical Details}

\todo{\dots}

\end{document}
